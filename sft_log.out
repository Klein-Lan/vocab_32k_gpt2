[2024-04-06 22:09:08,914] torch.distributed.run: [WARNING] 
[2024-04-06 22:09:08,914] torch.distributed.run: [WARNING] *****************************************
[2024-04-06 22:09:08,914] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-06 22:09:08,914] torch.distributed.run: [WARNING] *****************************************
[2024-04-06 22:09:12,501] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-06 22:09:12,551] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-06 22:09:12,595] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-06 22:09:12,602] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-06 22:09:12,622] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-06 22:09:12,727] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-06 22:09:12,746] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-06 22:09:12,811] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-06 22:09:12,923] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-06 22:09:13,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-06 22:09:13,092] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-06 22:09:13,093] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-06 22:09:13,238] [INFO] [comm.py:637:init_distributed] cdb=None
W0406 22:09:13.782234 140235438175232 logging.py:61] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
W0406 22:09:15.307381 139722906600448 train.py:63] Loaded ckpt from: ckpt/vocab_32k_gpt2/
W0406 22:09:15.308244 139722906600448 train.py:68] #parameters: 110418432
W0406 22:09:15.410269 140484466181120 train.py:63] Loaded ckpt from: ckpt/vocab_32k_gpt2/
W0406 22:09:15.411077 140484466181120 train.py:68] #parameters: 110418432
W0406 22:09:15.411285 140596130501632 train.py:63] Loaded ckpt from: ckpt/vocab_32k_gpt2/
W0406 22:09:15.413088 140596130501632 train.py:68] #parameters: 110418432
W0406 22:09:15.424099 140616465560576 train.py:63] Loaded ckpt from: ckpt/vocab_32k_gpt2/
W0406 22:09:15.425068 140616465560576 train.py:68] #parameters: 110418432
W0406 22:09:15.460053 140235438175232 train.py:63] Loaded ckpt from: ckpt/vocab_32k_gpt2/
W0406 22:09:15.460820 140235438175232 train.py:68] #parameters: 110418432
W0406 22:09:15.711204 139931872916480 train.py:63] Loaded ckpt from: ckpt/vocab_32k_gpt2/
W0406 22:09:15.712032 139931872916480 train.py:68] #parameters: 110418432
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /data/yslan/.cache/torch_extensions/py39_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.27300071716308594 seconds
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /data/yslan/.cache/torch_extensions/py39_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.33577466011047363 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.5021703243255615 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.3019566535949707 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.6024203300476074 seconds
wandb: Tracking run with wandb version 0.16.4
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /data/yslan/.cache/torch_extensions/py39_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.25119662284851074 seconds
[2024-04-06 22:09:19,680] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-06 22:09:20,492] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-04-06 22:09:20,494] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-04-06 22:09:20,494] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-06 22:09:20,498] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-04-06 22:09:20,498] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-04-06 22:09:20,498] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-04-06 22:09:20,498] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500,000,000
[2024-04-06 22:09:20,498] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500,000,000
[2024-04-06 22:09:20,498] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-04-06 22:09:20,499] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
W0406 22:09:21.177492 139722906600448 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2_instruction
W0406 22:09:21.228922 140596130501632 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2_instruction
W0406 22:09:21.221548 140484466181120 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2_instruction
W0406 22:09:21.287010 139931872916480 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2_instruction
W0406 22:09:21.420285 140616465560576 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2_instruction
[2024-04-06 22:09:21,984] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-06 22:09:21,985] [INFO] [utils.py:801:see_memory_usage] MA 0.29 GB         Max_MA 0.29 GB         CA 0.29 GB         Max_CA 0 GB 
[2024-04-06 22:09:21,985] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 81.88 GB, percent = 16.3%
[2024-04-06 22:09:22,146] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-06 22:09:22,147] [INFO] [utils.py:801:see_memory_usage] MA 0.29 GB         Max_MA 0.35 GB         CA 0.36 GB         Max_CA 0 GB 
[2024-04-06 22:09:22,147] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 84.9 GB, percent = 16.9%
[2024-04-06 22:09:22,148] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-04-06 22:09:22,317] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-06 22:09:22,318] [INFO] [utils.py:801:see_memory_usage] MA 0.29 GB         Max_MA 0.29 GB         CA 0.36 GB         Max_CA 0 GB 
[2024-04-06 22:09:22,318] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 86.52 GB, percent = 17.2%
[2024-04-06 22:09:22,319] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2024-04-06 22:09:22,320] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-04-06 22:09:22,320] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-04-06 22:09:22,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-04-06 22:09:22,320] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-06 22:09:22,321] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-06 22:09:22,321] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-06 22:09:22,321] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-06 22:09:22,321] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8a30548df0>
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-06 22:09:22,322] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-06 22:09:22,323] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 5
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-06 22:09:22,324] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-06 22:09:22,325] [INFO] [config.py:1000:print]   train_batch_size ............. 300
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  10
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   world_size ................... 6
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-06 22:09:22,326] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-04-06 22:09:22,326] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 300, 
    "train_micro_batch_size_per_gpu": 10, 
    "gradient_accumulation_steps": 5, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
W0406 22:09:22.327202 140235438175232 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2_instruction
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0406 22:09:24.167050 140235438175232 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 0, Global Step: 10, Data Step: 50, Loss: 2.546875, Token per second per gpu: 14945.10232740685
Epoch: 0, Global Step: 20, Data Step: 100, Loss: 2.125, Token per second per gpu: 20693.839193354463
Epoch: 0, Global Step: 30, Data Step: 150, Loss: 3.375, Token per second per gpu: 20249.42548712898
Epoch: 0, Global Step: 40, Data Step: 200, Loss: 1.640625, Token per second per gpu: 20162.209690605414
Epoch: 0, Global Step: 50, Data Step: 250, Loss: 2.78125, Token per second per gpu: 20463.81241431696
Epoch: 0, Global Step: 60, Data Step: 300, Loss: 2.515625, Token per second per gpu: 20117.221558507586
Epoch: 0, Global Step: 70, Data Step: 350, Loss: 2.359375, Token per second per gpu: 19978.394032637585
Epoch: 0, Global Step: 80, Data Step: 400, Loss: 2.5625, Token per second per gpu: 20082.01593292625
Epoch: 0, Global Step: 90, Data Step: 450, Loss: 2.328125, Token per second per gpu: 20057.50704313398
Epoch: 0, Global Step: 100, Data Step: 500, Loss: 2.40625, Token per second per gpu: 19938.827008676868
Epoch: 0, Global Step: 110, Data Step: 550, Loss: 2.1875, Token per second per gpu: 19779.466051850173
Epoch: 0, Global Step: 120, Data Step: 600, Loss: 2.421875, Token per second per gpu: 19827.944281713513
Epoch: 0, Global Step: 130, Data Step: 650, Loss: 2.125, Token per second per gpu: 19840.55892725979
Epoch: 0, Global Step: 140, Data Step: 700, Loss: 2.328125, Token per second per gpu: 20197.549318394063
Epoch: 0, Global Step: 150, Data Step: 750, Loss: 2.515625, Token per second per gpu: 20055.859547732114
Epoch: 0, Global Step: 160, Data Step: 800, Loss: 2.203125, Token per second per gpu: 20439.27878520527
Epoch: 0, Global Step: 170, Data Step: 850, Loss: 2.125, Token per second per gpu: 20278.61305997385
Epoch: 0, Global Step: 180, Data Step: 900, Loss: 2.34375, Token per second per gpu: 20023.084372502806
Epoch: 0, Global Step: 190, Data Step: 950, Loss: 2.15625, Token per second per gpu: 20221.30180412109
Epoch: 0, Global Step: 200, Data Step: 1000, Loss: 2.09375, Token per second per gpu: 20240.22219230071
Epoch: 0, Global Step: 210, Data Step: 1050, Loss: 2.515625, Token per second per gpu: 20500.62132868426
Epoch: 0, Global Step: 220, Data Step: 1100, Loss: 2.390625, Token per second per gpu: 20222.93260162706
Epoch: 0, Global Step: 230, Data Step: 1150, Loss: 3.421875, Token per second per gpu: 20373.308529348902
Epoch: 0, Global Step: 240, Data Step: 1200, Loss: 3.703125, Token per second per gpu: 20231.35622755845
Epoch: 0, Global Step: 250, Data Step: 1250, Loss: 3.71875, Token per second per gpu: 20070.47616083588
Epoch: 0, Global Step: 260, Data Step: 1300, Loss: 3.390625, Token per second per gpu: 19929.554802651404
Epoch: 0, Global Step: 270, Data Step: 1350, Loss: 3.1875, Token per second per gpu: 20177.96762824239
Epoch: 0, Global Step: 280, Data Step: 1400, Loss: 2.75, Token per second per gpu: 19748.68871616811
Epoch: 0, Global Step: 290, Data Step: 1450, Loss: 3.109375, Token per second per gpu: 20052.257355590504
Epoch: 0, Global Step: 300, Data Step: 1500, Loss: 3.015625, Token per second per gpu: 19914.443449801736
Epoch: 0, Global Step: 310, Data Step: 1550, Loss: 2.703125, Token per second per gpu: 20350.738412383736
Epoch: 0, Global Step: 320, Data Step: 1600, Loss: 3.5625, Token per second per gpu: 20091.08528228714
Epoch: 0, Global Step: 330, Data Step: 1650, Loss: 3.015625, Token per second per gpu: 20290.956752966395
Epoch: 0, Global Step: 340, Data Step: 1700, Loss: 3.25, Token per second per gpu: 19928.609359171685
Epoch: 0, Global Step: 350, Data Step: 1750, Loss: 2.96875, Token per second per gpu: 20071.29178985759
Epoch: 0, Global Step: 360, Data Step: 1800, Loss: 2.75, Token per second per gpu: 19883.773325776547
Epoch: 0, Global Step: 370, Data Step: 1850, Loss: 2.859375, Token per second per gpu: 20109.302622053907
Epoch: 0, Global Step: 380, Data Step: 1900, Loss: 2.78125, Token per second per gpu: 20248.238868317454
Epoch: 0, Global Step: 390, Data Step: 1950, Loss: 3.0, Token per second per gpu: 20050.56167396192
Epoch: 0, Global Step: 400, Data Step: 2000, Loss: 4.0625, Token per second per gpu: 19685.881235306293
Epoch: 0, Global Step: 410, Data Step: 2050, Loss: 3.65625, Token per second per gpu: 19595.965151383472
Epoch: 0, Global Step: 420, Data Step: 2100, Loss: 3.609375, Token per second per gpu: 19746.619817304996
Epoch: 0, Global Step: 430, Data Step: 2150, Loss: 3.359375, Token per second per gpu: 11397.12600558378
Epoch: 0, Global Step: 440, Data Step: 2200, Loss: 3.0, Token per second per gpu: 14164.938979484725
Epoch: 0, Global Step: 450, Data Step: 2250, Loss: 1.7890625, Token per second per gpu: 18324.760022320625
Epoch: 0, Global Step: 460, Data Step: 2300, Loss: 1.1796875, Token per second per gpu: 20139.15118125297
Epoch: 0, Global Step: 470, Data Step: 2350, Loss: 1.9140625, Token per second per gpu: 20057.247959123288
Epoch: 0, Global Step: 480, Data Step: 2400, Loss: 2.828125, Token per second per gpu: 19739.17696592302
Epoch: 0, Global Step: 490, Data Step: 2450, Loss: 4.25, Token per second per gpu: 20019.311609357585
Epoch: 0, Global Step: 500, Data Step: 2500, Loss: 4.03125, Token per second per gpu: 19839.67267977003
Epoch: 0, Global Step: 510, Data Step: 2550, Loss: 3.765625, Token per second per gpu: 19655.134632977035
Epoch: 0, Global Step: 520, Data Step: 2600, Loss: 3.703125, Token per second per gpu: 19677.021232368534
Epoch: 0, Global Step: 530, Data Step: 2650, Loss: 3.640625, Token per second per gpu: 19838.99379626476
Epoch: 0, Global Step: 540, Data Step: 2700, Loss: 3.5625, Token per second per gpu: 19750.41219125203
Epoch: 0, Global Step: 550, Data Step: 2750, Loss: 4.375, Token per second per gpu: 20008.504242743624
Epoch: 0, Global Step: 560, Data Step: 2800, Loss: 4.21875, Token per second per gpu: 19996.34101884624
Epoch: 0, Global Step: 570, Data Step: 2850, Loss: 3.796875, Token per second per gpu: 19707.979283328354
Epoch: 0, Global Step: 580, Data Step: 2900, Loss: 3.125, Token per second per gpu: 20296.367863831325
Epoch: 0, Global Step: 590, Data Step: 2950, Loss: 3.65625, Token per second per gpu: 19920.702736130977
Epoch: 0, Global Step: 600, Data Step: 3000, Loss: 3.78125, Token per second per gpu: 19938.655767837445
Epoch: 0, Global Step: 610, Data Step: 3050, Loss: 2.015625, Token per second per gpu: 19950.72554118114
Epoch: 0, Global Step: 620, Data Step: 3100, Loss: 3.875, Token per second per gpu: 19806.288759922554
Epoch: 0, Global Step: 630, Data Step: 3150, Loss: 3.390625, Token per second per gpu: 20322.317994773144
Epoch: 0, Global Step: 640, Data Step: 3200, Loss: 4.1875, Token per second per gpu: 19746.756180949564
Epoch: 0, Global Step: 650, Data Step: 3250, Loss: 3.71875, Token per second per gpu: 19865.154103661003
Epoch: 0, Global Step: 660, Data Step: 3300, Loss: 4.03125, Token per second per gpu: 19772.210504943563
Epoch: 0, Global Step: 670, Data Step: 3350, Loss: 2.421875, Token per second per gpu: 19738.305920592913
Epoch: 0, Global Step: 680, Data Step: 3400, Loss: 4.03125, Token per second per gpu: 19879.43986486136
Epoch: 0, Global Step: 690, Data Step: 3450, Loss: 2.6875, Token per second per gpu: 19945.41229269036
Epoch: 0, Global Step: 700, Data Step: 3500, Loss: 3.0, Token per second per gpu: 19978.27638250097
Epoch: 0, Global Step: 710, Data Step: 3550, Loss: 3.328125, Token per second per gpu: 19861.84346095595
Epoch: 0, Global Step: 720, Data Step: 3600, Loss: 4.21875, Token per second per gpu: 20192.454294584095
Epoch: 0, Global Step: 730, Data Step: 3650, Loss: 3.8125, Token per second per gpu: 19812.967922897435
Epoch: 0, Global Step: 740, Data Step: 3700, Loss: 3.296875, Token per second per gpu: 19889.867257580983
Epoch: 0, Global Step: 750, Data Step: 3750, Loss: 3.78125, Token per second per gpu: 19891.032141151656
Epoch: 0, Global Step: 760, Data Step: 3800, Loss: 3.625, Token per second per gpu: 19713.62877104624
Epoch: 0, Global Step: 770, Data Step: 3850, Loss: 4.125, Token per second per gpu: 19882.46810052434
Epoch: 0, Global Step: 780, Data Step: 3900, Loss: 3.78125, Token per second per gpu: 20144.795683969754
Epoch: 0, Global Step: 790, Data Step: 3950, Loss: 3.234375, Token per second per gpu: 19723.357556923536
Epoch: 0, Global Step: 800, Data Step: 4000, Loss: 2.65625, Token per second per gpu: 19907.67632866095
Epoch: 0, Global Step: 810, Data Step: 4050, Loss: 3.984375, Token per second per gpu: 19918.290589361062
Epoch: 0, Global Step: 820, Data Step: 4100, Loss: 3.421875, Token per second per gpu: 21298.387855968445
Epoch: 0, Global Step: 830, Data Step: 4150, Loss: 3.828125, Token per second per gpu: 21209.829208463394
Epoch: 0, Global Step: 840, Data Step: 4200, Loss: 3.21875, Token per second per gpu: 20012.424739779883
Epoch: 0, Global Step: 850, Data Step: 4250, Loss: 3.796875, Token per second per gpu: 19932.81682734248
Epoch: 0, Global Step: 860, Data Step: 4300, Loss: 3.59375, Token per second per gpu: 19891.348671126452
Epoch: 0, Global Step: 870, Data Step: 4350, Loss: 3.09375, Token per second per gpu: 20236.08608895171
Epoch: 0, Global Step: 880, Data Step: 4400, Loss: 2.453125, Token per second per gpu: 19878.34662844614
Epoch: 0, Global Step: 890, Data Step: 4450, Loss: 4.03125, Token per second per gpu: 19958.54898914965
Epoch: 0, Global Step: 900, Data Step: 4500, Loss: 3.96875, Token per second per gpu: 19738.52000084304
Epoch: 0, Global Step: 910, Data Step: 4550, Loss: 4.6875, Token per second per gpu: 19657.658721973698
Epoch: 0, Global Step: 920, Data Step: 4600, Loss: 3.484375, Token per second per gpu: 19536.693879509763
Epoch: 0, Global Step: 930, Data Step: 4650, Loss: 3.625, Token per second per gpu: 19721.53103686149
Epoch: 0, Global Step: 940, Data Step: 4700, Loss: 4.6875, Token per second per gpu: 19631.375702324098
Epoch: 0, Global Step: 950, Data Step: 4750, Loss: 3.34375, Token per second per gpu: 19623.983411663397
Epoch: 0, Global Step: 960, Data Step: 4800, Loss: 4.25, Token per second per gpu: 19605.7567541479
Epoch: 0, Global Step: 970, Data Step: 4850, Loss: 2.359375, Token per second per gpu: 19522.2439004364
Epoch: 0, Global Step: 980, Data Step: 4900, Loss: 3.421875, Token per second per gpu: 19672.403283912034
Epoch: 0, Global Step: 990, Data Step: 4950, Loss: 3.609375, Token per second per gpu: 19787.668618020543
Epoch: 0, Global Step: 1000, Data Step: 5000, Loss: 3.859375, Token per second per gpu: 19723.45827532855
Epoch: 0, Global Step: 1010, Data Step: 5050, Loss: 3.671875, Token per second per gpu: 19886.803802726627
Epoch: 0, Global Step: 1020, Data Step: 5100, Loss: 3.953125, Token per second per gpu: 19778.508198965552
Epoch: 0, Global Step: 1030, Data Step: 5150, Loss: 3.625, Token per second per gpu: 19933.269754923433
Epoch: 0, Global Step: 1040, Data Step: 5200, Loss: 4.6875, Token per second per gpu: 19832.99949761868
Epoch: 0, Global Step: 1050, Data Step: 5250, Loss: 3.234375, Token per second per gpu: 19885.345898757623
Epoch: 0, Global Step: 1060, Data Step: 5300, Loss: 3.4375, Token per second per gpu: 19797.694033698448
Epoch: 0, Global Step: 1070, Data Step: 5350, Loss: 4.03125, Token per second per gpu: 20132.11236149798
Epoch: 0, Global Step: 1080, Data Step: 5400, Loss: 3.328125, Token per second per gpu: 19825.45772299039
Epoch: 0, Global Step: 1090, Data Step: 5450, Loss: 3.671875, Token per second per gpu: 19849.284120758657
Epoch: 0, Global Step: 1100, Data Step: 5500, Loss: 3.34375, Token per second per gpu: 19722.616873489842
Epoch: 0, Global Step: 1110, Data Step: 5550, Loss: 2.703125, Token per second per gpu: 19982.7244472513
Epoch: 0, Global Step: 1120, Data Step: 5600, Loss: 3.703125, Token per second per gpu: 19783.462053179162
Epoch: 0, Global Step: 1130, Data Step: 5650, Loss: 3.296875, Token per second per gpu: 19737.44184556293
Epoch: 0, Global Step: 1140, Data Step: 5700, Loss: 2.109375, Token per second per gpu: 19928.876411971643
Epoch: 0, Global Step: 1150, Data Step: 5750, Loss: 3.328125, Token per second per gpu: 19598.240833905977
Epoch: 0, Global Step: 1160, Data Step: 5800, Loss: 2.5625, Token per second per gpu: 20203.362888765525
Epoch: 0, Global Step: 1170, Data Step: 5850, Loss: 3.71875, Token per second per gpu: 19838.618084275517
Epoch: 0, Global Step: 1180, Data Step: 5900, Loss: 4.03125, Token per second per gpu: 19767.001727644503
Epoch: 0, Global Step: 1190, Data Step: 5950, Loss: 3.890625, Token per second per gpu: 19796.60046224377
Epoch: 0, Global Step: 1200, Data Step: 6000, Loss: 2.265625, Token per second per gpu: 19791.66155051776
Epoch: 0, Global Step: 1210, Data Step: 6050, Loss: 4.28125, Token per second per gpu: 20019.123680355653
Epoch: 0, Global Step: 1220, Data Step: 6100, Loss: 2.484375, Token per second per gpu: 19867.27640565474
Epoch: 0, Global Step: 1230, Data Step: 6150, Loss: 2.703125, Token per second per gpu: 19708.994168626352
Epoch: 0, Global Step: 1240, Data Step: 6200, Loss: 3.265625, Token per second per gpu: 19632.829628645788
Epoch: 0, Global Step: 1250, Data Step: 6250, Loss: 3.4375, Token per second per gpu: 19936.379377213958
Epoch: 0, Global Step: 1260, Data Step: 6300, Loss: 3.09375, Token per second per gpu: 19671.031601819308
Epoch: 0, Global Step: 1270, Data Step: 6350, Loss: 4.0, Token per second per gpu: 19863.496898921476
Epoch: 0, Global Step: 1280, Data Step: 6400, Loss: 3.046875, Token per second per gpu: 20103.090255586783
Epoch: 0, Global Step: 1290, Data Step: 6450, Loss: 3.703125, Token per second per gpu: 20087.951831053902
Epoch: 0, Global Step: 1300, Data Step: 6500, Loss: 3.046875, Token per second per gpu: 19825.304347049805
Epoch: 0, Global Step: 1310, Data Step: 6550, Loss: 3.359375, Token per second per gpu: 19913.027652539567
Epoch: 0, Global Step: 1320, Data Step: 6600, Loss: 3.96875, Token per second per gpu: 19764.945544192182
Epoch: 0, Global Step: 1330, Data Step: 6650, Loss: 3.40625, Token per second per gpu: 19882.60910814121
Epoch: 0, Global Step: 1340, Data Step: 6700, Loss: 3.46875, Token per second per gpu: 19953.358383916373
Epoch: 0, Global Step: 1350, Data Step: 6750, Loss: 3.9375, Token per second per gpu: 20037.315765625648
Epoch: 0, Global Step: 1360, Data Step: 6800, Loss: 2.90625, Token per second per gpu: 20073.352355008676
Epoch: 0, Global Step: 1370, Data Step: 6850, Loss: 2.03125, Token per second per gpu: 19837.024307306157
Epoch: 0, Global Step: 1380, Data Step: 6900, Loss: 3.421875, Token per second per gpu: 19790.92776594583
Epoch: 0, Global Step: 1390, Data Step: 6950, Loss: 3.140625, Token per second per gpu: 20192.59498664307
Epoch: 0, Global Step: 1400, Data Step: 7000, Loss: 3.515625, Token per second per gpu: 19911.209219956792
Epoch: 0, Global Step: 1410, Data Step: 7050, Loss: 3.890625, Token per second per gpu: 19980.656418734034
Epoch: 0, Global Step: 1420, Data Step: 7100, Loss: 3.34375, Token per second per gpu: 19908.70191929599
Epoch: 0, Global Step: 1430, Data Step: 7150, Loss: 2.890625, Token per second per gpu: 20388.35897581814
Epoch: 0, Global Step: 1440, Data Step: 7200, Loss: 3.453125, Token per second per gpu: 19917.21967630761
Epoch: 0, Global Step: 1450, Data Step: 7250, Loss: 4.21875, Token per second per gpu: 19978.88565049128
Epoch: 0, Global Step: 1460, Data Step: 7300, Loss: 3.5625, Token per second per gpu: 19977.105904165815
Epoch: 0, Global Step: 1470, Data Step: 7350, Loss: 2.3125, Token per second per gpu: 19883.015390617333
Epoch: 0, Global Step: 1480, Data Step: 7400, Loss: 2.96875, Token per second per gpu: 20037.711194302123
Epoch: 0, Global Step: 1490, Data Step: 7450, Loss: 2.8125, Token per second per gpu: 19986.767849220054
Epoch: 0, Global Step: 1500, Data Step: 7500, Loss: 3.984375, Token per second per gpu: 20037.445142935765
Epoch: 0, Global Step: 1510, Data Step: 7550, Loss: 3.671875, Token per second per gpu: 19928.058631606676
Epoch: 0, Global Step: 1520, Data Step: 7600, Loss: 3.375, Token per second per gpu: 20269.44750313287
Epoch: 0, Global Step: 1530, Data Step: 7650, Loss: 2.59375, Token per second per gpu: 19758.028942057186
Epoch: 0, Global Step: 1540, Data Step: 7700, Loss: 2.109375, Token per second per gpu: 19890.832242370383
Epoch: 0, Global Step: 1550, Data Step: 7750, Loss: 4.53125, Token per second per gpu: 19886.713932058152
Epoch: 0, Global Step: 1560, Data Step: 7800, Loss: 3.0625, Token per second per gpu: 19822.28617913225
Epoch: 0, Global Step: 1570, Data Step: 7850, Loss: 3.3125, Token per second per gpu: 19691.335331102902
Epoch: 0, Global Step: 1580, Data Step: 7900, Loss: 3.03125, Token per second per gpu: 20202.448875303
Epoch: 0, Global Step: 1590, Data Step: 7950, Loss: 4.15625, Token per second per gpu: 19783.756396178604
Epoch: 0, Global Step: 1600, Data Step: 8000, Loss: 3.546875, Token per second per gpu: 20103.815000130875
Epoch: 0, Global Step: 1610, Data Step: 8050, Loss: 3.546875, Token per second per gpu: 19687.863061054853
Epoch: 0, Global Step: 1620, Data Step: 8100, Loss: 3.359375, Token per second per gpu: 19961.513796806248
Epoch: 0, Global Step: 1630, Data Step: 8150, Loss: 3.3125, Token per second per gpu: 20131.31499435161
Epoch: 0, Global Step: 1640, Data Step: 8200, Loss: 3.453125, Token per second per gpu: 19583.637730128303
Epoch: 0, Global Step: 1650, Data Step: 8250, Loss: 3.828125, Token per second per gpu: 20243.97849704524
Epoch: 0, Global Step: 1660, Data Step: 8300, Loss: 2.9375, Token per second per gpu: 19953.5102250184
Epoch: 0, Global Step: 1670, Data Step: 8350, Loss: 3.0625, Token per second per gpu: 19853.893901980056
Epoch: 0, Global Step: 1680, Data Step: 8400, Loss: 4.5625, Token per second per gpu: 19911.80554209805
Epoch: 0, Global Step: 1690, Data Step: 8450, Loss: 3.59375, Token per second per gpu: 20253.554247612927
Epoch: 0, Global Step: 1700, Data Step: 8500, Loss: 4.1875, Token per second per gpu: 19810.29725229131
Epoch: 0, Global Step: 1710, Data Step: 8550, Loss: 3.390625, Token per second per gpu: 20038.73003257151
Epoch: 0, Global Step: 1720, Data Step: 8600, Loss: 3.578125, Token per second per gpu: 20004.79958619978
Epoch: 0, Global Step: 1730, Data Step: 8650, Loss: 3.546875, Token per second per gpu: 19919.420188429965
Epoch: 0, Global Step: 1740, Data Step: 8700, Loss: 3.21875, Token per second per gpu: 20339.929244542946
Epoch: 0, Global Step: 1750, Data Step: 8750, Loss: 2.53125, Token per second per gpu: 19707.095978674733
Epoch: 0, Global Step: 1760, Data Step: 8800, Loss: 3.6875, Token per second per gpu: 19891.884657832477
Epoch: 0, Global Step: 1770, Data Step: 8850, Loss: 3.90625, Token per second per gpu: 19830.39154429729
Epoch: 0, Global Step: 1780, Data Step: 8900, Loss: 3.59375, Token per second per gpu: 20119.760358759748
Epoch: 0, Global Step: 1790, Data Step: 8950, Loss: 2.359375, Token per second per gpu: 19808.928017792325
Epoch: 0, Global Step: 1800, Data Step: 9000, Loss: 2.515625, Token per second per gpu: 19879.613402588417
Epoch: 0, Global Step: 1810, Data Step: 9050, Loss: 3.671875, Token per second per gpu: 20080.806791712526
Epoch: 0, Global Step: 1820, Data Step: 9100, Loss: 1.6875, Token per second per gpu: 19637.762485841613
Epoch: 0, Global Step: 1830, Data Step: 9150, Loss: 4.21875, Token per second per gpu: 19905.218072848405
Epoch: 0, Global Step: 1840, Data Step: 9200, Loss: 3.8125, Token per second per gpu: 19847.729353026287
Epoch: 0, Global Step: 1850, Data Step: 9250, Loss: 3.171875, Token per second per gpu: 19617.141443471224
Epoch: 0, Global Step: 1860, Data Step: 9300, Loss: 3.640625, Token per second per gpu: 20094.970329585474
Epoch: 0, Global Step: 1870, Data Step: 9350, Loss: 3.4375, Token per second per gpu: 20080.20574929234
Epoch: 0, Global Step: 1880, Data Step: 9400, Loss: 3.640625, Token per second per gpu: 20322.40626844497
Epoch: 0, Global Step: 1890, Data Step: 9450, Loss: 3.328125, Token per second per gpu: 20007.29144853157
Epoch: 0, Global Step: 1900, Data Step: 9500, Loss: 3.21875, Token per second per gpu: 20307.21334258851
Epoch: 0, Global Step: 1910, Data Step: 9550, Loss: 3.921875, Token per second per gpu: 20200.481242621536
Epoch: 0, Global Step: 1920, Data Step: 9600, Loss: 3.28125, Token per second per gpu: 19849.581893449722
Epoch: 0, Global Step: 1930, Data Step: 9650, Loss: 3.71875, Token per second per gpu: 20210.500701345005
Epoch: 0, Global Step: 1940, Data Step: 9700, Loss: 3.640625, Token per second per gpu: 20093.303513864143
Epoch: 0, Global Step: 1950, Data Step: 9750, Loss: 3.71875, Token per second per gpu: 19854.748561617682
Epoch: 0, Global Step: 1960, Data Step: 9800, Loss: 2.234375, Token per second per gpu: 19948.22884760095
Epoch: 0, Global Step: 1970, Data Step: 9850, Loss: 3.15625, Token per second per gpu: 20018.677479749473
Epoch: 0, Global Step: 1980, Data Step: 9900, Loss: 4.25, Token per second per gpu: 19828.287367412264
Epoch: 0, Global Step: 1990, Data Step: 9950, Loss: 3.328125, Token per second per gpu: 20125.93059825274
Epoch: 0, Global Step: 2000, Data Step: 10000, Loss: 3.265625, Token per second per gpu: 19964.913064576132
Epoch: 0, Global Step: 2010, Data Step: 10050, Loss: 2.875, Token per second per gpu: 19777.169400707327
Epoch: 0, Global Step: 2020, Data Step: 10100, Loss: 3.015625, Token per second per gpu: 20321.149739001114
Epoch: 0, Global Step: 2030, Data Step: 10150, Loss: 2.859375, Token per second per gpu: 20039.057451486704
Epoch: 0, Global Step: 2040, Data Step: 10200, Loss: 3.609375, Token per second per gpu: 19843.80141639711
Epoch: 0, Global Step: 2050, Data Step: 10250, Loss: 3.875, Token per second per gpu: 19956.255072563214
Epoch: 0, Global Step: 2060, Data Step: 10300, Loss: 3.140625, Token per second per gpu: 20072.727555519297
Epoch: 0, Global Step: 2070, Data Step: 10350, Loss: 3.78125, Token per second per gpu: 19861.839970655863
Epoch: 0, Global Step: 2080, Data Step: 10400, Loss: 3.546875, Token per second per gpu: 20039.75551965325
Epoch: 0, Global Step: 2090, Data Step: 10450, Loss: 3.578125, Token per second per gpu: 19815.41130689543
Epoch: 0, Global Step: 2100, Data Step: 10500, Loss: 4.3125, Token per second per gpu: 20333.205093507135
Epoch: 0, Global Step: 2110, Data Step: 10550, Loss: 3.125, Token per second per gpu: 19825.2148483432
Epoch: 0, Global Step: 2120, Data Step: 10600, Loss: 3.234375, Token per second per gpu: 19833.840818763438
Epoch: 0, Global Step: 2130, Data Step: 10650, Loss: 3.546875, Token per second per gpu: 20082.26457699925
Epoch: 0, Global Step: 2140, Data Step: 10700, Loss: 3.75, Token per second per gpu: 20223.982742873613
Epoch: 0, Global Step: 2150, Data Step: 10750, Loss: 1.15625, Token per second per gpu: 19850.40021808153
Epoch: 0, Global Step: 2160, Data Step: 10800, Loss: 3.015625, Token per second per gpu: 19766.236658079848
Epoch: 0, Global Step: 2170, Data Step: 10850, Loss: 3.6875, Token per second per gpu: 19975.17244355843
Epoch: 0, Global Step: 2180, Data Step: 10900, Loss: 3.515625, Token per second per gpu: 19684.681974239684
Epoch: 0, Global Step: 2190, Data Step: 10950, Loss: 3.671875, Token per second per gpu: 19755.821595081452
Epoch: 0, Global Step: 2200, Data Step: 11000, Loss: 3.734375, Token per second per gpu: 19945.16276523774
Epoch: 0, Global Step: 2210, Data Step: 11050, Loss: 3.046875, Token per second per gpu: 19702.64503165512
Epoch: 0, Global Step: 2220, Data Step: 11100, Loss: 3.265625, Token per second per gpu: 19865.54442029222
Epoch: 0, Global Step: 2230, Data Step: 11150, Loss: 3.953125, Token per second per gpu: 19983.48219318596
Epoch: 0, Global Step: 2240, Data Step: 11200, Loss: 3.796875, Token per second per gpu: 19733.455707726138
Epoch: 0, Global Step: 2250, Data Step: 11250, Loss: 1.90625, Token per second per gpu: 19791.254432894588
Epoch: 0, Global Step: 2260, Data Step: 11300, Loss: 3.71875, Token per second per gpu: 19673.876992121182
Epoch: 0, Global Step: 2270, Data Step: 11350, Loss: 3.796875, Token per second per gpu: 19873.218616129197
Epoch: 0, Global Step: 2280, Data Step: 11400, Loss: 3.78125, Token per second per gpu: 20021.342475404304
Epoch: 0, Global Step: 2290, Data Step: 11450, Loss: 3.46875, Token per second per gpu: 19744.465848415446
Epoch: 0, Global Step: 2300, Data Step: 11500, Loss: 3.90625, Token per second per gpu: 19957.30848753573
Epoch: 0, Global Step: 2310, Data Step: 11550, Loss: 3.5625, Token per second per gpu: 20072.77783824323
Epoch: 0, Global Step: 2320, Data Step: 11600, Loss: 1.921875, Token per second per gpu: 19878.421886954337
Epoch: 0, Global Step: 2330, Data Step: 11650, Loss: 3.9375, Token per second per gpu: 19978.760188059106
Epoch: 0, Global Step: 2340, Data Step: 11700, Loss: 1.7578125, Token per second per gpu: 19879.920551549014
Epoch: 0, Global Step: 2350, Data Step: 11750, Loss: 3.703125, Token per second per gpu: 19709.16980811199
Epoch: 0, Global Step: 2360, Data Step: 11800, Loss: 3.875, Token per second per gpu: 19625.444134722064
Epoch: 0, Global Step: 2370, Data Step: 11850, Loss: 3.578125, Token per second per gpu: 19782.849519759497
Epoch: 0, Global Step: 2380, Data Step: 11900, Loss: 3.59375, Token per second per gpu: 19813.735334679466
Epoch: 0, Global Step: 2390, Data Step: 11950, Loss: 3.421875, Token per second per gpu: 20090.57647398876
Epoch: 0, Global Step: 2400, Data Step: 12000, Loss: 3.328125, Token per second per gpu: 20139.94312511781
Epoch: 0, Global Step: 2410, Data Step: 12050, Loss: 3.171875, Token per second per gpu: 19688.70186114705
Epoch: 0, Global Step: 2420, Data Step: 12100, Loss: 3.328125, Token per second per gpu: 19814.33735032138
Epoch: 0, Global Step: 2430, Data Step: 12150, Loss: 4.25, Token per second per gpu: 19840.305050962532
Epoch: 0, Global Step: 2440, Data Step: 12200, Loss: 3.28125, Token per second per gpu: 19872.87323818928
Epoch: 0, Global Step: 2450, Data Step: 12250, Loss: 2.40625, Token per second per gpu: 20094.983492231357
Epoch: 0, Global Step: 2460, Data Step: 12300, Loss: 2.640625, Token per second per gpu: 19625.100679360818
Epoch: 0, Global Step: 2470, Data Step: 12350, Loss: 2.4375, Token per second per gpu: 19963.06528708301
Epoch: 0, Global Step: 2480, Data Step: 12400, Loss: 3.40625, Token per second per gpu: 19734.612678971785
Epoch: 0, Global Step: 2490, Data Step: 12450, Loss: 3.765625, Token per second per gpu: 19864.3814175275
Epoch: 0, Global Step: 2500, Data Step: 12500, Loss: 3.4375, Token per second per gpu: 20216.700656400553
Epoch: 0, Global Step: 2510, Data Step: 12550, Loss: 2.359375, Token per second per gpu: 19545.920178368644
Epoch: 0, Global Step: 2520, Data Step: 12600, Loss: 2.890625, Token per second per gpu: 20107.79327446825
Epoch: 0, Global Step: 2530, Data Step: 12650, Loss: 2.375, Token per second per gpu: 19833.78073511321
Epoch: 0, Global Step: 2540, Data Step: 12700, Loss: 3.171875, Token per second per gpu: 19616.41624190979
Epoch: 0, Global Step: 2550, Data Step: 12750, Loss: 3.484375, Token per second per gpu: 19703.877939882263
Epoch: 0, Global Step: 2560, Data Step: 12800, Loss: 3.296875, Token per second per gpu: 19737.981724851892
Epoch: 0, Global Step: 2570, Data Step: 12850, Loss: 2.640625, Token per second per gpu: 19775.517555249673
Epoch: 0, Global Step: 2580, Data Step: 12900, Loss: 3.828125, Token per second per gpu: 20171.13620963308
Epoch: 0, Global Step: 2590, Data Step: 12950, Loss: 3.625, Token per second per gpu: 19922.184495898982
Epoch: 0, Global Step: 2600, Data Step: 13000, Loss: 1.3984375, Token per second per gpu: 21062.368949506472
Epoch: 0, Global Step: 2610, Data Step: 13050, Loss: 3.203125, Token per second per gpu: 20515.59084525174
Epoch: 0, Global Step: 2620, Data Step: 13100, Loss: 2.71875, Token per second per gpu: 19741.0468614838
Epoch: 0, Global Step: 2630, Data Step: 13150, Loss: 3.921875, Token per second per gpu: 19759.747866089703
Epoch: 0, Global Step: 2640, Data Step: 13200, Loss: 1.8515625, Token per second per gpu: 20058.519277145053
Epoch: 0, Global Step: 2650, Data Step: 13250, Loss: 2.125, Token per second per gpu: 19919.963786667853
Epoch: 0, Global Step: 2660, Data Step: 13300, Loss: 2.90625, Token per second per gpu: 20259.276066733535
Epoch: 0, Global Step: 2670, Data Step: 13350, Loss: 3.703125, Token per second per gpu: 20076.83130423193
Epoch: 0, Global Step: 2680, Data Step: 13400, Loss: 2.625, Token per second per gpu: 19746.67320045151
Epoch: 0, Global Step: 2690, Data Step: 13450, Loss: 3.84375, Token per second per gpu: 19960.420049255536
Epoch: 0, Global Step: 2700, Data Step: 13500, Loss: 3.515625, Token per second per gpu: 19934.884031885922
Epoch: 0, Global Step: 2710, Data Step: 13550, Loss: 3.765625, Token per second per gpu: 20043.377727707477
Epoch: 0, Global Step: 2720, Data Step: 13600, Loss: 3.15625, Token per second per gpu: 19837.6488130594
Epoch: 0, Global Step: 2730, Data Step: 13650, Loss: 2.984375, Token per second per gpu: 19800.445104486844
Epoch: 0, Global Step: 2740, Data Step: 13700, Loss: 3.53125, Token per second per gpu: 19878.28498681627
Epoch: 0, Global Step: 2750, Data Step: 13750, Loss: 2.046875, Token per second per gpu: 19923.067595006734
Epoch: 0, Global Step: 2760, Data Step: 13800, Loss: 3.109375, Token per second per gpu: 19793.66254360538
Epoch: 0, Global Step: 2770, Data Step: 13850, Loss: 3.09375, Token per second per gpu: 19781.019070963524
Epoch: 0, Global Step: 2780, Data Step: 13900, Loss: 3.953125, Token per second per gpu: 19746.950833782383
Epoch: 0, Global Step: 2790, Data Step: 13950, Loss: 4.0625, Token per second per gpu: 20207.920505585225
Epoch: 0, Global Step: 2800, Data Step: 14000, Loss: 2.84375, Token per second per gpu: 19728.41268034249
Epoch: 0, Global Step: 2810, Data Step: 14050, Loss: 3.546875, Token per second per gpu: 19651.84416140042
Epoch: 0, Global Step: 2820, Data Step: 14100, Loss: 3.046875, Token per second per gpu: 19721.161390654874
Epoch: 0, Global Step: 2830, Data Step: 14150, Loss: 3.25, Token per second per gpu: 19814.548695189347
Epoch: 0, Global Step: 2840, Data Step: 14200, Loss: 3.484375, Token per second per gpu: 19678.98017552391
Epoch: 0, Global Step: 2850, Data Step: 14250, Loss: 3.125, Token per second per gpu: 19939.9204276444
Epoch: 0, Global Step: 2860, Data Step: 14300, Loss: 2.25, Token per second per gpu: 19934.35904862025
Epoch: 0, Global Step: 2870, Data Step: 14350, Loss: 3.546875, Token per second per gpu: 19954.293383589113
Epoch: 0, Global Step: 2880, Data Step: 14400, Loss: 3.8125, Token per second per gpu: 19532.425624209005
Epoch: 0, Global Step: 2890, Data Step: 14450, Loss: 3.65625, Token per second per gpu: 19575.821114410803
Epoch: 0, Global Step: 2900, Data Step: 14500, Loss: 3.78125, Token per second per gpu: 19696.48406873411
Epoch: 0, Global Step: 2910, Data Step: 14550, Loss: 3.046875, Token per second per gpu: 19806.516922171297
Epoch: 0, Global Step: 2920, Data Step: 14600, Loss: 4.28125, Token per second per gpu: 19828.013666817547
Epoch: 0, Global Step: 2930, Data Step: 14650, Loss: 3.5625, Token per second per gpu: 19562.98293606617
Epoch: 0, Global Step: 2940, Data Step: 14700, Loss: 3.71875, Token per second per gpu: 19775.1772037425
Epoch: 0, Global Step: 2950, Data Step: 14750, Loss: 3.3125, Token per second per gpu: 19826.188396089954
Epoch: 0, Global Step: 2960, Data Step: 14800, Loss: 2.859375, Token per second per gpu: 19691.94961386383
Epoch: 0, Global Step: 2970, Data Step: 14850, Loss: 3.90625, Token per second per gpu: 19576.360218709946
Epoch: 0, Global Step: 2980, Data Step: 14900, Loss: 3.21875, Token per second per gpu: 19791.03081716899
Epoch: 0, Global Step: 2990, Data Step: 14950, Loss: 3.28125, Token per second per gpu: 19649.821214088384
Epoch: 0, Global Step: 3000, Data Step: 15000, Loss: 3.15625, Token per second per gpu: 20076.158428169638
Epoch: 0, Global Step: 3010, Data Step: 15050, Loss: 2.671875, Token per second per gpu: 19624.481413989608
Epoch: 0, Global Step: 3020, Data Step: 15100, Loss: 3.484375, Token per second per gpu: 19707.535450822506
Epoch: 0, Global Step: 3030, Data Step: 15150, Loss: 2.921875, Token per second per gpu: 19741.956624361716
Epoch: 0, Global Step: 3040, Data Step: 15200, Loss: 3.234375, Token per second per gpu: 19809.20649083134
Epoch: 0, Global Step: 3050, Data Step: 15250, Loss: 3.3125, Token per second per gpu: 19929.65708311531
Epoch: 0, Global Step: 3060, Data Step: 15300, Loss: 3.765625, Token per second per gpu: 19712.86493208474
Epoch: 0, Global Step: 3070, Data Step: 15350, Loss: 2.171875, Token per second per gpu: 19807.94885619685
Epoch: 0, Global Step: 3080, Data Step: 15400, Loss: 3.140625, Token per second per gpu: 20151.310308228625
Epoch: 0, Global Step: 3090, Data Step: 15450, Loss: 3.8125, Token per second per gpu: 19689.206944611746
Epoch: 0, Global Step: 3100, Data Step: 15500, Loss: 3.3125, Token per second per gpu: 19741.745010352017
Epoch: 0, Global Step: 3110, Data Step: 15550, Loss: 1.046875, Token per second per gpu: 19875.441055360545
Epoch: 0, Global Step: 3120, Data Step: 15600, Loss: 3.15625, Token per second per gpu: 19724.749409731805
Epoch: 0, Global Step: 3130, Data Step: 15650, Loss: 3.734375, Token per second per gpu: 19812.876524788906
Epoch: 0, Global Step: 3140, Data Step: 15700, Loss: 2.546875, Token per second per gpu: 19723.16155750846
Epoch: 0, Global Step: 3150, Data Step: 15750, Loss: 2.984375, Token per second per gpu: 19992.57683701527
Epoch: 0, Global Step: 3160, Data Step: 15800, Loss: 3.5, Token per second per gpu: 19699.01282656293
Epoch: 0, Global Step: 3170, Data Step: 15850, Loss: 3.171875, Token per second per gpu: 19665.267114739032
Epoch: 0, Global Step: 3180, Data Step: 15900, Loss: 3.546875, Token per second per gpu: 19729.025291085036
Epoch: 0, Global Step: 3190, Data Step: 15950, Loss: 3.609375, Token per second per gpu: 20074.47934396532
Epoch: 0, Global Step: 3200, Data Step: 16000, Loss: 2.96875, Token per second per gpu: 20193.93593161509
Epoch: 0, Global Step: 3210, Data Step: 16050, Loss: 3.21875, Token per second per gpu: 20095.38514925974
Epoch: 0, Global Step: 3220, Data Step: 16100, Loss: 3.234375, Token per second per gpu: 20030.508987363737
Epoch: 0, Global Step: 3230, Data Step: 16150, Loss: 3.328125, Token per second per gpu: 19999.051660565336
Epoch: 0, Global Step: 3240, Data Step: 16200, Loss: 3.9375, Token per second per gpu: 19889.86209945646
Epoch: 0, Global Step: 3250, Data Step: 16250, Loss: 4.125, Token per second per gpu: 19832.067038899775
Epoch: 0, Global Step: 3260, Data Step: 16300, Loss: 3.34375, Token per second per gpu: 19851.481757772937
Epoch: 0, Global Step: 3270, Data Step: 16350, Loss: 3.328125, Token per second per gpu: 20099.233295687816
Epoch: 0, Global Step: 3280, Data Step: 16400, Loss: 1.859375, Token per second per gpu: 19834.715186976566
Epoch: 0, Global Step: 3290, Data Step: 16450, Loss: 3.90625, Token per second per gpu: 19776.604973964073
Epoch: 0, Global Step: 3300, Data Step: 16500, Loss: 3.90625, Token per second per gpu: 19881.182188624778
Epoch: 0, Global Step: 3310, Data Step: 16550, Loss: 3.234375, Token per second per gpu: 19805.105286862556
Epoch: 0, Global Step: 3320, Data Step: 16600, Loss: 3.46875, Token per second per gpu: 19995.341194347817
Epoch: 0, Global Step: 3330, Data Step: 16650, Loss: 3.09375, Token per second per gpu: 19821.486636662692
Epoch: 0, Global Step: 3340, Data Step: 16700, Loss: 3.4375, Token per second per gpu: 19951.382250184284
Epoch: 0, Global Step: 3350, Data Step: 16750, Loss: 3.109375, Token per second per gpu: 19868.50298243535
Epoch: 0, Global Step: 3360, Data Step: 16800, Loss: 3.0625, Token per second per gpu: 20071.597574031508
Epoch: 0, Global Step: 3370, Data Step: 16850, Loss: 3.34375, Token per second per gpu: 20079.082059845452
Epoch: 0, Global Step: 3380, Data Step: 16900, Loss: 3.265625, Token per second per gpu: 19764.581181148096
Epoch: 0, Global Step: 3390, Data Step: 16950, Loss: 1.5, Token per second per gpu: 20011.817340692793
Epoch: 0, Global Step: 3400, Data Step: 17000, Loss: 3.8125, Token per second per gpu: 20127.237425418527
Epoch: 0, Global Step: 3410, Data Step: 17050, Loss: 2.96875, Token per second per gpu: 20000.777386541897
Epoch: 0, Global Step: 3420, Data Step: 17100, Loss: 3.734375, Token per second per gpu: 20153.46412485269
Epoch: 0, Global Step: 3430, Data Step: 17150, Loss: 3.375, Token per second per gpu: 19864.157616219294
Epoch: 0, Global Step: 3440, Data Step: 17200, Loss: 3.453125, Token per second per gpu: 19793.463502469484
Epoch: 0, Global Step: 3450, Data Step: 17250, Loss: 3.734375, Token per second per gpu: 19891.29247618707
Epoch: 0, Global Step: 3460, Data Step: 17300, Loss: 3.640625, Token per second per gpu: 20255.526501924298
Epoch: 0, Global Step: 3470, Data Step: 17350, Loss: 1.2109375, Token per second per gpu: 19991.156422220236
Epoch: 0, Global Step: 3480, Data Step: 17400, Loss: 3.53125, Token per second per gpu: 19939.801008463
Epoch: 0, Global Step: 3490, Data Step: 17450, Loss: 2.5, Token per second per gpu: 19889.631460336943
Epoch: 0, Global Step: 3500, Data Step: 17500, Loss: 3.40625, Token per second per gpu: 19953.835050373527
Epoch: 0, Global Step: 3510, Data Step: 17550, Loss: 3.859375, Token per second per gpu: 20464.848327302534
Epoch: 0, Global Step: 3520, Data Step: 17600, Loss: 2.984375, Token per second per gpu: 20027.46873818889
Epoch: 0, Global Step: 3530, Data Step: 17650, Loss: 3.8125, Token per second per gpu: 19949.59794867336
Epoch: 0, Global Step: 3540, Data Step: 17700, Loss: 2.640625, Token per second per gpu: 19888.04329049223
Epoch: 0, Global Step: 3550, Data Step: 17750, Loss: 3.328125, Token per second per gpu: 19806.267935127213
Epoch: 0, Global Step: 3560, Data Step: 17800, Loss: 3.984375, Token per second per gpu: 20191.06589631828
Epoch: 0, Global Step: 3570, Data Step: 17850, Loss: 3.640625, Token per second per gpu: 20027.459399367675
Epoch: 0, Global Step: 3580, Data Step: 17900, Loss: 2.8125, Token per second per gpu: 19919.33279420093
Epoch: 0, Global Step: 3590, Data Step: 17950, Loss: 4.0625, Token per second per gpu: 20238.46767989642
Epoch: 0, Global Step: 3600, Data Step: 18000, Loss: 3.390625, Token per second per gpu: 20018.53938739839
Epoch: 0, Global Step: 3610, Data Step: 18050, Loss: 3.40625, Token per second per gpu: 20028.709197827815
Epoch: 0, Global Step: 3620, Data Step: 18100, Loss: 2.25, Token per second per gpu: 19772.28132100064
Epoch: 0, Global Step: 3630, Data Step: 18150, Loss: 3.625, Token per second per gpu: 19987.41019046067
Epoch: 0, Global Step: 3640, Data Step: 18200, Loss: 3.59375, Token per second per gpu: 20091.57663474522
Epoch: 0, Global Step: 3650, Data Step: 18250, Loss: 3.1875, Token per second per gpu: 20098.297078677406
Epoch: 0, Global Step: 3660, Data Step: 18300, Loss: 3.859375, Token per second per gpu: 19907.977516894607
Epoch: 0, Global Step: 3670, Data Step: 18350, Loss: 4.125, Token per second per gpu: 20088.093513439268
Epoch: 0, Global Step: 3680, Data Step: 18400, Loss: 3.40625, Token per second per gpu: 19704.939413066546
Epoch: 0, Global Step: 3690, Data Step: 18450, Loss: 3.546875, Token per second per gpu: 20046.75890075638
Epoch: 0, Global Step: 3700, Data Step: 18500, Loss: 3.828125, Token per second per gpu: 19983.600834439636
Epoch: 0, Global Step: 3710, Data Step: 18550, Loss: 2.828125, Token per second per gpu: 19754.82659623917
Epoch: 0, Global Step: 3720, Data Step: 18600, Loss: 3.234375, Token per second per gpu: 20009.521790963765
Epoch: 0, Global Step: 3730, Data Step: 18650, Loss: 2.65625, Token per second per gpu: 20209.797343925864
Epoch: 0, Global Step: 3740, Data Step: 18700, Loss: 2.859375, Token per second per gpu: 19800.11740383457
Epoch: 0, Global Step: 3750, Data Step: 18750, Loss: 3.96875, Token per second per gpu: 19789.757069966432
Epoch: 0, Global Step: 3760, Data Step: 18800, Loss: 3.109375, Token per second per gpu: 19917.488271246148
Epoch: 0, Global Step: 3770, Data Step: 18850, Loss: 3.84375, Token per second per gpu: 19722.298626749078
Epoch: 0, Global Step: 3780, Data Step: 18900, Loss: 3.375, Token per second per gpu: 20146.090598602088
Epoch: 0, Global Step: 3790, Data Step: 18950, Loss: 3.28125, Token per second per gpu: 19962.70044900011
Epoch: 0, Global Step: 3800, Data Step: 19000, Loss: 2.859375, Token per second per gpu: 19544.42875650171
Epoch: 0, Global Step: 3810, Data Step: 19050, Loss: 3.265625, Token per second per gpu: 19746.633980152474
Epoch: 0, Global Step: 3820, Data Step: 19100, Loss: 4.28125, Token per second per gpu: 19810.303100222114
Epoch: 0, Global Step: 3830, Data Step: 19150, Loss: 2.359375, Token per second per gpu: 19970.499121579633
Epoch: 0, Global Step: 3840, Data Step: 19200, Loss: 2.390625, Token per second per gpu: 19899.7064578667
Epoch: 0, Global Step: 3850, Data Step: 19250, Loss: 3.1875, Token per second per gpu: 19836.525170769997
Epoch: 0, Global Step: 3860, Data Step: 19300, Loss: 2.875, Token per second per gpu: 20175.503773716966
Epoch: 0, Global Step: 3870, Data Step: 19350, Loss: 3.59375, Token per second per gpu: 19783.498868356823
Epoch: 0, Global Step: 3880, Data Step: 19400, Loss: 2.8125, Token per second per gpu: 19755.966446192695
Epoch: 0, Global Step: 3890, Data Step: 19450, Loss: 2.5, Token per second per gpu: 19944.688365377362
Epoch: 0, Global Step: 3900, Data Step: 19500, Loss: 1.53125, Token per second per gpu: 19880.483161167347
Epoch: 0, Global Step: 3910, Data Step: 19550, Loss: 3.28125, Token per second per gpu: 20224.13663562771
Epoch: 0, Global Step: 3920, Data Step: 19600, Loss: 2.671875, Token per second per gpu: 19822.67389811932
Epoch: 0, Global Step: 3930, Data Step: 19650, Loss: 3.109375, Token per second per gpu: 20069.960142764998
Epoch: 0, Global Step: 3940, Data Step: 19700, Loss: 2.4375, Token per second per gpu: 20377.87469949404
Epoch: 0, Global Step: 3950, Data Step: 19750, Loss: 2.9375, Token per second per gpu: 19981.993346629453
Epoch: 0, Global Step: 3960, Data Step: 19800, Loss: 3.59375, Token per second per gpu: 19998.2832355839
Epoch: 0, Global Step: 3970, Data Step: 19850, Loss: 3.09375, Token per second per gpu: 19821.461937865173
Epoch: 0, Global Step: 3980, Data Step: 19900, Loss: 2.296875, Token per second per gpu: 20219.45957073445
Epoch: 0, Global Step: 3990, Data Step: 19950, Loss: 3.390625, Token per second per gpu: 19785.128350582872
Epoch: 0, Global Step: 4000, Data Step: 20000, Loss: 3.046875, Token per second per gpu: 19742.253000493667
Epoch: 0, Global Step: 4010, Data Step: 20050, Loss: 3.40625, Token per second per gpu: 19781.55696362625
Epoch: 0, Global Step: 4020, Data Step: 20100, Loss: 2.828125, Token per second per gpu: 19913.709396483526
Epoch: 0, Global Step: 4030, Data Step: 20150, Loss: 2.328125, Token per second per gpu: 19835.125743462657
Epoch: 0, Global Step: 4040, Data Step: 20200, Loss: 3.5625, Token per second per gpu: 19718.779216555136
Epoch: 0, Global Step: 4050, Data Step: 20250, Loss: 3.578125, Token per second per gpu: 19870.95990005132
Epoch: 0, Global Step: 4060, Data Step: 20300, Loss: 2.171875, Token per second per gpu: 20182.07848797254
Epoch: 0, Global Step: 4070, Data Step: 20350, Loss: 2.734375, Token per second per gpu: 19870.746614637454
Epoch: 0, Global Step: 4080, Data Step: 20400, Loss: 1.8671875, Token per second per gpu: 19894.005672285082
Epoch: 0, Global Step: 4090, Data Step: 20450, Loss: 2.84375, Token per second per gpu: 19960.537489177077
Epoch: 0, Global Step: 4100, Data Step: 20500, Loss: 3.171875, Token per second per gpu: 19724.304821571615
Epoch: 0, Global Step: 4110, Data Step: 20550, Loss: 2.859375, Token per second per gpu: 19932.701378506
Epoch: 0, Global Step: 4120, Data Step: 20600, Loss: 2.453125, Token per second per gpu: 19897.978032224073
Epoch: 0, Global Step: 4130, Data Step: 20650, Loss: 3.0, Token per second per gpu: 19683.089561186673
Epoch: 0, Global Step: 4140, Data Step: 20700, Loss: 3.6875, Token per second per gpu: 20144.6382721005
Epoch: 0, Global Step: 4150, Data Step: 20750, Loss: 3.234375, Token per second per gpu: 19841.545532069395
Epoch: 0, Global Step: 4160, Data Step: 20800, Loss: 2.046875, Token per second per gpu: 19857.78231628781
Epoch: 0, Global Step: 4170, Data Step: 20850, Loss: 2.734375, Token per second per gpu: 19760.702629867268
Epoch: 0, Global Step: 4180, Data Step: 20900, Loss: 3.671875, Token per second per gpu: 19710.475716206125
Epoch: 0, Global Step: 4190, Data Step: 20950, Loss: 2.953125, Token per second per gpu: 20175.10288793648
Epoch: 0, Global Step: 4200, Data Step: 21000, Loss: 3.6875, Token per second per gpu: 19614.548740830458
Epoch: 0, Global Step: 4210, Data Step: 21050, Loss: 3.328125, Token per second per gpu: 20127.77959723252
Epoch: 0, Global Step: 4220, Data Step: 21100, Loss: 3.21875, Token per second per gpu: 19824.71978318358
Epoch: 0, Global Step: 4230, Data Step: 21150, Loss: 3.3125, Token per second per gpu: 20220.64567466126
Epoch: 0, Global Step: 4240, Data Step: 21200, Loss: 3.125, Token per second per gpu: 20040.88098349205
Epoch: 0, Global Step: 4250, Data Step: 21250, Loss: 2.53125, Token per second per gpu: 19913.240000062684
Epoch: 0, Global Step: 4260, Data Step: 21300, Loss: 2.0, Token per second per gpu: 20274.0891551811
Epoch: 0, Global Step: 4270, Data Step: 21350, Loss: 2.734375, Token per second per gpu: 20008.36498576735
Epoch: 0, Global Step: 4280, Data Step: 21400, Loss: 2.90625, Token per second per gpu: 19865.38674816813
Epoch: 0, Global Step: 4290, Data Step: 21450, Loss: 2.53125, Token per second per gpu: 20174.811758178384
Epoch: 0, Global Step: 4300, Data Step: 21500, Loss: 3.078125, Token per second per gpu: 20049.37091783881
Epoch: 0, Global Step: 4310, Data Step: 21550, Loss: 2.40625, Token per second per gpu: 19916.058927017333
Epoch: 0, Global Step: 4320, Data Step: 21600, Loss: 2.859375, Token per second per gpu: 19968.577523294698
Epoch: 0, Global Step: 4330, Data Step: 21650, Loss: 2.5, Token per second per gpu: 19785.653706207773
Epoch: 0, Global Step: 4340, Data Step: 21700, Loss: 2.328125, Token per second per gpu: 20201.16361846111
Epoch: 0, Global Step: 4350, Data Step: 21750, Loss: 2.15625, Token per second per gpu: 19892.58375024477
Epoch: 0, Global Step: 4360, Data Step: 21800, Loss: 2.796875, Token per second per gpu: 19966.99714289853
Epoch: 0, Global Step: 4370, Data Step: 21850, Loss: 2.328125, Token per second per gpu: 20096.004213913846
Epoch: 0, Global Step: 4380, Data Step: 21900, Loss: 2.046875, Token per second per gpu: 21086.146060893818
Epoch: 0, Global Step: 4390, Data Step: 21950, Loss: 2.59375, Token per second per gpu: 20449.318494490188
Epoch: 0, Global Step: 4400, Data Step: 22000, Loss: 2.421875, Token per second per gpu: 19769.605591201875
Epoch: 0, Global Step: 4410, Data Step: 22050, Loss: 1.515625, Token per second per gpu: 19715.507585368636
Epoch: 0, Global Step: 4420, Data Step: 22100, Loss: 2.46875, Token per second per gpu: 20096.056117786713
Epoch: 0, Global Step: 4430, Data Step: 22150, Loss: 2.6875, Token per second per gpu: 19974.643849853343
Epoch: 0, Global Step: 4440, Data Step: 22200, Loss: 2.90625, Token per second per gpu: 19840.61850206135
Epoch: 0, Global Step: 4450, Data Step: 22250, Loss: 2.015625, Token per second per gpu: 20206.870320474278
Epoch: 0, Global Step: 4460, Data Step: 22300, Loss: 2.578125, Token per second per gpu: 19759.402966496466
Epoch: 0, Global Step: 4470, Data Step: 22350, Loss: 2.171875, Token per second per gpu: 19950.643803119805
Epoch: 0, Global Step: 4480, Data Step: 22400, Loss: 2.1875, Token per second per gpu: 19643.898801436797
Epoch: 0, Global Step: 4490, Data Step: 22450, Loss: 2.640625, Token per second per gpu: 19602.22065869805
Epoch: 0, Global Step: 4500, Data Step: 22500, Loss: 2.296875, Token per second per gpu: 19833.619170517835
Epoch: 0, Global Step: 4510, Data Step: 22550, Loss: 2.21875, Token per second per gpu: 20261.964981925154
Epoch: 0, Global Step: 4520, Data Step: 22600, Loss: 2.515625, Token per second per gpu: 19937.56785555699
Epoch: 0, Global Step: 4530, Data Step: 22650, Loss: 3.109375, Token per second per gpu: 19793.88968526425
Epoch: 0, Global Step: 4540, Data Step: 22700, Loss: 1.796875, Token per second per gpu: 19929.52465508389
Epoch: 0, Global Step: 4550, Data Step: 22750, Loss: 2.875, Token per second per gpu: 20015.553513026684
Epoch: 0, Global Step: 4560, Data Step: 22800, Loss: 2.34375, Token per second per gpu: 19910.270684949308
Epoch: 0, Global Step: 4570, Data Step: 22850, Loss: 2.3125, Token per second per gpu: 20274.077288090073
Epoch: 0, Global Step: 4580, Data Step: 22900, Loss: 2.203125, Token per second per gpu: 19847.41182511054
Epoch: 0, Global Step: 4590, Data Step: 22950, Loss: 2.40625, Token per second per gpu: 19821.94586255381
Epoch: 0, Global Step: 4600, Data Step: 23000, Loss: 2.59375, Token per second per gpu: 19784.13021529996
Epoch: 0, Global Step: 4610, Data Step: 23050, Loss: 2.984375, Token per second per gpu: 20151.140503651353
Epoch: 0, Global Step: 4620, Data Step: 23100, Loss: 1.9765625, Token per second per gpu: 19953.8202179351
Epoch: 0, Global Step: 4630, Data Step: 23150, Loss: 2.265625, Token per second per gpu: 20113.048726078963
Epoch: 0, Global Step: 4640, Data Step: 23200, Loss: 2.546875, Token per second per gpu: 19886.891464243196
Epoch: 0, Global Step: 4650, Data Step: 23250, Loss: 2.015625, Token per second per gpu: 19996.371555097114
Epoch: 0, Global Step: 4660, Data Step: 23300, Loss: 3.546875, Token per second per gpu: 20163.798027269775
Epoch: 0, Global Step: 4670, Data Step: 23350, Loss: 2.546875, Token per second per gpu: 19870.794419590726
Epoch: 0, Global Step: 4680, Data Step: 23400, Loss: 2.03125, Token per second per gpu: 19881.179979929806
Epoch: 0, Global Step: 4690, Data Step: 23450, Loss: 2.390625, Token per second per gpu: 19926.417355517413
Epoch: 0, Global Step: 4700, Data Step: 23500, Loss: 2.59375, Token per second per gpu: 20172.432618043327
Epoch: 0, Global Step: 4710, Data Step: 23550, Loss: 2.34375, Token per second per gpu: 19940.85916776815
Epoch: 0, Global Step: 4720, Data Step: 23600, Loss: 2.390625, Token per second per gpu: 19850.808121228492
Epoch: 0, Global Step: 4730, Data Step: 23650, Loss: 2.53125, Token per second per gpu: 19946.59165836566
Epoch: 0, Global Step: 4740, Data Step: 23700, Loss: 2.578125, Token per second per gpu: 19980.23163624094
Epoch: 0, Global Step: 4750, Data Step: 23750, Loss: 2.484375, Token per second per gpu: 20245.576117025998
Epoch: 0, Global Step: 4760, Data Step: 23800, Loss: 2.015625, Token per second per gpu: 20133.31258891011
Epoch: 0, Global Step: 4770, Data Step: 23850, Loss: 2.34375, Token per second per gpu: 19885.495785807718
Epoch: 0, Global Step: 4780, Data Step: 23900, Loss: 2.28125, Token per second per gpu: 19798.97701709872
Epoch: 0, Global Step: 4790, Data Step: 23950, Loss: 2.671875, Token per second per gpu: 20297.677542372705
Epoch: 0, Global Step: 4800, Data Step: 24000, Loss: 1.984375, Token per second per gpu: 19994.167784866666
Epoch: 0, Global Step: 4810, Data Step: 24050, Loss: 2.609375, Token per second per gpu: 19948.69804190821
Epoch: 0, Global Step: 4820, Data Step: 24100, Loss: 2.203125, Token per second per gpu: 20380.10874851893
Epoch: 0, Global Step: 4830, Data Step: 24150, Loss: 2.0, Token per second per gpu: 20065.091821689406
Epoch: 0, Global Step: 4840, Data Step: 24200, Loss: 2.234375, Token per second per gpu: 19755.943727872946
Epoch: 0, Global Step: 4850, Data Step: 24250, Loss: 2.34375, Token per second per gpu: 19893.30058289065
Epoch: 0, Global Step: 4860, Data Step: 24300, Loss: 2.484375, Token per second per gpu: 19702.479450455634
Epoch: 0, Global Step: 4870, Data Step: 24350, Loss: 2.703125, Token per second per gpu: 19992.01233196772
Epoch: 0, Global Step: 4880, Data Step: 24400, Loss: 1.5859375, Token per second per gpu: 20110.762852790416
Epoch: 0, Global Step: 4890, Data Step: 24450, Loss: 2.078125, Token per second per gpu: 19973.44295921195
Epoch: 0, Global Step: 4900, Data Step: 24500, Loss: 2.484375, Token per second per gpu: 20065.530531948414
Epoch: 0, Global Step: 4910, Data Step: 24550, Loss: 1.984375, Token per second per gpu: 19902.714309749983
Epoch: 0, Global Step: 4920, Data Step: 24600, Loss: 3.03125, Token per second per gpu: 20402.088543743408
Epoch: 0, Global Step: 4930, Data Step: 24650, Loss: 2.296875, Token per second per gpu: 19890.72888614312
Epoch: 0, Global Step: 4940, Data Step: 24700, Loss: 2.59375, Token per second per gpu: 19805.925610996244
Epoch: 0, Global Step: 4950, Data Step: 24750, Loss: 2.59375, Token per second per gpu: 20086.21084173902
Epoch: 0, Global Step: 4960, Data Step: 24800, Loss: 2.328125, Token per second per gpu: 20025.977999045925
Epoch: 0, Global Step: 4970, Data Step: 24850, Loss: 2.625, Token per second per gpu: 20356.923023721592
Epoch: 0, Global Step: 4980, Data Step: 24900, Loss: 1.96875, Token per second per gpu: 19922.93525453184
Epoch: 0, Global Step: 4990, Data Step: 24950, Loss: 2.515625, Token per second per gpu: 19917.075037054852
Epoch: 0, Global Step: 5000, Data Step: 25000, Loss: 2.625, Token per second per gpu: 19828.02758051164
I0407 01:44:02.024643 140235438175232 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2_instruction
I0407 01:44:02.025053 140235438175232 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-07 01:44:02,025] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-07 01:44:02,030] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2_instruction/pytorch_model/mp_rank_00_model_states.pt
[2024-04-07 01:44:02,031] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-07 01:44:02,348] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-07 01:44:02,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-07 01:44:02,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-07 01:44:02,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-07 01:44:02,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-07 01:44:02,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-07 01:44:02,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-07 01:44:02,702] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-07 01:44:02,702] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-07 01:44:02,702] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 01:44:02,702] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-07 01:44:02,702] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-07 01:44:02,702] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-07 01:44:02,702] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 01:44:02,703] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-07 01:44:02,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-07 01:44:02,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-07 01:44:02,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 01:44:02,703] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-07 01:44:02,703] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-07 01:44:02,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 01:44:02,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 01:44:02,705] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-07 01:44:02,705] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-07 01:44:02,705] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0407 01:44:02.705907 140235438175232 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2_instruction/pytorch_model
I0407 01:44:02.706514 140235438175232 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2_instruction/scheduler.bin
I0407 01:44:02.706778 140235438175232 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2_instruction/sampler.bin
I0407 01:44:02.707983 140235438175232 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2_instruction/random_states_0.pkl
Epoch: 0, Global Step: 5010, Data Step: 25050, Loss: 2.625, Token per second per gpu: 19272.451630310155
Epoch: 0, Global Step: 5020, Data Step: 25100, Loss: 1.9453125, Token per second per gpu: 20194.16228823803
Epoch: 0, Global Step: 5030, Data Step: 25150, Loss: 2.734375, Token per second per gpu: 19997.573528564775
Epoch: 0, Global Step: 5040, Data Step: 25200, Loss: 2.265625, Token per second per gpu: 19842.239990573962
Epoch: 0, Global Step: 5050, Data Step: 25250, Loss: 2.515625, Token per second per gpu: 19964.521246298376
Epoch: 0, Global Step: 5060, Data Step: 25300, Loss: 2.078125, Token per second per gpu: 20114.79267541715
Epoch: 0, Global Step: 5070, Data Step: 25350, Loss: 1.90625, Token per second per gpu: 20012.36991010151
Epoch: 0, Global Step: 5080, Data Step: 25400, Loss: 1.96875, Token per second per gpu: 19804.97925778064
Epoch: 0, Global Step: 5090, Data Step: 25450, Loss: 2.296875, Token per second per gpu: 19862.483676220683
Epoch: 0, Global Step: 5100, Data Step: 25500, Loss: 2.84375, Token per second per gpu: 20032.658547409534
Epoch: 0, Global Step: 5110, Data Step: 25550, Loss: 2.234375, Token per second per gpu: 20294.2085198883
Epoch: 0, Global Step: 5120, Data Step: 25600, Loss: 1.421875, Token per second per gpu: 19864.43507179251
Epoch: 0, Global Step: 5130, Data Step: 25650, Loss: 2.296875, Token per second per gpu: 19969.796584847718
Epoch: 0, Global Step: 5140, Data Step: 25700, Loss: 1.828125, Token per second per gpu: 19859.07236517046
Epoch: 0, Global Step: 5150, Data Step: 25750, Loss: 2.53125, Token per second per gpu: 19887.423527296596
Epoch: 0, Global Step: 5160, Data Step: 25800, Loss: 2.609375, Token per second per gpu: 19929.641176862115
Epoch: 0, Global Step: 5170, Data Step: 25850, Loss: 2.390625, Token per second per gpu: 20155.24611357203
Epoch: 0, Global Step: 5180, Data Step: 25900, Loss: 2.703125, Token per second per gpu: 19842.516467519894
Epoch: 0, Global Step: 5190, Data Step: 25950, Loss: 1.734375, Token per second per gpu: 19933.88349723643
Epoch: 0, Global Step: 5200, Data Step: 26000, Loss: 2.375, Token per second per gpu: 19777.305093772815
Epoch: 0, Global Step: 5210, Data Step: 26050, Loss: 2.171875, Token per second per gpu: 20298.110749944295
Epoch: 0, Global Step: 5220, Data Step: 26100, Loss: 2.65625, Token per second per gpu: 19888.075707104268
Epoch: 0, Global Step: 5230, Data Step: 26150, Loss: 2.234375, Token per second per gpu: 19895.831470545872
Epoch: 0, Global Step: 5240, Data Step: 26200, Loss: 2.015625, Token per second per gpu: 19864.705184994862
Epoch: 0, Global Step: 5250, Data Step: 26250, Loss: 2.296875, Token per second per gpu: 20229.927790524536
Epoch: 0, Global Step: 5260, Data Step: 26300, Loss: 2.28125, Token per second per gpu: 19755.12699354667
Epoch: 0, Global Step: 5270, Data Step: 26350, Loss: 1.7578125, Token per second per gpu: 19968.136172437415
Epoch: 0, Global Step: 5280, Data Step: 26400, Loss: 2.71875, Token per second per gpu: 19841.4406708596
Epoch: 0, Global Step: 5290, Data Step: 26450, Loss: 2.015625, Token per second per gpu: 20181.78089788868
Epoch: 0, Global Step: 5300, Data Step: 26500, Loss: 2.078125, Token per second per gpu: 20103.703772632205
Epoch: 0, Global Step: 5310, Data Step: 26550, Loss: 2.421875, Token per second per gpu: 19903.476514580827
Epoch: 0, Global Step: 5320, Data Step: 26600, Loss: 1.8984375, Token per second per gpu: 19923.072955202682
Epoch: 0, Global Step: 5330, Data Step: 26650, Loss: 2.609375, Token per second per gpu: 20327.230369209963
Epoch: 0, Global Step: 5340, Data Step: 26700, Loss: 2.21875, Token per second per gpu: 19810.358290239135
Epoch: 0, Global Step: 5350, Data Step: 26750, Loss: 2.21875, Token per second per gpu: 19938.787946994005
Epoch: 0, Global Step: 5360, Data Step: 26800, Loss: 2.75, Token per second per gpu: 20055.855801604153
Epoch: 0, Global Step: 5370, Data Step: 26850, Loss: 2.171875, Token per second per gpu: 19636.978118643565
Epoch: 0, Global Step: 5380, Data Step: 26900, Loss: 2.234375, Token per second per gpu: 19933.835573277665
Epoch: 0, Global Step: 5390, Data Step: 26950, Loss: 2.0625, Token per second per gpu: 20161.739863524777
Epoch: 0, Global Step: 5400, Data Step: 27000, Loss: 2.5, Token per second per gpu: 19777.67010825306
Epoch: 0, Global Step: 5410, Data Step: 27050, Loss: 2.203125, Token per second per gpu: 19916.074257483684
Epoch: 0, Global Step: 5420, Data Step: 27100, Loss: 2.078125, Token per second per gpu: 20076.799958655032
Epoch: 0, Global Step: 5430, Data Step: 27150, Loss: 2.671875, Token per second per gpu: 19958.50855170749
Epoch: 0, Global Step: 5440, Data Step: 27200, Loss: 2.0, Token per second per gpu: 19989.48073374975
Epoch: 0, Global Step: 5450, Data Step: 27250, Loss: 2.390625, Token per second per gpu: 20149.279270039395
Epoch: 0, Global Step: 5460, Data Step: 27300, Loss: 2.171875, Token per second per gpu: 19826.932486043348
Epoch: 0, Global Step: 5470, Data Step: 27350, Loss: 2.28125, Token per second per gpu: 19758.51613711232
Epoch: 0, Global Step: 5480, Data Step: 27400, Loss: 1.8984375, Token per second per gpu: 19971.857536581745
Epoch: 0, Global Step: 5490, Data Step: 27450, Loss: 2.390625, Token per second per gpu: 19854.686148414887
Epoch: 0, Global Step: 5500, Data Step: 27500, Loss: 2.125, Token per second per gpu: 19782.374244746265
Epoch: 0, Global Step: 5510, Data Step: 27550, Loss: 2.40625, Token per second per gpu: 19780.780745761836
Epoch: 0, Global Step: 5520, Data Step: 27600, Loss: 2.0, Token per second per gpu: 19529.19777345989
Epoch: 0, Global Step: 5530, Data Step: 27650, Loss: 2.09375, Token per second per gpu: 20357.773680048784
Epoch: 0, Global Step: 5540, Data Step: 27700, Loss: 2.265625, Token per second per gpu: 19891.38828414337
Epoch: 0, Global Step: 5550, Data Step: 27750, Loss: 2.015625, Token per second per gpu: 19939.793047235096
Epoch: 0, Global Step: 5560, Data Step: 27800, Loss: 1.578125, Token per second per gpu: 19708.496389688004
Epoch: 0, Global Step: 5570, Data Step: 27850, Loss: 2.84375, Token per second per gpu: 19649.45874593317
Epoch: 0, Global Step: 5580, Data Step: 27900, Loss: 2.0, Token per second per gpu: 20506.86351088255
Epoch: 0, Global Step: 5590, Data Step: 27950, Loss: 2.125, Token per second per gpu: 19895.944649366258
Epoch: 0, Global Step: 5600, Data Step: 28000, Loss: 1.984375, Token per second per gpu: 19835.643314343895
Epoch: 0, Global Step: 5610, Data Step: 28050, Loss: 1.5390625, Token per second per gpu: 19890.249150446027
Epoch: 0, Global Step: 5620, Data Step: 28100, Loss: 2.4375, Token per second per gpu: 19877.351760847803
Epoch: 0, Global Step: 5630, Data Step: 28150, Loss: 2.40625, Token per second per gpu: 20321.449530427577
Epoch: 0, Global Step: 5640, Data Step: 28200, Loss: 2.453125, Token per second per gpu: 19863.588948390283
Epoch: 0, Global Step: 5650, Data Step: 28250, Loss: 1.9375, Token per second per gpu: 19807.73162297764
Epoch: 0, Global Step: 5660, Data Step: 28300, Loss: 2.1875, Token per second per gpu: 19943.61164849893
Epoch: 0, Global Step: 5670, Data Step: 28350, Loss: 1.875, Token per second per gpu: 19975.076941605752
Epoch: 0, Global Step: 5680, Data Step: 28400, Loss: 2.53125, Token per second per gpu: 19860.65866639642
Epoch: 0, Global Step: 5690, Data Step: 28450, Loss: 2.5, Token per second per gpu: 19689.164341860625
Epoch: 0, Global Step: 5700, Data Step: 28500, Loss: 2.1875, Token per second per gpu: 19871.32709264966
Epoch: 0, Global Step: 5710, Data Step: 28550, Loss: 1.6640625, Token per second per gpu: 19705.591070699676
Epoch: 0, Global Step: 5720, Data Step: 28600, Loss: 2.09375, Token per second per gpu: 20249.098986024896
Epoch: 0, Global Step: 5730, Data Step: 28650, Loss: 2.40625, Token per second per gpu: 19874.494117391845
Epoch: 0, Global Step: 5740, Data Step: 28700, Loss: 1.765625, Token per second per gpu: 19985.466923111708
Epoch: 0, Global Step: 5750, Data Step: 28750, Loss: 1.8984375, Token per second per gpu: 19840.632250143284
Epoch: 0, Global Step: 5760, Data Step: 28800, Loss: 1.984375, Token per second per gpu: 19938.310518223614
Epoch: 0, Global Step: 5770, Data Step: 28850, Loss: 2.046875, Token per second per gpu: 19930.887305264583
Epoch: 0, Global Step: 5780, Data Step: 28900, Loss: 2.0625, Token per second per gpu: 20425.258042643316
Epoch: 0, Global Step: 5790, Data Step: 28950, Loss: 2.21875, Token per second per gpu: 19843.142235904223
Epoch: 0, Global Step: 5800, Data Step: 29000, Loss: 2.5625, Token per second per gpu: 20072.767331384573
Epoch: 0, Global Step: 5810, Data Step: 29050, Loss: 2.21875, Token per second per gpu: 19911.452359997016
Epoch: 0, Global Step: 5820, Data Step: 29100, Loss: 2.71875, Token per second per gpu: 19954.214953717994
Epoch: 0, Global Step: 5830, Data Step: 29150, Loss: 2.140625, Token per second per gpu: 20050.502142156874
Epoch: 0, Global Step: 5840, Data Step: 29200, Loss: 1.6953125, Token per second per gpu: 20086.735211142004
Epoch: 0, Global Step: 5850, Data Step: 29250, Loss: 2.453125, Token per second per gpu: 20057.38021664676
Epoch: 0, Global Step: 5860, Data Step: 29300, Loss: 2.4375, Token per second per gpu: 20144.47840614135
Epoch: 0, Global Step: 5870, Data Step: 29350, Loss: 1.671875, Token per second per gpu: 20057.341625744397
Epoch: 0, Global Step: 5880, Data Step: 29400, Loss: 1.75, Token per second per gpu: 19815.676797252894
Epoch: 0, Global Step: 5890, Data Step: 29450, Loss: 2.0, Token per second per gpu: 19829.831033778748
Epoch: 0, Global Step: 5900, Data Step: 29500, Loss: 2.015625, Token per second per gpu: 20084.124344449425
Epoch: 0, Global Step: 5910, Data Step: 29550, Loss: 2.375, Token per second per gpu: 20047.59786873041
Epoch: 0, Global Step: 5920, Data Step: 29600, Loss: 2.140625, Token per second per gpu: 20194.806255007774
Epoch: 0, Global Step: 5930, Data Step: 29650, Loss: 2.296875, Token per second per gpu: 19729.781138420807
Epoch: 0, Global Step: 5940, Data Step: 29700, Loss: 2.203125, Token per second per gpu: 19945.11478702969
Epoch: 0, Global Step: 5950, Data Step: 29750, Loss: 1.953125, Token per second per gpu: 20184.149533138243
Epoch: 0, Global Step: 5960, Data Step: 29800, Loss: 2.25, Token per second per gpu: 19783.448930970742
Epoch: 0, Global Step: 5970, Data Step: 29850, Loss: 2.140625, Token per second per gpu: 19842.409212617546
Epoch: 0, Global Step: 5980, Data Step: 29900, Loss: 1.671875, Token per second per gpu: 20301.533125788796
Epoch: 0, Global Step: 5990, Data Step: 29950, Loss: 2.296875, Token per second per gpu: 19754.997238234955
Epoch: 0, Global Step: 6000, Data Step: 30000, Loss: 2.625, Token per second per gpu: 19949.924499992558
Epoch: 0, Global Step: 6010, Data Step: 30050, Loss: 1.90625, Token per second per gpu: 19816.954614823433
Epoch: 0, Global Step: 6020, Data Step: 30100, Loss: 1.875, Token per second per gpu: 19909.008860341248
Epoch: 0, Global Step: 6030, Data Step: 30150, Loss: 1.5859375, Token per second per gpu: 19987.27085489815
Epoch: 0, Global Step: 6040, Data Step: 30200, Loss: 1.96875, Token per second per gpu: 19828.540753159916
Epoch: 0, Global Step: 6050, Data Step: 30250, Loss: 2.234375, Token per second per gpu: 19865.74877296962
Epoch: 0, Global Step: 6060, Data Step: 30300, Loss: 1.6953125, Token per second per gpu: 19917.72805469113
Epoch: 0, Global Step: 6070, Data Step: 30350, Loss: 1.71875, Token per second per gpu: 20064.239578729794
Epoch: 0, Global Step: 6080, Data Step: 30400, Loss: 1.4609375, Token per second per gpu: 19621.83728752381
Epoch: 0, Global Step: 6090, Data Step: 30450, Loss: 1.7109375, Token per second per gpu: 19748.56630984069
Epoch: 0, Global Step: 6100, Data Step: 30500, Loss: 2.015625, Token per second per gpu: 20057.11776397308
Epoch: 0, Global Step: 6110, Data Step: 30550, Loss: 1.3984375, Token per second per gpu: 19769.015935908978
Epoch: 0, Global Step: 6120, Data Step: 30600, Loss: 1.421875, Token per second per gpu: 20022.703335783535
Epoch: 0, Global Step: 6130, Data Step: 30650, Loss: 1.7421875, Token per second per gpu: 19684.145187144026
Epoch: 0, Global Step: 6140, Data Step: 30700, Loss: 1.859375, Token per second per gpu: 19661.221329028846
Epoch: 0, Global Step: 6150, Data Step: 30750, Loss: 1.3984375, Token per second per gpu: 19773.473255025117
Epoch: 0, Global Step: 6160, Data Step: 30800, Loss: 1.296875, Token per second per gpu: 20161.279333089282
Epoch: 0, Global Step: 6170, Data Step: 30850, Loss: 1.515625, Token per second per gpu: 19857.07648717459
Epoch: 0, Global Step: 6180, Data Step: 30900, Loss: 1.296875, Token per second per gpu: 20134.922994560933
Epoch: 0, Global Step: 6190, Data Step: 30950, Loss: 1.3671875, Token per second per gpu: 20992.03540960563
Epoch: 0, Global Step: 6200, Data Step: 31000, Loss: 1.375, Token per second per gpu: 19968.228451588267
Epoch: 0, Global Step: 6210, Data Step: 31050, Loss: 1.40625, Token per second per gpu: 20315.022202525408
Epoch: 0, Global Step: 6220, Data Step: 31100, Loss: 1.6640625, Token per second per gpu: 19832.115390525814
Epoch: 0, Global Step: 6230, Data Step: 31150, Loss: 1.4375, Token per second per gpu: 20047.499240032957
Epoch: 0, Global Step: 6240, Data Step: 31200, Loss: 1.3125, Token per second per gpu: 19780.80443226034
Epoch: 0, Global Step: 6250, Data Step: 31250, Loss: 1.5234375, Token per second per gpu: 20434.027258200815
Epoch: 0, Global Step: 6260, Data Step: 31300, Loss: 1.4375, Token per second per gpu: 20089.50349361231
Epoch: 0, Global Step: 6270, Data Step: 31350, Loss: 1.296875, Token per second per gpu: 20088.93087083854
Epoch: 0, Global Step: 6280, Data Step: 31400, Loss: 1.34375, Token per second per gpu: 20033.230956757736
Epoch: 0, Global Step: 6290, Data Step: 31450, Loss: 1.359375, Token per second per gpu: 20062.879252031482
Epoch: 0, Global Step: 6300, Data Step: 31500, Loss: 1.34375, Token per second per gpu: 20204.098302949133
Epoch: 0, Global Step: 6310, Data Step: 31550, Loss: 1.296875, Token per second per gpu: 20062.490139175978
Epoch: 0, Global Step: 6320, Data Step: 31600, Loss: 1.234375, Token per second per gpu: 20111.75278158663
Epoch: 0, Global Step: 6330, Data Step: 31650, Loss: 1.203125, Token per second per gpu: 19846.8870366568
Epoch: 0, Global Step: 6340, Data Step: 31700, Loss: 1.15625, Token per second per gpu: 19841.740408205595
Epoch: 0, Global Step: 6350, Data Step: 31750, Loss: 1.3515625, Token per second per gpu: 19834.747246777573
Epoch: 0, Global Step: 6360, Data Step: 31800, Loss: 1.3671875, Token per second per gpu: 19956.244316434604
Epoch: 0, Global Step: 6370, Data Step: 31850, Loss: 1.515625, Token per second per gpu: 19915.70596340965
Epoch: 0, Global Step: 6380, Data Step: 31900, Loss: 1.2265625, Token per second per gpu: 19789.67901633442
Epoch: 0, Global Step: 6390, Data Step: 31950, Loss: 1.203125, Token per second per gpu: 19742.43431520278
Epoch: 0, Global Step: 6400, Data Step: 32000, Loss: 1.4375, Token per second per gpu: 19700.645409066536
Epoch: 0, Global Step: 6410, Data Step: 32050, Loss: 1.265625, Token per second per gpu: 19827.923045208812
Epoch: 0, Global Step: 6420, Data Step: 32100, Loss: 1.1484375, Token per second per gpu: 20036.956060612818
Epoch: 0, Global Step: 6430, Data Step: 32150, Loss: 1.3671875, Token per second per gpu: 20087.18989971547
Epoch: 0, Global Step: 6440, Data Step: 32200, Loss: 1.3359375, Token per second per gpu: 19921.75517315101
Epoch: 0, Global Step: 6450, Data Step: 32250, Loss: 1.2890625, Token per second per gpu: 19860.718913003064
Epoch: 0, Global Step: 6460, Data Step: 32300, Loss: 1.2890625, Token per second per gpu: 19875.530272310534
Epoch: 0, Global Step: 6470, Data Step: 32350, Loss: 1.2109375, Token per second per gpu: 20104.51382304682
Epoch: 0, Global Step: 6480, Data Step: 32400, Loss: 1.2578125, Token per second per gpu: 19890.570077063556
Epoch: 0, Global Step: 6490, Data Step: 32450, Loss: 1.2890625, Token per second per gpu: 19740.281622195616
Epoch: 0, Global Step: 6500, Data Step: 32500, Loss: 1.1171875, Token per second per gpu: 19685.88953644882
Epoch: 0, Global Step: 6510, Data Step: 32550, Loss: 1.3828125, Token per second per gpu: 19721.491191920075
Epoch: 0, Global Step: 6520, Data Step: 32600, Loss: 1.421875, Token per second per gpu: 19850.362603026017
Epoch: 0, Global Step: 6530, Data Step: 32650, Loss: 1.40625, Token per second per gpu: 19870.237322298814
Epoch: 0, Global Step: 6540, Data Step: 32700, Loss: 1.234375, Token per second per gpu: 20225.3518611713
Epoch: 0, Global Step: 6550, Data Step: 32750, Loss: 1.40625, Token per second per gpu: 19760.82082262177
Epoch: 0, Global Step: 6560, Data Step: 32800, Loss: 1.140625, Token per second per gpu: 19715.914669789654
Epoch: 0, Global Step: 6570, Data Step: 32850, Loss: 1.1953125, Token per second per gpu: 19586.421270546667
Epoch: 0, Global Step: 6580, Data Step: 32900, Loss: 1.1953125, Token per second per gpu: 19841.307579381773
Epoch: 0, Global Step: 6590, Data Step: 32950, Loss: 1.3046875, Token per second per gpu: 19625.127043393382
Epoch: 0, Global Step: 6600, Data Step: 33000, Loss: 1.2109375, Token per second per gpu: 20009.016546133826
Epoch: 0, Global Step: 6610, Data Step: 33050, Loss: 1.15625, Token per second per gpu: 19750.94533250797
Epoch: 0, Global Step: 6620, Data Step: 33100, Loss: 1.25, Token per second per gpu: 19713.68867184256
Epoch: 0, Global Step: 6630, Data Step: 33150, Loss: 1.140625, Token per second per gpu: 19727.878036377806
Epoch: 0, Global Step: 6640, Data Step: 33200, Loss: 1.09375, Token per second per gpu: 20124.301263243167
Epoch: 0, Global Step: 6650, Data Step: 33250, Loss: 1.2265625, Token per second per gpu: 20068.227896662127
Epoch: 0, Global Step: 6660, Data Step: 33300, Loss: 1.1328125, Token per second per gpu: 19870.464202376006
Epoch: 0, Global Step: 6670, Data Step: 33350, Loss: 1.3125, Token per second per gpu: 19848.10375919016
Epoch: 0, Global Step: 6680, Data Step: 33400, Loss: 1.046875, Token per second per gpu: 19774.353415846897
Epoch: 0, Global Step: 6690, Data Step: 33450, Loss: 1.140625, Token per second per gpu: 20117.11282094393
Epoch: 0, Global Step: 6700, Data Step: 33500, Loss: 1.3125, Token per second per gpu: 19887.81269355168
Epoch: 0, Global Step: 6710, Data Step: 33550, Loss: 1.0703125, Token per second per gpu: 19838.775331756966
Epoch: 0, Global Step: 6720, Data Step: 33600, Loss: 1.140625, Token per second per gpu: 19903.292414262265
Epoch: 0, Global Step: 6730, Data Step: 33650, Loss: 1.34375, Token per second per gpu: 20171.446559410237
Epoch: 0, Global Step: 6740, Data Step: 33700, Loss: 1.2109375, Token per second per gpu: 19762.058840310307
Epoch: 0, Global Step: 6750, Data Step: 33750, Loss: 1.0859375, Token per second per gpu: 19681.24109027556
Epoch: 0, Global Step: 6760, Data Step: 33800, Loss: 1.3515625, Token per second per gpu: 19878.760097260503
Epoch: 0, Global Step: 6770, Data Step: 33850, Loss: 1.140625, Token per second per gpu: 19887.310261229195
Epoch: 0, Global Step: 6780, Data Step: 33900, Loss: 1.2109375, Token per second per gpu: 20139.42239565463
Epoch: 0, Global Step: 6790, Data Step: 33950, Loss: 1.3515625, Token per second per gpu: 19739.741435637698
Epoch: 0, Global Step: 6800, Data Step: 34000, Loss: 1.1953125, Token per second per gpu: 19672.455906110143
Epoch: 0, Global Step: 6810, Data Step: 34050, Loss: 1.1953125, Token per second per gpu: 19702.140885334436
Epoch: 0, Global Step: 6820, Data Step: 34100, Loss: 1.3359375, Token per second per gpu: 20252.065568454724
Epoch: 0, Global Step: 6830, Data Step: 34150, Loss: 1.2265625, Token per second per gpu: 19922.27801419937
Epoch: 0, Global Step: 6840, Data Step: 34200, Loss: 1.1171875, Token per second per gpu: 19882.554067071054
Epoch: 0, Global Step: 6850, Data Step: 34250, Loss: 1.15625, Token per second per gpu: 19783.163162749304
Epoch: 0, Global Step: 6860, Data Step: 34300, Loss: 0.99609375, Token per second per gpu: 20038.511073636142
Epoch: 0, Global Step: 6870, Data Step: 34350, Loss: 1.1015625, Token per second per gpu: 19906.625747664028
Epoch: 0, Global Step: 6880, Data Step: 34400, Loss: 1.34375, Token per second per gpu: 19843.52196986142
Epoch: 0, Global Step: 6890, Data Step: 34450, Loss: 1.1015625, Token per second per gpu: 19642.389695502865
Epoch: 0, Global Step: 6900, Data Step: 34500, Loss: 1.34375, Token per second per gpu: 19688.440845324214
Epoch: 0, Global Step: 6910, Data Step: 34550, Loss: 1.2265625, Token per second per gpu: 19993.803484413467
Epoch: 0, Global Step: 6920, Data Step: 34600, Loss: 1.203125, Token per second per gpu: 19840.1653759436
Epoch: 0, Global Step: 6930, Data Step: 34650, Loss: 1.296875, Token per second per gpu: 19859.7232377834
Epoch: 0, Global Step: 6940, Data Step: 34700, Loss: 1.375, Token per second per gpu: 20093.29373753183
Epoch: 0, Global Step: 6950, Data Step: 34750, Loss: 1.21875, Token per second per gpu: 19913.57662632938
Epoch: 0, Global Step: 6960, Data Step: 34800, Loss: 1.171875, Token per second per gpu: 19860.00809980067
Epoch: 0, Global Step: 6970, Data Step: 34850, Loss: 1.2578125, Token per second per gpu: 19898.056389380807
Epoch: 0, Global Step: 6980, Data Step: 34900, Loss: 1.0625, Token per second per gpu: 20001.62648224707
Epoch: 0, Global Step: 6990, Data Step: 34950, Loss: 1.3515625, Token per second per gpu: 19903.43611552185
Epoch: 0, Global Step: 7000, Data Step: 35000, Loss: 1.1875, Token per second per gpu: 19762.205055931103
Epoch: 0, Global Step: 7010, Data Step: 35050, Loss: 1.1796875, Token per second per gpu: 19976.136618503704
Epoch: 0, Global Step: 7020, Data Step: 35100, Loss: 1.234375, Token per second per gpu: 19932.901934712867
Epoch: 0, Global Step: 7030, Data Step: 35150, Loss: 1.2734375, Token per second per gpu: 19772.84932417958
Epoch: 0, Global Step: 7040, Data Step: 35200, Loss: 1.21875, Token per second per gpu: 19920.29177072335
Epoch: 0, Global Step: 7050, Data Step: 35250, Loss: 1.171875, Token per second per gpu: 20140.78084399546
Epoch: 0, Global Step: 7060, Data Step: 35300, Loss: 1.484375, Token per second per gpu: 19842.58265445697
Epoch: 0, Global Step: 7070, Data Step: 35350, Loss: 1.15625, Token per second per gpu: 19710.22877614169
Epoch: 0, Global Step: 7080, Data Step: 35400, Loss: 1.15625, Token per second per gpu: 19779.344356761336
Epoch: 0, Global Step: 7090, Data Step: 35450, Loss: 1.125, Token per second per gpu: 19897.79679909419
Epoch: 0, Global Step: 7100, Data Step: 35500, Loss: 1.3671875, Token per second per gpu: 20167.805571745488
Epoch: 0, Global Step: 7110, Data Step: 35550, Loss: 1.4921875, Token per second per gpu: 19994.127202963424
Epoch: 0, Global Step: 7120, Data Step: 35600, Loss: 1.2265625, Token per second per gpu: 19699.99064484124
Epoch: 0, Global Step: 7130, Data Step: 35650, Loss: 1.2421875, Token per second per gpu: 19858.585891465413
Epoch: 0, Global Step: 7140, Data Step: 35700, Loss: 1.203125, Token per second per gpu: 20076.084104877064
Epoch: 0, Global Step: 7150, Data Step: 35750, Loss: 1.140625, Token per second per gpu: 19882.562534908167
Epoch: 0, Global Step: 7160, Data Step: 35800, Loss: 1.109375, Token per second per gpu: 19835.085438191363
Epoch: 0, Global Step: 7170, Data Step: 35850, Loss: 1.3828125, Token per second per gpu: 20155.662668844616
Epoch: 0, Global Step: 7180, Data Step: 35900, Loss: 1.171875, Token per second per gpu: 19763.688975901525
Epoch: 0, Global Step: 7190, Data Step: 35950, Loss: 1.125, Token per second per gpu: 19770.702006768694
Epoch: 0, Global Step: 7200, Data Step: 36000, Loss: 1.078125, Token per second per gpu: 20290.797048608107
Epoch: 0, Global Step: 7210, Data Step: 36050, Loss: 1.265625, Token per second per gpu: 19736.585645964115
Epoch: 0, Global Step: 7220, Data Step: 36100, Loss: 1.203125, Token per second per gpu: 19772.240542540785
Epoch: 0, Global Step: 7230, Data Step: 36150, Loss: 1.078125, Token per second per gpu: 20340.936083272023
Epoch: 0, Global Step: 7240, Data Step: 36200, Loss: 1.171875, Token per second per gpu: 19903.145394710275
Epoch: 0, Global Step: 7250, Data Step: 36250, Loss: 1.1328125, Token per second per gpu: 19888.981941528207
Epoch: 0, Global Step: 7260, Data Step: 36300, Loss: 1.1875, Token per second per gpu: 19669.385367373474
Epoch: 0, Global Step: 7270, Data Step: 36350, Loss: 1.2890625, Token per second per gpu: 19732.88035545147
Epoch: 0, Global Step: 7280, Data Step: 36400, Loss: 1.3359375, Token per second per gpu: 20132.07385995252
Epoch: 0, Global Step: 7290, Data Step: 36450, Loss: 1.0859375, Token per second per gpu: 19731.14054054953
Epoch: 0, Global Step: 7300, Data Step: 36500, Loss: 1.3125, Token per second per gpu: 19815.09298382768
Epoch: 0, Global Step: 7310, Data Step: 36550, Loss: 1.296875, Token per second per gpu: 20093.302761838244
Epoch: 0, Global Step: 7320, Data Step: 36600, Loss: 1.1171875, Token per second per gpu: 20056.136390461572
Epoch: 0, Global Step: 7330, Data Step: 36650, Loss: 1.3515625, Token per second per gpu: 19838.910955130046
Epoch: 0, Global Step: 7340, Data Step: 36700, Loss: 1.3671875, Token per second per gpu: 19699.653249521303
Epoch: 0, Global Step: 7350, Data Step: 36750, Loss: 1.1015625, Token per second per gpu: 20310.42883061364
Epoch: 0, Global Step: 7360, Data Step: 36800, Loss: 1.109375, Token per second per gpu: 19791.682709418405
Epoch: 0, Global Step: 7370, Data Step: 36850, Loss: 1.25, Token per second per gpu: 19824.473632446163
Epoch: 0, Global Step: 7380, Data Step: 36900, Loss: 1.0, Token per second per gpu: 20032.705265849596
Epoch: 0, Global Step: 7390, Data Step: 36950, Loss: 1.0390625, Token per second per gpu: 19885.34092711286
Epoch: 0, Global Step: 7400, Data Step: 37000, Loss: 1.15625, Token per second per gpu: 19735.76380050679
Epoch: 0, Global Step: 7410, Data Step: 37050, Loss: 1.0546875, Token per second per gpu: 19596.177228026576
Epoch: 0, Global Step: 7420, Data Step: 37100, Loss: 1.3359375, Token per second per gpu: 20096.747066619966
Epoch: 0, Global Step: 7430, Data Step: 37150, Loss: 1.3203125, Token per second per gpu: 19743.473989763104
Epoch: 0, Global Step: 7440, Data Step: 37200, Loss: 1.1484375, Token per second per gpu: 19625.85432405644
Epoch: 0, Global Step: 7450, Data Step: 37250, Loss: 1.0625, Token per second per gpu: 19837.072499865295
Epoch: 0, Global Step: 7460, Data Step: 37300, Loss: 1.25, Token per second per gpu: 20142.518081365663
Epoch: 0, Global Step: 7470, Data Step: 37350, Loss: 1.0546875, Token per second per gpu: 19560.799531338802
Epoch: 0, Global Step: 7480, Data Step: 37400, Loss: 1.140625, Token per second per gpu: 19987.18993326694
Epoch: 0, Global Step: 7490, Data Step: 37450, Loss: 1.2421875, Token per second per gpu: 20150.970891022946
Epoch: 0, Global Step: 7500, Data Step: 37500, Loss: 1.203125, Token per second per gpu: 19832.161361419214
Epoch: 0, Global Step: 7510, Data Step: 37550, Loss: 1.109375, Token per second per gpu: 19900.967841246023
Epoch: 0, Global Step: 7520, Data Step: 37600, Loss: 1.375, Token per second per gpu: 20167.754243595457
Epoch: 0, Global Step: 7530, Data Step: 37650, Loss: 1.3671875, Token per second per gpu: 19546.64782722587
Epoch: 0, Global Step: 7540, Data Step: 37700, Loss: 1.2265625, Token per second per gpu: 19790.714371249793
Epoch: 0, Global Step: 7550, Data Step: 37750, Loss: 1.0625, Token per second per gpu: 19786.82956850599
Epoch: 0, Global Step: 7560, Data Step: 37800, Loss: 1.453125, Token per second per gpu: 20156.625049246835
Epoch: 0, Global Step: 7570, Data Step: 37850, Loss: 1.09375, Token per second per gpu: 19932.41276226556
Epoch: 0, Global Step: 7580, Data Step: 37900, Loss: 1.296875, Token per second per gpu: 19872.78698753129
Epoch: 0, Global Step: 7590, Data Step: 37950, Loss: 1.1796875, Token per second per gpu: 19977.037701734036
Epoch: 0, Global Step: 7600, Data Step: 38000, Loss: 1.21875, Token per second per gpu: 20091.03246426833
Epoch: 0, Global Step: 7610, Data Step: 38050, Loss: 1.0234375, Token per second per gpu: 19860.082668512583
Epoch: 0, Global Step: 7620, Data Step: 38100, Loss: 1.390625, Token per second per gpu: 20180.79639056543
Epoch: 0, Global Step: 7630, Data Step: 38150, Loss: 1.4140625, Token per second per gpu: 19838.102557276987
Epoch: 0, Global Step: 7640, Data Step: 38200, Loss: 1.3203125, Token per second per gpu: 19916.719266682238
Epoch: 0, Global Step: 7650, Data Step: 38250, Loss: 1.078125, Token per second per gpu: 19970.306722254503
Epoch: 0, Global Step: 7660, Data Step: 38300, Loss: 1.375, Token per second per gpu: 20038.513878374484
Epoch: 0, Global Step: 7670, Data Step: 38350, Loss: 1.1640625, Token per second per gpu: 19801.785230055433
Epoch: 0, Global Step: 7680, Data Step: 38400, Loss: 1.3671875, Token per second per gpu: 19678.918141152717
Epoch: 0, Global Step: 7690, Data Step: 38450, Loss: 1.0625, Token per second per gpu: 19690.170249247454
Epoch: 0, Global Step: 7700, Data Step: 38500, Loss: 0.99609375, Token per second per gpu: 19983.729890945327
Epoch: 0, Global Step: 7710, Data Step: 38550, Loss: 1.0234375, Token per second per gpu: 19964.65562468549
Epoch: 0, Global Step: 7720, Data Step: 38600, Loss: 1.0859375, Token per second per gpu: 19730.738990756425
Epoch: 0, Global Step: 7730, Data Step: 38650, Loss: 1.1875, Token per second per gpu: 19889.358089905418
Epoch: 0, Global Step: 7740, Data Step: 38700, Loss: 1.1640625, Token per second per gpu: 20060.756183888447
Epoch: 0, Global Step: 7750, Data Step: 38750, Loss: 1.1484375, Token per second per gpu: 19622.758688112237
Epoch: 0, Global Step: 7760, Data Step: 38800, Loss: 0.98046875, Token per second per gpu: 19862.59427138811
Epoch: 0, Global Step: 7770, Data Step: 38850, Loss: 1.1171875, Token per second per gpu: 20040.87200621155
Epoch: 0, Global Step: 7780, Data Step: 38900, Loss: 1.2421875, Token per second per gpu: 19838.25264929064
Epoch: 0, Global Step: 7790, Data Step: 38950, Loss: 1.1328125, Token per second per gpu: 19933.34209942955
Epoch: 0, Global Step: 7800, Data Step: 39000, Loss: 1.1328125, Token per second per gpu: 19885.774574365252
Epoch: 0, Global Step: 7810, Data Step: 39050, Loss: 1.28125, Token per second per gpu: 19709.18717325693
Epoch: 0, Global Step: 7820, Data Step: 39100, Loss: 0.9765625, Token per second per gpu: 20031.01158134283
Epoch: 0, Global Step: 7830, Data Step: 39150, Loss: 1.046875, Token per second per gpu: 19898.44265309017
Epoch: 0, Global Step: 7840, Data Step: 39200, Loss: 1.09375, Token per second per gpu: 19818.04220615295
Epoch: 0, Global Step: 7850, Data Step: 39250, Loss: 1.1640625, Token per second per gpu: 20046.177484559426
Epoch: 0, Global Step: 7860, Data Step: 39300, Loss: 1.046875, Token per second per gpu: 19810.096780001302
Epoch: 0, Global Step: 7870, Data Step: 39350, Loss: 1.1875, Token per second per gpu: 20081.179527439257
Epoch: 0, Global Step: 7880, Data Step: 39400, Loss: 1.125, Token per second per gpu: 19764.249937138164
Epoch: 0, Global Step: 7890, Data Step: 39450, Loss: 1.15625, Token per second per gpu: 19696.762821782206
Epoch: 0, Global Step: 7900, Data Step: 39500, Loss: 1.671875, Token per second per gpu: 19723.82529030638
Epoch: 0, Global Step: 7910, Data Step: 39550, Loss: 1.4765625, Token per second per gpu: 19606.565301306495
Epoch: 0, Global Step: 7920, Data Step: 39600, Loss: 1.6015625, Token per second per gpu: 19640.307441790646
Epoch: 0, Global Step: 7930, Data Step: 39650, Loss: 1.7734375, Token per second per gpu: 20363.96179640471
Epoch: 0, Global Step: 7940, Data Step: 39700, Loss: 1.578125, Token per second per gpu: 19583.273413531893
Epoch: 0, Global Step: 7950, Data Step: 39750, Loss: 1.8359375, Token per second per gpu: 20017.151478465483
Epoch: 0, Global Step: 7960, Data Step: 39800, Loss: 1.828125, Token per second per gpu: 20295.482818658493
Epoch: 0, Global Step: 7970, Data Step: 39850, Loss: 1.53125, Token per second per gpu: 19833.28322716712
Epoch: 0, Global Step: 7980, Data Step: 39900, Loss: 1.5859375, Token per second per gpu: 19983.58930497701
Epoch: 0, Global Step: 7990, Data Step: 39950, Loss: 1.7734375, Token per second per gpu: 20314.30002149974
Epoch: 0, Global Step: 8000, Data Step: 40000, Loss: 1.8125, Token per second per gpu: 19749.116242160126
Epoch: 0, Global Step: 8010, Data Step: 40050, Loss: 1.796875, Token per second per gpu: 20064.715183613956
Epoch: 0, Global Step: 8020, Data Step: 40100, Loss: 1.6875, Token per second per gpu: 21076.135018625653
Epoch: 0, Global Step: 8030, Data Step: 40150, Loss: 1.4296875, Token per second per gpu: 21015.132556832545
Epoch: 0, Global Step: 8040, Data Step: 40200, Loss: 1.5078125, Token per second per gpu: 19965.816476833927
Epoch: 0, Global Step: 8050, Data Step: 40250, Loss: 1.4765625, Token per second per gpu: 20162.143057873895
Epoch: 0, Global Step: 8060, Data Step: 40300, Loss: 1.2421875, Token per second per gpu: 19970.138283031712
Epoch: 0, Global Step: 8070, Data Step: 40350, Loss: 1.5390625, Token per second per gpu: 19901.22124368632
Epoch: 0, Global Step: 8080, Data Step: 40400, Loss: 1.6171875, Token per second per gpu: 19760.2515102123
Epoch: 0, Global Step: 8090, Data Step: 40450, Loss: 1.4375, Token per second per gpu: 20104.515328777517
Epoch: 0, Global Step: 8100, Data Step: 40500, Loss: 1.9296875, Token per second per gpu: 19674.755879313037
Epoch: 0, Global Step: 8110, Data Step: 40550, Loss: 1.6875, Token per second per gpu: 19885.766287934774
Epoch: 0, Global Step: 8120, Data Step: 40600, Loss: 1.921875, Token per second per gpu: 19528.99407027021
Epoch: 0, Global Step: 8130, Data Step: 40650, Loss: 1.6875, Token per second per gpu: 20184.78603146151
Epoch: 0, Global Step: 8140, Data Step: 40700, Loss: 1.4140625, Token per second per gpu: 19921.61342493381
Epoch: 0, Global Step: 8150, Data Step: 40750, Loss: 1.5703125, Token per second per gpu: 20012.696095161966
Epoch: 0, Global Step: 8160, Data Step: 40800, Loss: 1.390625, Token per second per gpu: 19797.081166189433
Epoch: 0, Global Step: 8170, Data Step: 40850, Loss: 1.6015625, Token per second per gpu: 19907.167540563212
Epoch: 0, Global Step: 8180, Data Step: 40900, Loss: 1.75, Token per second per gpu: 20162.368513011323
Epoch: 0, Global Step: 8190, Data Step: 40950, Loss: 1.609375, Token per second per gpu: 19815.25552660912
Epoch: 0, Global Step: 8200, Data Step: 41000, Loss: 1.453125, Token per second per gpu: 19751.59967331387
Epoch: 0, Global Step: 8210, Data Step: 41050, Loss: 1.4609375, Token per second per gpu: 19735.988164235478
Epoch: 0, Global Step: 8220, Data Step: 41100, Loss: 1.71875, Token per second per gpu: 20043.977128691247
Epoch: 0, Global Step: 8230, Data Step: 41150, Loss: 2.34375, Token per second per gpu: 20078.80251821423
Epoch: 0, Global Step: 8240, Data Step: 41200, Loss: 1.7109375, Token per second per gpu: 20006.338801687292
Epoch: 0, Global Step: 8250, Data Step: 41250, Loss: 1.5625, Token per second per gpu: 20149.57817111597
Epoch: 0, Global Step: 8260, Data Step: 41300, Loss: 1.546875, Token per second per gpu: 19918.08386110474
Epoch: 0, Global Step: 8270, Data Step: 41350, Loss: 1.4921875, Token per second per gpu: 20057.77137941674
Epoch: 0, Global Step: 8280, Data Step: 41400, Loss: 1.265625, Token per second per gpu: 19948.000558826938
Epoch: 0, Global Step: 8290, Data Step: 41450, Loss: 1.6015625, Token per second per gpu: 20103.580689829643
Epoch: 0, Global Step: 8300, Data Step: 41500, Loss: 1.53125, Token per second per gpu: 19836.375654582287
Epoch: 0, Global Step: 8310, Data Step: 41550, Loss: 1.71875, Token per second per gpu: 19705.6605062403
Epoch: 0, Global Step: 8320, Data Step: 41600, Loss: 1.5390625, Token per second per gpu: 20175.73900536692
Epoch: 0, Global Step: 8330, Data Step: 41650, Loss: 1.53125, Token per second per gpu: 20021.299729837658
Epoch: 0, Global Step: 8340, Data Step: 41700, Loss: 1.6015625, Token per second per gpu: 19581.20545937101
Epoch: 0, Global Step: 8350, Data Step: 41750, Loss: 1.4296875, Token per second per gpu: 19827.52505223404
Epoch: 0, Global Step: 8360, Data Step: 41800, Loss: 1.6328125, Token per second per gpu: 20110.56491639115
Epoch: 0, Global Step: 8370, Data Step: 41850, Loss: 1.625, Token per second per gpu: 19979.2339796374
Epoch: 0, Global Step: 8380, Data Step: 41900, Loss: 1.5859375, Token per second per gpu: 19551.24500909133
Epoch: 0, Global Step: 8390, Data Step: 41950, Loss: 1.703125, Token per second per gpu: 19846.34430142009
Epoch: 0, Global Step: 8400, Data Step: 42000, Loss: 1.484375, Token per second per gpu: 19587.645750182306
Epoch: 0, Global Step: 8410, Data Step: 42050, Loss: 1.5234375, Token per second per gpu: 19749.674741762414
Epoch: 0, Global Step: 8420, Data Step: 42100, Loss: 1.703125, Token per second per gpu: 20266.03459687115
Epoch: 0, Global Step: 8430, Data Step: 42150, Loss: 1.3828125, Token per second per gpu: 19942.273925371625
Epoch: 0, Global Step: 8440, Data Step: 42200, Loss: 1.4609375, Token per second per gpu: 19809.461216464784
Epoch: 0, Global Step: 8450, Data Step: 42250, Loss: 1.5390625, Token per second per gpu: 20216.05757629756
Epoch: 0, Global Step: 8460, Data Step: 42300, Loss: 1.5234375, Token per second per gpu: 19757.784262857906
Epoch: 0, Global Step: 8470, Data Step: 42350, Loss: 1.65625, Token per second per gpu: 19902.523767663646
Epoch: 0, Global Step: 8480, Data Step: 42400, Loss: 1.5, Token per second per gpu: 19978.809629362768
Epoch: 0, Global Step: 8490, Data Step: 42450, Loss: 1.6875, Token per second per gpu: 20140.060420453763
Epoch: 0, Global Step: 8500, Data Step: 42500, Loss: 1.328125, Token per second per gpu: 19672.027367767976
Epoch: 0, Global Step: 8510, Data Step: 42550, Loss: 1.515625, Token per second per gpu: 19945.26298301154
Epoch: 0, Global Step: 8520, Data Step: 42600, Loss: 1.640625, Token per second per gpu: 20251.0264495979
Epoch: 0, Global Step: 8530, Data Step: 42650, Loss: 1.4375, Token per second per gpu: 19736.81909810735
Epoch: 0, Global Step: 8540, Data Step: 42700, Loss: 1.5859375, Token per second per gpu: 19788.11734362769
Epoch: 0, Global Step: 8550, Data Step: 42750, Loss: 1.4140625, Token per second per gpu: 19617.84985248498
Epoch: 0, Global Step: 8560, Data Step: 42800, Loss: 1.7734375, Token per second per gpu: 19939.181903110846
Epoch: 0, Global Step: 8570, Data Step: 42850, Loss: 1.4140625, Token per second per gpu: 19958.223268353977
Epoch: 0, Global Step: 8580, Data Step: 42900, Loss: 1.6484375, Token per second per gpu: 20003.387495839892
Epoch: 0, Global Step: 8590, Data Step: 42950, Loss: 1.4296875, Token per second per gpu: 19672.863016807183
Epoch: 0, Global Step: 8600, Data Step: 43000, Loss: 1.640625, Token per second per gpu: 20138.275261513554
Epoch: 0, Global Step: 8610, Data Step: 43050, Loss: 1.6484375, Token per second per gpu: 19728.141548798507
Epoch: 0, Global Step: 8620, Data Step: 43100, Loss: 1.7421875, Token per second per gpu: 19703.70926458765
Epoch: 0, Global Step: 8630, Data Step: 43150, Loss: 1.5546875, Token per second per gpu: 20172.358717148454
Epoch: 0, Global Step: 8640, Data Step: 43200, Loss: 2.171875, Token per second per gpu: 19533.07125133791
Epoch: 0, Global Step: 8650, Data Step: 43250, Loss: 1.6328125, Token per second per gpu: 19886.32407219012
Epoch: 0, Global Step: 8660, Data Step: 43300, Loss: 1.59375, Token per second per gpu: 19817.43612489478
Epoch: 0, Global Step: 8670, Data Step: 43350, Loss: 1.3984375, Token per second per gpu: 19991.089240408506
Epoch: 0, Global Step: 8680, Data Step: 43400, Loss: 1.7265625, Token per second per gpu: 19856.19794325208
Epoch: 0, Global Step: 8690, Data Step: 43450, Loss: 1.671875, Token per second per gpu: 19722.100293764644
Epoch: 0, Global Step: 8700, Data Step: 43500, Loss: 1.53125, Token per second per gpu: 20004.94419789088
Epoch: 0, Global Step: 8710, Data Step: 43550, Loss: 1.5390625, Token per second per gpu: 19890.727780734727
Epoch: 0, Global Step: 8720, Data Step: 43600, Loss: 1.546875, Token per second per gpu: 19756.546053636906
Epoch: 0, Global Step: 8730, Data Step: 43650, Loss: 1.2265625, Token per second per gpu: 19720.91907253763
Epoch: 0, Global Step: 8740, Data Step: 43700, Loss: 1.4296875, Token per second per gpu: 19865.063142243074
Epoch: 0, Global Step: 8750, Data Step: 43750, Loss: 1.375, Token per second per gpu: 20104.927342813895
Epoch: 0, Global Step: 8760, Data Step: 43800, Loss: 1.1171875, Token per second per gpu: 19826.693053218572
Epoch: 0, Global Step: 8770, Data Step: 43850, Loss: 1.6875, Token per second per gpu: 19931.05952258795
Epoch: 0, Global Step: 8780, Data Step: 43900, Loss: 1.5390625, Token per second per gpu: 20122.801542594323
Epoch: 0, Global Step: 8790, Data Step: 43950, Loss: 1.0703125, Token per second per gpu: 19900.861428899218
Epoch: 0, Global Step: 8800, Data Step: 44000, Loss: 1.296875, Token per second per gpu: 19936.407509622397
Epoch: 0, Global Step: 8810, Data Step: 44050, Loss: 1.765625, Token per second per gpu: 20121.6168968074
Epoch: 0, Global Step: 8820, Data Step: 44100, Loss: 1.3984375, Token per second per gpu: 19816.152576696637
Epoch: 0, Global Step: 8830, Data Step: 44150, Loss: 1.4296875, Token per second per gpu: 19905.93950669156
Epoch: 0, Global Step: 8840, Data Step: 44200, Loss: 1.3828125, Token per second per gpu: 20114.59880459832
Epoch: 0, Global Step: 8850, Data Step: 44250, Loss: 1.3203125, Token per second per gpu: 19724.010795669346
Epoch: 0, Global Step: 8860, Data Step: 44300, Loss: 1.3046875, Token per second per gpu: 19741.854628020545
Epoch: 0, Global Step: 8870, Data Step: 44350, Loss: 0.98046875, Token per second per gpu: 20050.27974344634
Epoch: 0, Global Step: 8880, Data Step: 44400, Loss: 1.171875, Token per second per gpu: 19990.81381831054
Epoch: 0, Global Step: 8890, Data Step: 44450, Loss: 1.140625, Token per second per gpu: 19793.2779654372
Epoch: 0, Global Step: 8900, Data Step: 44500, Loss: 1.3046875, Token per second per gpu: 19979.702217087448
Epoch: 0, Global Step: 8910, Data Step: 44550, Loss: 1.328125, Token per second per gpu: 19890.343474531117
Epoch: 0, Global Step: 8920, Data Step: 44600, Loss: 1.453125, Token per second per gpu: 19817.30481827084
Epoch: 0, Global Step: 8930, Data Step: 44650, Loss: 1.40625, Token per second per gpu: 19673.707027716035
Epoch: 0, Global Step: 8940, Data Step: 44700, Loss: 1.3203125, Token per second per gpu: 19964.467792415257
Epoch: 0, Global Step: 8950, Data Step: 44750, Loss: 1.0625, Token per second per gpu: 20067.694179057227
Epoch: 0, Global Step: 8960, Data Step: 44800, Loss: 1.046875, Token per second per gpu: 19917.18051451454
Epoch: 0, Global Step: 8970, Data Step: 44850, Loss: 1.28125, Token per second per gpu: 19680.62008052922
Epoch: 0, Global Step: 8980, Data Step: 44900, Loss: 1.046875, Token per second per gpu: 19800.53273658299
Epoch: 0, Global Step: 8990, Data Step: 44950, Loss: 1.203125, Token per second per gpu: 20046.046497473944
Epoch: 0, Global Step: 9000, Data Step: 45000, Loss: 1.375, Token per second per gpu: 19885.033610638835
Epoch: 0, Global Step: 9010, Data Step: 45050, Loss: 0.9765625, Token per second per gpu: 19610.882320815235
Epoch: 0, Global Step: 9020, Data Step: 45100, Loss: 1.0234375, Token per second per gpu: 20070.55306874551
Epoch: 0, Global Step: 9030, Data Step: 45150, Loss: 1.3359375, Token per second per gpu: 20027.51524564823
Epoch: 0, Global Step: 9040, Data Step: 45200, Loss: 1.0, Token per second per gpu: 19577.550778969555
Epoch: 0, Global Step: 9050, Data Step: 45250, Loss: 1.3515625, Token per second per gpu: 19914.399128066318
Epoch: 0, Global Step: 9060, Data Step: 45300, Loss: 1.1171875, Token per second per gpu: 20202.335033296335
Epoch: 0, Global Step: 9070, Data Step: 45350, Loss: 1.4375, Token per second per gpu: 19978.460013968375
Epoch: 0, Global Step: 9080, Data Step: 45400, Loss: 1.515625, Token per second per gpu: 19691.652940479038
Epoch: 0, Global Step: 9090, Data Step: 45450, Loss: 0.890625, Token per second per gpu: 20180.42165439727
Epoch: 0, Global Step: 9100, Data Step: 45500, Loss: 1.3515625, Token per second per gpu: 19866.489402884086
Epoch: 0, Global Step: 9110, Data Step: 45550, Loss: 1.1953125, Token per second per gpu: 19630.253413464256
Epoch: 0, Global Step: 9120, Data Step: 45600, Loss: 1.125, Token per second per gpu: 19780.648102418538
Epoch: 0, Global Step: 9130, Data Step: 45650, Loss: 1.25, Token per second per gpu: 20153.517839044438
Epoch: 0, Global Step: 9140, Data Step: 45700, Loss: 1.015625, Token per second per gpu: 19771.704248627717
Epoch: 0, Global Step: 9150, Data Step: 45750, Loss: 1.296875, Token per second per gpu: 19826.867684845703
Epoch: 0, Global Step: 9160, Data Step: 45800, Loss: 1.1328125, Token per second per gpu: 20212.012571958137
Epoch: 0, Global Step: 9170, Data Step: 45850, Loss: 1.1796875, Token per second per gpu: 19891.25360052957
Epoch: 0, Global Step: 9180, Data Step: 45900, Loss: 1.3515625, Token per second per gpu: 19857.142587667524
Epoch: 0, Global Step: 9190, Data Step: 45950, Loss: 1.1484375, Token per second per gpu: 19831.852756122134
Epoch: 0, Global Step: 9200, Data Step: 46000, Loss: 1.265625, Token per second per gpu: 20109.39771703269
Epoch: 0, Global Step: 9210, Data Step: 46050, Loss: 1.09375, Token per second per gpu: 19060.863170860288
Epoch: 0, Global Step: 9220, Data Step: 46100, Loss: 1.2578125, Token per second per gpu: 18787.533615834083
Epoch: 0, Global Step: 9230, Data Step: 46150, Loss: 1.21875, Token per second per gpu: 18965.801983190635
Epoch: 0, Global Step: 9240, Data Step: 46200, Loss: 1.1875, Token per second per gpu: 19906.98152659762
Epoch: 0, Global Step: 9250, Data Step: 46250, Loss: 1.3515625, Token per second per gpu: 19437.925568787676
Epoch: 0, Global Step: 9260, Data Step: 46300, Loss: 1.6875, Token per second per gpu: 18701.62879843206
Epoch: 0, Global Step: 9270, Data Step: 46350, Loss: 1.0859375, Token per second per gpu: 19274.16131225341
Epoch: 0, Global Step: 9280, Data Step: 46400, Loss: 1.046875, Token per second per gpu: 19239.173523964968
Epoch: 0, Global Step: 9290, Data Step: 46450, Loss: 1.2421875, Token per second per gpu: 19775.989404888107
Epoch: 0, Global Step: 9300, Data Step: 46500, Loss: 1.53125, Token per second per gpu: 19652.148268868656
Epoch: 0, Global Step: 9310, Data Step: 46550, Loss: 1.375, Token per second per gpu: 19655.760871876333
Epoch: 0, Global Step: 9320, Data Step: 46600, Loss: 1.078125, Token per second per gpu: 19136.119296090914
Epoch: 0, Global Step: 9330, Data Step: 46650, Loss: 1.2578125, Token per second per gpu: 19001.49609938565
Epoch: 0, Global Step: 9340, Data Step: 46700, Loss: 0.9140625, Token per second per gpu: 19086.10049458129
Epoch: 0, Global Step: 9350, Data Step: 46750, Loss: 1.0703125, Token per second per gpu: 19369.707603130384
Epoch: 0, Global Step: 9360, Data Step: 46800, Loss: 1.0234375, Token per second per gpu: 19554.970359948325
Epoch: 0, Global Step: 9370, Data Step: 46850, Loss: 0.95703125, Token per second per gpu: 18956.3138864641
Epoch: 0, Global Step: 9380, Data Step: 46900, Loss: 1.1328125, Token per second per gpu: 19003.476206935673
Epoch: 0, Global Step: 9390, Data Step: 46950, Loss: 0.95703125, Token per second per gpu: 19642.078703678424
Epoch: 0, Global Step: 9400, Data Step: 47000, Loss: 1.078125, Token per second per gpu: 19290.439226209903
Epoch: 0, Global Step: 9410, Data Step: 47050, Loss: 1.625, Token per second per gpu: 19184.577751151934
Epoch: 0, Global Step: 9420, Data Step: 47100, Loss: 1.28125, Token per second per gpu: 19468.739765235037
Epoch: 0, Global Step: 9430, Data Step: 47150, Loss: 1.09375, Token per second per gpu: 19288.065030204172
Epoch: 0, Global Step: 9440, Data Step: 47200, Loss: 1.0703125, Token per second per gpu: 19218.627119827357
Epoch: 0, Global Step: 9450, Data Step: 47250, Loss: 1.4765625, Token per second per gpu: 19486.690334895502
Epoch: 0, Global Step: 9460, Data Step: 47300, Loss: 1.171875, Token per second per gpu: 18576.878538322177
Epoch: 0, Global Step: 9470, Data Step: 47350, Loss: 1.2109375, Token per second per gpu: 18619.559276748212
Epoch: 0, Global Step: 9480, Data Step: 47400, Loss: 1.328125, Token per second per gpu: 18735.1844610078
Epoch: 0, Global Step: 9490, Data Step: 47450, Loss: 1.1796875, Token per second per gpu: 19338.02128959911
Epoch: 0, Global Step: 9500, Data Step: 47500, Loss: 1.0859375, Token per second per gpu: 19161.002146512554
Epoch: 0, Global Step: 9510, Data Step: 47550, Loss: 1.1875, Token per second per gpu: 19084.418584428357
Epoch: 0, Global Step: 9520, Data Step: 47600, Loss: 1.1953125, Token per second per gpu: 20061.175963981645
Epoch: 0, Global Step: 9530, Data Step: 47650, Loss: 0.98828125, Token per second per gpu: 18644.636257176633
Epoch: 0, Global Step: 9540, Data Step: 47700, Loss: 0.9453125, Token per second per gpu: 18858.33922198058
Epoch: 0, Global Step: 9550, Data Step: 47750, Loss: 1.046875, Token per second per gpu: 19949.38853148994
Epoch: 0, Global Step: 9560, Data Step: 47800, Loss: 1.15625, Token per second per gpu: 19224.214613013748
Epoch: 0, Global Step: 9570, Data Step: 47850, Loss: 1.015625, Token per second per gpu: 19014.381418858015
Epoch: 0, Global Step: 9580, Data Step: 47900, Loss: 0.91015625, Token per second per gpu: 19514.108076068944
Epoch: 0, Global Step: 9590, Data Step: 47950, Loss: 1.203125, Token per second per gpu: 19860.841060446215
Epoch: 0, Global Step: 9600, Data Step: 48000, Loss: 0.9140625, Token per second per gpu: 18919.797815730373
Epoch: 0, Global Step: 9610, Data Step: 48050, Loss: 1.1796875, Token per second per gpu: 19073.68346594463
Epoch: 0, Global Step: 9620, Data Step: 48100, Loss: 1.0390625, Token per second per gpu: 19163.599795096503
Epoch: 0, Global Step: 9630, Data Step: 48150, Loss: 1.1953125, Token per second per gpu: 18841.360163365443
Epoch: 0, Global Step: 9640, Data Step: 48200, Loss: 1.3046875, Token per second per gpu: 19335.938646180817
Epoch: 0, Global Step: 9650, Data Step: 48250, Loss: 1.125, Token per second per gpu: 19118.6016953321
Epoch: 0, Global Step: 9660, Data Step: 48300, Loss: 1.2890625, Token per second per gpu: 18393.613722948012
Epoch: 0, Global Step: 9670, Data Step: 48350, Loss: 1.390625, Token per second per gpu: 18784.689379259566
Epoch: 0, Global Step: 9680, Data Step: 48400, Loss: 1.3515625, Token per second per gpu: 19297.581276212728
Epoch: 0, Global Step: 9690, Data Step: 48450, Loss: 1.03125, Token per second per gpu: 18597.579157853463
Epoch: 0, Global Step: 9700, Data Step: 48500, Loss: 1.2890625, Token per second per gpu: 18758.797139207058
Epoch: 0, Global Step: 9710, Data Step: 48550, Loss: 0.95703125, Token per second per gpu: 19217.23492425748
Epoch: 0, Global Step: 9720, Data Step: 48600, Loss: 0.828125, Token per second per gpu: 18740.770900739535
Epoch: 0, Global Step: 9730, Data Step: 48650, Loss: 1.2109375, Token per second per gpu: 19561.884134137228
Epoch: 0, Global Step: 9740, Data Step: 48700, Loss: 0.80859375, Token per second per gpu: 19170.725569415725
Epoch: 0, Global Step: 9750, Data Step: 48750, Loss: 0.8671875, Token per second per gpu: 18519.40628651143
Epoch: 0, Global Step: 9760, Data Step: 48800, Loss: 1.015625, Token per second per gpu: 19562.50105897931
Epoch: 0, Global Step: 9770, Data Step: 48850, Loss: 1.1015625, Token per second per gpu: 19606.824508755602
Epoch: 0, Global Step: 9780, Data Step: 48900, Loss: 1.1796875, Token per second per gpu: 19775.774146876047
Epoch: 0, Global Step: 9790, Data Step: 48950, Loss: 0.9921875, Token per second per gpu: 19253.221153425467
Epoch: 0, Global Step: 9800, Data Step: 49000, Loss: 1.2734375, Token per second per gpu: 19088.589980676155
Epoch: 0, Global Step: 9810, Data Step: 49050, Loss: 0.98828125, Token per second per gpu: 19344.524928089617
Epoch: 0, Global Step: 9820, Data Step: 49100, Loss: 0.8671875, Token per second per gpu: 18469.378272991187
Epoch: 0, Global Step: 9830, Data Step: 49150, Loss: 1.078125, Token per second per gpu: 20110.936874308638
Epoch: 0, Global Step: 9840, Data Step: 49200, Loss: 1.7890625, Token per second per gpu: 19723.00378262081
Epoch: 0, Global Step: 9850, Data Step: 49250, Loss: 1.0078125, Token per second per gpu: 20447.733728278963
Epoch: 0, Global Step: 9860, Data Step: 49300, Loss: 1.390625, Token per second per gpu: 20948.811815317382
Epoch: 0, Global Step: 9870, Data Step: 49350, Loss: 1.21875, Token per second per gpu: 20010.730939840978
Epoch: 0, Global Step: 9880, Data Step: 49400, Loss: 1.203125, Token per second per gpu: 19976.144794599808
Epoch: 0, Global Step: 9890, Data Step: 49450, Loss: 1.140625, Token per second per gpu: 20101.82946387877
Epoch: 0, Global Step: 9900, Data Step: 49500, Loss: 1.3203125, Token per second per gpu: 19926.895694122763
Epoch: 0, Global Step: 9910, Data Step: 49550, Loss: 1.1484375, Token per second per gpu: 19576.138398946652
Epoch: 0, Global Step: 9920, Data Step: 49600, Loss: 1.15625, Token per second per gpu: 20180.66875871863
Epoch: 0, Global Step: 9930, Data Step: 49650, Loss: 1.25, Token per second per gpu: 19781.391693356858
Epoch: 0, Global Step: 9940, Data Step: 49700, Loss: 1.3125, Token per second per gpu: 19980.999604126628
Epoch: 0, Global Step: 9950, Data Step: 49750, Loss: 1.140625, Token per second per gpu: 19675.284943814928
Epoch: 0, Global Step: 9960, Data Step: 49800, Loss: 1.0703125, Token per second per gpu: 20079.64548473124
Epoch: 0, Global Step: 9970, Data Step: 49850, Loss: 1.125, Token per second per gpu: 19623.451722998783
Epoch: 0, Global Step: 9980, Data Step: 49900, Loss: 1.28125, Token per second per gpu: 19691.94311331652
Epoch: 0, Global Step: 9990, Data Step: 49950, Loss: 1.046875, Token per second per gpu: 19618.79256562417
Epoch: 0, Global Step: 10000, Data Step: 50000, Loss: 1.109375, Token per second per gpu: 19784.687416095818
I0407 05:19:14.434341 140235438175232 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2_instruction
I0407 05:19:14.435450 140235438175232 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-07 05:19:14,436] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-07 05:19:14,445] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2_instruction/pytorch_model/mp_rank_00_model_states.pt
[2024-04-07 05:19:14,446] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-07 05:19:15,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-07 05:19:15,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-07 05:19:15,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-07 05:19:15,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-07 05:19:15,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-07 05:19:15,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-07 05:19:15,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-07 05:19:17,571] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-07 05:19:17,571] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-07 05:19:17,571] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 05:19:17,600] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-07 05:19:17,601] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-07 05:19:17,601] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 05:19:17,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-07 05:19:17,682] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-07 05:19:17,682] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 05:19:17,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-07 05:19:17,692] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-07 05:19:17,692] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 05:19:17,692] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-07 05:19:17,692] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-07 05:19:17,692] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 05:19:17,722] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-07 05:19:17,722] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-07 05:19:17,722] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0407 05:19:17.723220 140235438175232 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2_instruction/pytorch_model
I0407 05:19:17.723971 140235438175232 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2_instruction/scheduler.bin
I0407 05:19:17.724276 140235438175232 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2_instruction/sampler.bin
I0407 05:19:17.725665 140235438175232 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2_instruction/random_states_0.pkl
Epoch: 0, Global Step: 10010, Data Step: 50050, Loss: 0.94140625, Token per second per gpu: 17402.408680256267
Epoch: 0, Global Step: 10020, Data Step: 50100, Loss: 1.0703125, Token per second per gpu: 19577.041770414
Epoch: 0, Global Step: 10030, Data Step: 50150, Loss: 0.9453125, Token per second per gpu: 19837.31823058531
Epoch: 0, Global Step: 10040, Data Step: 50200, Loss: 1.0, Token per second per gpu: 19883.79633904254
Epoch: 0, Global Step: 10050, Data Step: 50250, Loss: 0.99609375, Token per second per gpu: 19683.703329584037
Epoch: 0, Global Step: 10060, Data Step: 50300, Loss: 1.2578125, Token per second per gpu: 19849.548501366164
Epoch: 0, Global Step: 10070, Data Step: 50350, Loss: 0.90234375, Token per second per gpu: 19859.660793301045
Epoch: 0, Global Step: 10080, Data Step: 50400, Loss: 1.0546875, Token per second per gpu: 20014.41055989032
Epoch: 0, Global Step: 10090, Data Step: 50450, Loss: 1.109375, Token per second per gpu: 19832.043229473587
Epoch: 0, Global Step: 10100, Data Step: 50500, Loss: 0.984375, Token per second per gpu: 19793.012162790288
Epoch: 0, Global Step: 10110, Data Step: 50550, Loss: 1.109375, Token per second per gpu: 20100.60420115093
Epoch: 0, Global Step: 10120, Data Step: 50600, Loss: 0.93359375, Token per second per gpu: 19677.62813126708
Epoch: 0, Global Step: 10130, Data Step: 50650, Loss: 1.1796875, Token per second per gpu: 19816.34366288067
Epoch: 0, Global Step: 10140, Data Step: 50700, Loss: 1.1171875, Token per second per gpu: 19817.8172531401
Epoch: 0, Global Step: 10150, Data Step: 50750, Loss: 0.87109375, Token per second per gpu: 19899.735777682592
Epoch: 0, Global Step: 10160, Data Step: 50800, Loss: 0.73828125, Token per second per gpu: 19921.34619714468
Epoch: 0, Global Step: 10170, Data Step: 50850, Loss: 0.83984375, Token per second per gpu: 20051.295179740246
Epoch: 0, Global Step: 10180, Data Step: 50900, Loss: 0.87890625, Token per second per gpu: 20259.27281760546
Epoch: 0, Global Step: 10190, Data Step: 50950, Loss: 0.8515625, Token per second per gpu: 19879.759154536798
Epoch: 0, Global Step: 10200, Data Step: 51000, Loss: 1.046875, Token per second per gpu: 19923.315276038822
Epoch: 0, Global Step: 10210, Data Step: 51050, Loss: 1.03125, Token per second per gpu: 20218.70152711815
Epoch: 0, Global Step: 10220, Data Step: 51100, Loss: 0.90234375, Token per second per gpu: 19766.17589171628
Epoch: 0, Global Step: 10230, Data Step: 51150, Loss: 0.921875, Token per second per gpu: 19640.07752452621
Epoch: 0, Global Step: 10240, Data Step: 51200, Loss: 0.82421875, Token per second per gpu: 20256.09815303632
Epoch: 0, Global Step: 10250, Data Step: 51250, Loss: 0.98828125, Token per second per gpu: 19749.84547640025
Epoch: 0, Global Step: 10260, Data Step: 51300, Loss: 0.91015625, Token per second per gpu: 20181.73784383588
Epoch: 0, Global Step: 10270, Data Step: 51350, Loss: 1.0859375, Token per second per gpu: 19830.132434712614
Epoch: 0, Global Step: 10280, Data Step: 51400, Loss: 1.0859375, Token per second per gpu: 19639.732838336964
Epoch: 0, Global Step: 10290, Data Step: 51450, Loss: 1.015625, Token per second per gpu: 19694.6850999929
Epoch: 0, Global Step: 10300, Data Step: 51500, Loss: 0.796875, Token per second per gpu: 20127.394376815886
Epoch: 0, Global Step: 10310, Data Step: 51550, Loss: 1.484375, Token per second per gpu: 19908.840530706704
Epoch: 0, Global Step: 10320, Data Step: 51600, Loss: 0.86328125, Token per second per gpu: 19903.705630154727
Epoch: 0, Global Step: 10330, Data Step: 51650, Loss: 0.890625, Token per second per gpu: 19922.024814506447
Epoch: 0, Global Step: 10340, Data Step: 51700, Loss: 0.88671875, Token per second per gpu: 20130.326158105545
Epoch: 0, Global Step: 10350, Data Step: 51750, Loss: 0.84765625, Token per second per gpu: 19689.80719364767
Epoch: 0, Global Step: 10360, Data Step: 51800, Loss: 1.03125, Token per second per gpu: 19773.231106193132
Epoch: 0, Global Step: 10370, Data Step: 51850, Loss: 0.921875, Token per second per gpu: 20094.41243729244
Epoch: 0, Global Step: 10380, Data Step: 51900, Loss: 0.8828125, Token per second per gpu: 19713.4463563619
Epoch: 0, Global Step: 10390, Data Step: 51950, Loss: 0.7890625, Token per second per gpu: 19821.47017079084
Epoch: 0, Global Step: 10400, Data Step: 52000, Loss: 0.93359375, Token per second per gpu: 19838.610020369302
Epoch: 0, Global Step: 10410, Data Step: 52050, Loss: 1.03125, Token per second per gpu: 20148.39301714336
Epoch: 0, Global Step: 10420, Data Step: 52100, Loss: 1.1015625, Token per second per gpu: 19894.522818756017
Epoch: 0, Global Step: 10430, Data Step: 52150, Loss: 0.65625, Token per second per gpu: 19710.16274559784
Epoch: 0, Global Step: 10440, Data Step: 52200, Loss: 0.84375, Token per second per gpu: 19827.762674452515
Epoch: 0, Global Step: 10450, Data Step: 52250, Loss: 0.890625, Token per second per gpu: 19694.600569732676
Epoch: 0, Global Step: 10460, Data Step: 52300, Loss: 0.67578125, Token per second per gpu: 20016.372893204785
Epoch: 0, Global Step: 10470, Data Step: 52350, Loss: 0.94140625, Token per second per gpu: 19757.131148135977
Epoch: 0, Global Step: 10480, Data Step: 52400, Loss: 0.87890625, Token per second per gpu: 19755.980077209624
Epoch: 0, Global Step: 10490, Data Step: 52450, Loss: 0.82421875, Token per second per gpu: 20065.773330701355
Epoch: 0, Global Step: 10500, Data Step: 52500, Loss: 1.15625, Token per second per gpu: 19777.076693361425
Epoch: 0, Global Step: 10510, Data Step: 52550, Loss: 0.8515625, Token per second per gpu: 19916.296090126325
Epoch: 0, Global Step: 10520, Data Step: 52600, Loss: 0.65625, Token per second per gpu: 20080.968465308317
Epoch: 0, Global Step: 10530, Data Step: 52650, Loss: 0.89453125, Token per second per gpu: 19702.17179496161
Epoch: 0, Global Step: 10540, Data Step: 52700, Loss: 0.890625, Token per second per gpu: 19591.188205991177
Epoch: 0, Global Step: 10550, Data Step: 52750, Loss: 0.88671875, Token per second per gpu: 19705.759778214688
Epoch: 0, Global Step: 10560, Data Step: 52800, Loss: 0.75390625, Token per second per gpu: 20290.944290951244
Epoch: 0, Global Step: 10570, Data Step: 52850, Loss: 1.109375, Token per second per gpu: 19809.875296646493
Epoch: 0, Global Step: 10580, Data Step: 52900, Loss: 0.7890625, Token per second per gpu: 19799.93849663542
Epoch: 0, Global Step: 10590, Data Step: 52950, Loss: 0.78125, Token per second per gpu: 20206.913862102097
Epoch: 0, Global Step: 10600, Data Step: 53000, Loss: 0.9609375, Token per second per gpu: 19781.318261036173
Epoch: 0, Global Step: 10610, Data Step: 53050, Loss: 0.8125, Token per second per gpu: 19711.589827504882
Epoch: 0, Global Step: 10620, Data Step: 53100, Loss: 0.90625, Token per second per gpu: 19953.20135409634
Epoch: 0, Global Step: 10630, Data Step: 53150, Loss: 1.0078125, Token per second per gpu: 19936.12341296109
Epoch: 0, Global Step: 10640, Data Step: 53200, Loss: 0.796875, Token per second per gpu: 19988.935567532833
Epoch: 0, Global Step: 10650, Data Step: 53250, Loss: 0.9609375, Token per second per gpu: 20100.81906289895
Epoch: 0, Global Step: 10660, Data Step: 53300, Loss: 0.9296875, Token per second per gpu: 19785.02955330103
Epoch: 0, Global Step: 10670, Data Step: 53350, Loss: 0.80078125, Token per second per gpu: 20113.612363352368
Epoch: 0, Global Step: 10680, Data Step: 53400, Loss: 0.9453125, Token per second per gpu: 19816.444601759144
Epoch: 0, Global Step: 10690, Data Step: 53450, Loss: 0.765625, Token per second per gpu: 19701.056217674082
Epoch: 0, Global Step: 10700, Data Step: 53500, Loss: 0.7421875, Token per second per gpu: 20215.389988624414
Epoch: 0, Global Step: 10710, Data Step: 53550, Loss: 0.75, Token per second per gpu: 19753.482825395597
Epoch: 0, Global Step: 10720, Data Step: 53600, Loss: 0.9296875, Token per second per gpu: 19932.755772502733
Epoch: 0, Global Step: 10730, Data Step: 53650, Loss: 0.77734375, Token per second per gpu: 20009.888528930744
Epoch: 0, Global Step: 10740, Data Step: 53700, Loss: 0.85546875, Token per second per gpu: 19698.546630621644
Epoch: 0, Global Step: 10750, Data Step: 53750, Loss: 0.70703125, Token per second per gpu: 19614.308139811706
Epoch: 0, Global Step: 10760, Data Step: 53800, Loss: 0.85546875, Token per second per gpu: 19719.289465845974
Epoch: 0, Global Step: 10770, Data Step: 53850, Loss: 0.82421875, Token per second per gpu: 19970.33513620723
Epoch: 0, Global Step: 10780, Data Step: 53900, Loss: 0.75, Token per second per gpu: 19931.36992824448
Epoch: 0, Global Step: 10790, Data Step: 53950, Loss: 0.984375, Token per second per gpu: 19776.726271088424
Epoch: 0, Global Step: 10800, Data Step: 54000, Loss: 0.94140625, Token per second per gpu: 19886.346723053757
Epoch: 0, Global Step: 10810, Data Step: 54050, Loss: 0.82421875, Token per second per gpu: 20152.50904354835
Epoch: 0, Global Step: 10820, Data Step: 54100, Loss: 0.85546875, Token per second per gpu: 19929.005872275935
Epoch: 0, Global Step: 10830, Data Step: 54150, Loss: 0.84375, Token per second per gpu: 19961.53977360811
Epoch: 0, Global Step: 10840, Data Step: 54200, Loss: 0.9140625, Token per second per gpu: 19996.987141117694
Epoch: 0, Global Step: 10850, Data Step: 54250, Loss: 1.0078125, Token per second per gpu: 19937.37479453787
Epoch: 0, Global Step: 10860, Data Step: 54300, Loss: 0.88671875, Token per second per gpu: 19765.911362259496
Epoch: 0, Global Step: 10870, Data Step: 54350, Loss: 0.9140625, Token per second per gpu: 20070.12257961591
Epoch: 0, Global Step: 10880, Data Step: 54400, Loss: 0.88671875, Token per second per gpu: 19724.923880732476
Epoch: 0, Global Step: 10890, Data Step: 54450, Loss: 0.8515625, Token per second per gpu: 19450.783125286867
Epoch: 0, Global Step: 10900, Data Step: 54500, Loss: 0.90234375, Token per second per gpu: 19759.08153164255
Epoch: 0, Global Step: 10910, Data Step: 54550, Loss: 0.8828125, Token per second per gpu: 19826.557596842278
Epoch: 0, Global Step: 10920, Data Step: 54600, Loss: 0.8671875, Token per second per gpu: 19579.48336064939
Epoch: 0, Global Step: 10930, Data Step: 54650, Loss: 0.890625, Token per second per gpu: 19797.333754991327
Epoch: 0, Global Step: 10940, Data Step: 54700, Loss: 0.8828125, Token per second per gpu: 20070.624912520383
Epoch: 0, Global Step: 10950, Data Step: 54750, Loss: 0.6171875, Token per second per gpu: 20089.814907360505
Epoch: 0, Global Step: 10960, Data Step: 54800, Loss: 0.84765625, Token per second per gpu: 19664.866620962024
Epoch: 0, Global Step: 10970, Data Step: 54850, Loss: 0.94140625, Token per second per gpu: 20171.45565405323
Epoch: 0, Global Step: 10980, Data Step: 54900, Loss: 0.94921875, Token per second per gpu: 19820.606665626172
Epoch: 0, Global Step: 10990, Data Step: 54950, Loss: 0.9453125, Token per second per gpu: 19593.29312865414
Epoch: 0, Global Step: 11000, Data Step: 55000, Loss: 0.8359375, Token per second per gpu: 20016.151624094946
Epoch: 0, Global Step: 11010, Data Step: 55050, Loss: 1.03125, Token per second per gpu: 19810.18102538278
Epoch: 0, Global Step: 11020, Data Step: 55100, Loss: 0.74609375, Token per second per gpu: 19653.916271006754
Epoch: 0, Global Step: 11030, Data Step: 55150, Loss: 0.875, Token per second per gpu: 19746.74238108097
Epoch: 0, Global Step: 11040, Data Step: 55200, Loss: 0.8984375, Token per second per gpu: 19944.11341040138
Epoch: 0, Global Step: 11050, Data Step: 55250, Loss: 0.89453125, Token per second per gpu: 19957.371176650864
Epoch: 0, Global Step: 11060, Data Step: 55300, Loss: 0.65234375, Token per second per gpu: 19974.763872541935
Epoch: 0, Global Step: 11070, Data Step: 55350, Loss: 0.69921875, Token per second per gpu: 19990.045840153816
Epoch: 0, Global Step: 11080, Data Step: 55400, Loss: 0.73046875, Token per second per gpu: 19984.14050255913
Epoch: 0, Global Step: 11090, Data Step: 55450, Loss: 1.078125, Token per second per gpu: 20135.262248949926
Epoch: 0, Global Step: 11100, Data Step: 55500, Loss: 0.85546875, Token per second per gpu: 20180.242066642422
Epoch: 0, Global Step: 11110, Data Step: 55550, Loss: 0.7578125, Token per second per gpu: 19880.86598212212
Epoch: 0, Global Step: 11120, Data Step: 55600, Loss: 0.96875, Token per second per gpu: 19734.967413943446
Epoch: 0, Global Step: 11130, Data Step: 55650, Loss: 0.81640625, Token per second per gpu: 19786.540603345464
Epoch: 0, Global Step: 11140, Data Step: 55700, Loss: 0.70703125, Token per second per gpu: 20123.22241484856
Epoch: 0, Global Step: 11150, Data Step: 55750, Loss: 1.0234375, Token per second per gpu: 19811.0443534053
Epoch: 0, Global Step: 11160, Data Step: 55800, Loss: 0.8671875, Token per second per gpu: 20106.33818118393
Epoch: 0, Global Step: 11170, Data Step: 55850, Loss: 0.7421875, Token per second per gpu: 19659.373903449447
Epoch: 0, Global Step: 11180, Data Step: 55900, Loss: 0.8671875, Token per second per gpu: 19743.41445237033
Epoch: 0, Global Step: 11190, Data Step: 55950, Loss: 1.0546875, Token per second per gpu: 20019.28156287503
Epoch: 0, Global Step: 11200, Data Step: 56000, Loss: 0.69921875, Token per second per gpu: 20033.165547569915
Epoch: 0, Global Step: 11210, Data Step: 56050, Loss: 0.86328125, Token per second per gpu: 19783.868485753563
Epoch: 0, Global Step: 11220, Data Step: 56100, Loss: 0.8125, Token per second per gpu: 19609.2524051422
Epoch: 0, Global Step: 11230, Data Step: 56150, Loss: 0.828125, Token per second per gpu: 19930.43374606088
Epoch: 0, Global Step: 11240, Data Step: 56200, Loss: 0.76171875, Token per second per gpu: 20044.74064983668
Epoch: 0, Global Step: 11250, Data Step: 56250, Loss: 0.609375, Token per second per gpu: 19717.240661565975
Epoch: 0, Global Step: 11260, Data Step: 56300, Loss: 0.62890625, Token per second per gpu: 19897.82131976198
Epoch: 0, Global Step: 11270, Data Step: 56350, Loss: 0.62109375, Token per second per gpu: 20052.132467851494
Epoch: 0, Global Step: 11280, Data Step: 56400, Loss: 0.80859375, Token per second per gpu: 19792.06923212546
Epoch: 0, Global Step: 11290, Data Step: 56450, Loss: 0.6796875, Token per second per gpu: 19781.29038240708
Epoch: 0, Global Step: 11300, Data Step: 56500, Loss: 0.67578125, Token per second per gpu: 20019.65593728925
Epoch: 0, Global Step: 11310, Data Step: 56550, Loss: 0.5859375, Token per second per gpu: 19903.723155315147
Epoch: 0, Global Step: 11320, Data Step: 56600, Loss: 0.625, Token per second per gpu: 19614.005202322263
Epoch: 0, Global Step: 11330, Data Step: 56650, Loss: 0.65234375, Token per second per gpu: 20202.018601069125
Epoch: 0, Global Step: 11340, Data Step: 56700, Loss: 0.7421875, Token per second per gpu: 19928.012769837904
Epoch: 0, Global Step: 11350, Data Step: 56750, Loss: 0.78125, Token per second per gpu: 20070.55569487966
Epoch: 0, Global Step: 11360, Data Step: 56800, Loss: 0.7890625, Token per second per gpu: 19976.0537429982
Epoch: 0, Global Step: 11370, Data Step: 56850, Loss: 0.90625, Token per second per gpu: 20087.583165566048
Epoch: 0, Global Step: 11380, Data Step: 56900, Loss: 0.6484375, Token per second per gpu: 19705.21713979536
Epoch: 0, Global Step: 11390, Data Step: 56950, Loss: 0.67578125, Token per second per gpu: 19794.368797938052
Epoch: 0, Global Step: 11400, Data Step: 57000, Loss: 0.62890625, Token per second per gpu: 19710.909730624662
Epoch: 0, Global Step: 11410, Data Step: 57050, Loss: 1.046875, Token per second per gpu: 20045.144042981672
Epoch: 0, Global Step: 11420, Data Step: 57100, Loss: 0.84375, Token per second per gpu: 19920.843547423596
Epoch: 0, Global Step: 11430, Data Step: 57150, Loss: 0.81640625, Token per second per gpu: 19750.148083553355
Epoch: 0, Global Step: 11440, Data Step: 57200, Loss: 0.7734375, Token per second per gpu: 19756.026786303635
Epoch: 0, Global Step: 11450, Data Step: 57250, Loss: 0.8046875, Token per second per gpu: 20026.82307260527
Epoch: 0, Global Step: 11460, Data Step: 57300, Loss: 0.83203125, Token per second per gpu: 19697.38954814523
Epoch: 0, Global Step: 11470, Data Step: 57350, Loss: 0.79296875, Token per second per gpu: 19848.16062760408
Epoch: 0, Global Step: 11480, Data Step: 57400, Loss: 0.94921875, Token per second per gpu: 19844.20611440731
Epoch: 0, Global Step: 11490, Data Step: 57450, Loss: 0.71875, Token per second per gpu: 19653.521095824617
Epoch: 0, Global Step: 11500, Data Step: 57500, Loss: 0.75, Token per second per gpu: 20291.81801115859
Epoch: 0, Global Step: 11510, Data Step: 57550, Loss: 0.703125, Token per second per gpu: 19638.356908396305
Epoch: 0, Global Step: 11520, Data Step: 57600, Loss: 0.796875, Token per second per gpu: 19716.29009291066
Epoch: 0, Global Step: 11530, Data Step: 57650, Loss: 0.74609375, Token per second per gpu: 19709.38723640467
Epoch: 0, Global Step: 11540, Data Step: 57700, Loss: 0.828125, Token per second per gpu: 19904.82232125141
Epoch: 0, Global Step: 11550, Data Step: 57750, Loss: 0.92578125, Token per second per gpu: 20060.926717432518
Epoch: 0, Global Step: 11560, Data Step: 57800, Loss: 0.83203125, Token per second per gpu: 19693.678370061552
Epoch: 0, Global Step: 11570, Data Step: 57850, Loss: 0.89453125, Token per second per gpu: 19760.67353648301
Epoch: 0, Global Step: 11580, Data Step: 57900, Loss: 0.7578125, Token per second per gpu: 19789.65549094459
Epoch: 0, Global Step: 11590, Data Step: 57950, Loss: 0.94140625, Token per second per gpu: 20006.580729231493
Epoch: 0, Global Step: 11600, Data Step: 58000, Loss: 0.75390625, Token per second per gpu: 19874.16009864248
Epoch: 0, Global Step: 11610, Data Step: 58050, Loss: 1.21875, Token per second per gpu: 19952.84225268719
Epoch: 0, Global Step: 11620, Data Step: 58100, Loss: 0.90234375, Token per second per gpu: 19997.535167458213
Epoch: 0, Global Step: 11630, Data Step: 58150, Loss: 0.76953125, Token per second per gpu: 19822.736842132468
Epoch: 0, Global Step: 11640, Data Step: 58200, Loss: 0.7578125, Token per second per gpu: 19869.357981134985
Epoch: 0, Global Step: 11650, Data Step: 58250, Loss: 0.79296875, Token per second per gpu: 19729.210713020024
Epoch: 0, Global Step: 11660, Data Step: 58300, Loss: 0.875, Token per second per gpu: 19998.597973528267
Epoch: 0, Global Step: 11670, Data Step: 58350, Loss: 0.87109375, Token per second per gpu: 19841.532516019815
Epoch: 0, Global Step: 11680, Data Step: 58400, Loss: 0.71875, Token per second per gpu: 20275.42256875179
Epoch: 0, Global Step: 11690, Data Step: 58450, Loss: 1.078125, Token per second per gpu: 20039.262584608092
Epoch: 0, Global Step: 11700, Data Step: 58500, Loss: 0.73828125, Token per second per gpu: 20219.100530434607
Epoch: 0, Global Step: 11710, Data Step: 58550, Loss: 0.796875, Token per second per gpu: 21022.075834150328
Epoch: 0, Global Step: 11720, Data Step: 58600, Loss: 0.703125, Token per second per gpu: 20963.901546208603
Epoch: 0, Global Step: 11730, Data Step: 58650, Loss: 0.60546875, Token per second per gpu: 19671.037367817637
Epoch: 0, Global Step: 11740, Data Step: 58700, Loss: 0.88671875, Token per second per gpu: 19788.722726958542
Epoch: 0, Global Step: 11750, Data Step: 58750, Loss: 0.86328125, Token per second per gpu: 19747.406794013015
Epoch: 0, Global Step: 11760, Data Step: 58800, Loss: 0.88671875, Token per second per gpu: 20325.573858260286
Epoch: 0, Global Step: 11770, Data Step: 58850, Loss: 0.671875, Token per second per gpu: 19799.404350091798
Epoch: 0, Global Step: 11780, Data Step: 58900, Loss: 1.03125, Token per second per gpu: 19626.48425903307
Epoch: 0, Global Step: 11790, Data Step: 58950, Loss: 0.83984375, Token per second per gpu: 20072.37520827526
Epoch: 0, Global Step: 11800, Data Step: 59000, Loss: 0.77734375, Token per second per gpu: 19590.430608880146
Epoch: 0, Global Step: 11810, Data Step: 59050, Loss: 1.0078125, Token per second per gpu: 19931.701802912496
Epoch: 0, Global Step: 11820, Data Step: 59100, Loss: 1.0390625, Token per second per gpu: 19584.418199046962
Epoch: 0, Global Step: 11830, Data Step: 59150, Loss: 0.87109375, Token per second per gpu: 20146.990257845162
Epoch: 0, Global Step: 11840, Data Step: 59200, Loss: 0.466796875, Token per second per gpu: 19853.476694739187
Epoch: 0, Global Step: 11850, Data Step: 59250, Loss: 0.6796875, Token per second per gpu: 19932.24699671656
Epoch: 0, Global Step: 11860, Data Step: 59300, Loss: 0.5546875, Token per second per gpu: 20085.719375961362
Epoch: 0, Global Step: 11870, Data Step: 59350, Loss: 0.6796875, Token per second per gpu: 19801.649748629166
Epoch: 0, Global Step: 11880, Data Step: 59400, Loss: 0.6484375, Token per second per gpu: 19759.777502198987
Epoch: 0, Global Step: 11890, Data Step: 59450, Loss: 0.76171875, Token per second per gpu: 19671.72985275935
Epoch: 0, Global Step: 11900, Data Step: 59500, Loss: 0.77734375, Token per second per gpu: 20102.958335494204
Epoch: 0, Global Step: 11910, Data Step: 59550, Loss: 0.85546875, Token per second per gpu: 19752.68664338453
Epoch: 0, Global Step: 11920, Data Step: 59600, Loss: 0.625, Token per second per gpu: 19871.705147880526
Epoch: 0, Global Step: 11930, Data Step: 59650, Loss: 0.7578125, Token per second per gpu: 20006.81576613929
Epoch: 0, Global Step: 11940, Data Step: 59700, Loss: 0.55078125, Token per second per gpu: 19719.40933661409
Epoch: 0, Global Step: 11950, Data Step: 59750, Loss: 0.94921875, Token per second per gpu: 19833.89723910787
Epoch: 0, Global Step: 11960, Data Step: 59800, Loss: 0.71484375, Token per second per gpu: 19660.82100002243
Epoch: 0, Global Step: 11970, Data Step: 59850, Loss: 0.796875, Token per second per gpu: 20180.924213676368
Epoch: 0, Global Step: 11980, Data Step: 59900, Loss: 0.74609375, Token per second per gpu: 19790.110507436213
Epoch: 0, Global Step: 11990, Data Step: 59950, Loss: 0.70703125, Token per second per gpu: 19816.02530985694
Epoch: 0, Global Step: 12000, Data Step: 60000, Loss: 0.765625, Token per second per gpu: 19869.554874724996
Epoch: 0, Global Step: 12010, Data Step: 60050, Loss: 0.6640625, Token per second per gpu: 20191.88110388289
Epoch: 0, Global Step: 12020, Data Step: 60100, Loss: 0.734375, Token per second per gpu: 19653.016582559198
Epoch: 0, Global Step: 12030, Data Step: 60150, Loss: 0.91796875, Token per second per gpu: 19867.788670990707
Epoch: 0, Global Step: 12040, Data Step: 60200, Loss: 0.5859375, Token per second per gpu: 20239.988506803482
Epoch: 0, Global Step: 12050, Data Step: 60250, Loss: 0.7890625, Token per second per gpu: 19902.513253830923
Epoch: 0, Global Step: 12060, Data Step: 60300, Loss: 0.9296875, Token per second per gpu: 19681.379799229886
Epoch: 0, Global Step: 12070, Data Step: 60350, Loss: 0.76953125, Token per second per gpu: 19641.658673738828
Epoch: 0, Global Step: 12080, Data Step: 60400, Loss: 0.7578125, Token per second per gpu: 19839.440637146985
Epoch: 0, Global Step: 12090, Data Step: 60450, Loss: 0.90234375, Token per second per gpu: 19796.615609354332
Epoch: 0, Global Step: 12100, Data Step: 60500, Loss: 0.81640625, Token per second per gpu: 19634.570459665756
Epoch: 0, Global Step: 12110, Data Step: 60550, Loss: 0.625, Token per second per gpu: 19672.71902132356
Epoch: 0, Global Step: 12120, Data Step: 60600, Loss: 0.6328125, Token per second per gpu: 19804.032264467838
Epoch: 0, Global Step: 12130, Data Step: 60650, Loss: 0.7578125, Token per second per gpu: 20058.283023865373
Epoch: 0, Global Step: 12140, Data Step: 60700, Loss: 0.60546875, Token per second per gpu: 19780.157993407385
Epoch: 0, Global Step: 12150, Data Step: 60750, Loss: 0.7734375, Token per second per gpu: 19981.459557966926
Epoch: 0, Global Step: 12160, Data Step: 60800, Loss: 0.703125, Token per second per gpu: 19973.85686374985
Epoch: 0, Global Step: 12170, Data Step: 60850, Loss: 0.7890625, Token per second per gpu: 19763.016191826715
Epoch: 0, Global Step: 12180, Data Step: 60900, Loss: 0.578125, Token per second per gpu: 19685.5695866075
Epoch: 0, Global Step: 12190, Data Step: 60950, Loss: 0.8515625, Token per second per gpu: 19603.935127177196
Epoch: 0, Global Step: 12200, Data Step: 61000, Loss: 0.78515625, Token per second per gpu: 19912.24255998079
Epoch: 0, Global Step: 12210, Data Step: 61050, Loss: 0.59375, Token per second per gpu: 20018.82975676699
Epoch: 0, Global Step: 12220, Data Step: 61100, Loss: 0.609375, Token per second per gpu: 19620.1030156899
Epoch: 0, Global Step: 12230, Data Step: 61150, Loss: 0.76953125, Token per second per gpu: 19844.67813013046
Epoch: 0, Global Step: 12240, Data Step: 61200, Loss: 0.828125, Token per second per gpu: 20047.84603555237
Epoch: 0, Global Step: 12250, Data Step: 61250, Loss: 0.74609375, Token per second per gpu: 19914.23790941771
Epoch: 0, Global Step: 12260, Data Step: 61300, Loss: 0.72265625, Token per second per gpu: 19703.892945434207
Epoch: 0, Global Step: 12270, Data Step: 61350, Loss: 0.5546875, Token per second per gpu: 19787.93646519304
Epoch: 0, Global Step: 12280, Data Step: 61400, Loss: 0.91015625, Token per second per gpu: 20294.90587368345
Epoch: 0, Global Step: 12290, Data Step: 61450, Loss: 0.953125, Token per second per gpu: 19587.64092628256
Epoch: 0, Global Step: 12300, Data Step: 61500, Loss: 0.65234375, Token per second per gpu: 19629.523115317428
Epoch: 0, Global Step: 12310, Data Step: 61550, Loss: 0.75, Token per second per gpu: 19941.563373456993
Epoch: 0, Global Step: 12320, Data Step: 61600, Loss: 0.6953125, Token per second per gpu: 19715.169477339856
Epoch: 0, Global Step: 12330, Data Step: 61650, Loss: 0.71875, Token per second per gpu: 19784.416558272267
Epoch: 0, Global Step: 12340, Data Step: 61700, Loss: 0.83984375, Token per second per gpu: 19640.317321168724
Epoch: 0, Global Step: 12350, Data Step: 61750, Loss: 0.85546875, Token per second per gpu: 19785.157698305014
Epoch: 0, Global Step: 12360, Data Step: 61800, Loss: 0.67578125, Token per second per gpu: 19718.60231939909
Epoch: 0, Global Step: 12370, Data Step: 61850, Loss: 0.6484375, Token per second per gpu: 20053.60481909489
Epoch: 0, Global Step: 12380, Data Step: 61900, Loss: 0.6015625, Token per second per gpu: 19684.96797192366
Epoch: 0, Global Step: 12390, Data Step: 61950, Loss: 0.828125, Token per second per gpu: 19832.37162144896
Epoch: 0, Global Step: 12400, Data Step: 62000, Loss: 0.63671875, Token per second per gpu: 19742.22432439824
Epoch: 0, Global Step: 12410, Data Step: 62050, Loss: 0.87109375, Token per second per gpu: 19591.964093344694
Epoch: 0, Global Step: 12420, Data Step: 62100, Loss: 0.83203125, Token per second per gpu: 19850.139667940268
Epoch: 0, Global Step: 12430, Data Step: 62150, Loss: 0.86328125, Token per second per gpu: 19831.425669560664
Epoch: 0, Global Step: 12440, Data Step: 62200, Loss: 0.875, Token per second per gpu: 19659.97251482872
Epoch: 0, Global Step: 12450, Data Step: 62250, Loss: 0.76953125, Token per second per gpu: 19831.608625916895
Epoch: 0, Global Step: 12460, Data Step: 62300, Loss: 0.8046875, Token per second per gpu: 19559.329175205363
Epoch: 0, Global Step: 12470, Data Step: 62350, Loss: 0.71875, Token per second per gpu: 19852.825313830952
Epoch: 0, Global Step: 12480, Data Step: 62400, Loss: 0.64453125, Token per second per gpu: 19832.520894123896
Epoch: 0, Global Step: 12490, Data Step: 62450, Loss: 0.8203125, Token per second per gpu: 19837.313832666572
Epoch: 0, Global Step: 12500, Data Step: 62500, Loss: 0.59765625, Token per second per gpu: 19623.31347073376
Epoch: 0, Global Step: 12510, Data Step: 62550, Loss: 0.5078125, Token per second per gpu: 20165.55874193112
Epoch: 0, Global Step: 12520, Data Step: 62600, Loss: 1.109375, Token per second per gpu: 19827.398737656775
Epoch: 0, Global Step: 12530, Data Step: 62650, Loss: 0.89453125, Token per second per gpu: 19908.819305132507
Epoch: 0, Global Step: 12540, Data Step: 62700, Loss: 0.84375, Token per second per gpu: 20185.876234811647
Epoch: 0, Global Step: 12550, Data Step: 62750, Loss: 0.72265625, Token per second per gpu: 19674.83284876836
Epoch: 0, Global Step: 12560, Data Step: 62800, Loss: 0.734375, Token per second per gpu: 19685.838466487265
Epoch: 0, Global Step: 12570, Data Step: 62850, Loss: 0.671875, Token per second per gpu: 19636.053407216783
Epoch: 0, Global Step: 12580, Data Step: 62900, Loss: 0.71484375, Token per second per gpu: 20187.758279539794
Epoch: 0, Global Step: 12590, Data Step: 62950, Loss: 0.9765625, Token per second per gpu: 19663.412627133057
Epoch: 0, Global Step: 12600, Data Step: 63000, Loss: 0.54296875, Token per second per gpu: 19857.546544686615
Epoch: 0, Global Step: 12610, Data Step: 63050, Loss: 0.79296875, Token per second per gpu: 19653.498252774687
Epoch: 0, Global Step: 12620, Data Step: 63100, Loss: 0.63671875, Token per second per gpu: 20078.167805507146
Epoch: 0, Global Step: 12630, Data Step: 63150, Loss: 0.8671875, Token per second per gpu: 19531.0201026249
Epoch: 0, Global Step: 12640, Data Step: 63200, Loss: 0.69921875, Token per second per gpu: 19640.075369077324
Epoch: 0, Global Step: 12650, Data Step: 63250, Loss: 0.8828125, Token per second per gpu: 19726.90487750909
Epoch: 0, Global Step: 12660, Data Step: 63300, Loss: 0.69921875, Token per second per gpu: 20163.178565135786
Epoch: 0, Global Step: 12670, Data Step: 63350, Loss: 0.578125, Token per second per gpu: 19653.0369064788
Epoch: 0, Global Step: 12680, Data Step: 63400, Loss: 0.71875, Token per second per gpu: 19665.463225779935
Epoch: 0, Global Step: 12690, Data Step: 63450, Loss: 0.76171875, Token per second per gpu: 19508.923258669987
Epoch: 0, Global Step: 12700, Data Step: 63500, Loss: 0.76953125, Token per second per gpu: 20073.485763181932
Epoch: 0, Global Step: 12710, Data Step: 63550, Loss: 0.62109375, Token per second per gpu: 19767.155476475287
Epoch: 0, Global Step: 12720, Data Step: 63600, Loss: 0.71484375, Token per second per gpu: 19798.87059744098
Epoch: 0, Global Step: 12730, Data Step: 63650, Loss: 0.78515625, Token per second per gpu: 19716.81741095549
Epoch: 0, Global Step: 12740, Data Step: 63700, Loss: 0.72265625, Token per second per gpu: 19872.34618295962
Epoch: 0, Global Step: 12750, Data Step: 63750, Loss: 0.82421875, Token per second per gpu: 19672.470863841125
Epoch: 0, Global Step: 12760, Data Step: 63800, Loss: 0.9765625, Token per second per gpu: 19551.565235575577
Epoch: 0, Global Step: 12770, Data Step: 63850, Loss: 0.57421875, Token per second per gpu: 20253.061435052794
Epoch: 0, Global Step: 12780, Data Step: 63900, Loss: 0.90625, Token per second per gpu: 19617.61382993567
Epoch: 0, Global Step: 12790, Data Step: 63950, Loss: 0.7265625, Token per second per gpu: 20018.727305430355
Epoch: 0, Global Step: 12800, Data Step: 64000, Loss: 0.66796875, Token per second per gpu: 19453.324429515935
Epoch: 0, Global Step: 12810, Data Step: 64050, Loss: 0.6015625, Token per second per gpu: 19900.64934529377
Epoch: 0, Global Step: 12820, Data Step: 64100, Loss: 0.8046875, Token per second per gpu: 20259.95515737289
Epoch: 0, Global Step: 12830, Data Step: 64150, Loss: 0.875, Token per second per gpu: 19805.019258141583
Epoch: 0, Global Step: 12840, Data Step: 64200, Loss: 0.73046875, Token per second per gpu: 19796.1831045928
Epoch: 0, Global Step: 12850, Data Step: 64250, Loss: 0.828125, Token per second per gpu: 19696.21002044771
Epoch: 0, Global Step: 12860, Data Step: 64300, Loss: 0.81640625, Token per second per gpu: 20146.1321777205
Epoch: 0, Global Step: 12870, Data Step: 64350, Loss: 0.5703125, Token per second per gpu: 19875.65664903114
Epoch: 0, Global Step: 12880, Data Step: 64400, Loss: 0.73828125, Token per second per gpu: 19558.742020951577
Epoch: 0, Global Step: 12890, Data Step: 64450, Loss: 0.7734375, Token per second per gpu: 19810.882976255478
Epoch: 0, Global Step: 12900, Data Step: 64500, Loss: 0.6953125, Token per second per gpu: 20116.22977223927
Epoch: 0, Global Step: 12910, Data Step: 64550, Loss: 0.796875, Token per second per gpu: 19771.94035271624
Epoch: 0, Global Step: 12920, Data Step: 64600, Loss: 1.0546875, Token per second per gpu: 19895.453234058572
Epoch: 0, Global Step: 12930, Data Step: 64650, Loss: 0.5546875, Token per second per gpu: 19943.70388633449
Epoch: 0, Global Step: 12940, Data Step: 64700, Loss: 0.72265625, Token per second per gpu: 19777.155740434766
Epoch: 0, Global Step: 12950, Data Step: 64750, Loss: 0.8046875, Token per second per gpu: 19547.27339947732
Epoch: 0, Global Step: 12960, Data Step: 64800, Loss: 0.6875, Token per second per gpu: 19719.038140326003
Epoch: 0, Global Step: 12970, Data Step: 64850, Loss: 0.65625, Token per second per gpu: 19768.830128983325
Epoch: 0, Global Step: 12980, Data Step: 64900, Loss: 0.8359375, Token per second per gpu: 20163.057024770544
Epoch: 0, Global Step: 12990, Data Step: 64950, Loss: 0.7265625, Token per second per gpu: 19858.19658275517
Epoch: 0, Global Step: 13000, Data Step: 65000, Loss: 0.6640625, Token per second per gpu: 19682.821478007012
Epoch: 0, Global Step: 13010, Data Step: 65050, Loss: 0.8359375, Token per second per gpu: 19907.98674461918
Epoch: 0, Global Step: 13020, Data Step: 65100, Loss: 0.78515625, Token per second per gpu: 20168.897540600465
Epoch: 0, Global Step: 13030, Data Step: 65150, Loss: 0.7265625, Token per second per gpu: 19728.623464893553
Epoch: 0, Global Step: 13040, Data Step: 65200, Loss: 0.7890625, Token per second per gpu: 19865.16163787641
Epoch: 0, Global Step: 13050, Data Step: 65250, Loss: 0.68359375, Token per second per gpu: 19975.345055435657
Epoch: 0, Global Step: 13060, Data Step: 65300, Loss: 0.546875, Token per second per gpu: 19842.985652594383
Epoch: 0, Global Step: 13070, Data Step: 65350, Loss: 0.86328125, Token per second per gpu: 19725.82382327056
Epoch: 0, Global Step: 13080, Data Step: 65400, Loss: 0.94921875, Token per second per gpu: 20116.03700431026
Epoch: 0, Global Step: 13090, Data Step: 65450, Loss: 0.84765625, Token per second per gpu: 19823.093837397664
Epoch: 0, Global Step: 13100, Data Step: 65500, Loss: 0.6328125, Token per second per gpu: 19891.07617469509
Epoch: 0, Global Step: 13110, Data Step: 65550, Loss: 0.74609375, Token per second per gpu: 19905.559040689906
Epoch: 0, Global Step: 13120, Data Step: 65600, Loss: 0.6171875, Token per second per gpu: 19782.101627974807
Epoch: 0, Global Step: 13130, Data Step: 65650, Loss: 0.59765625, Token per second per gpu: 20155.453443199458
Epoch: 0, Global Step: 13140, Data Step: 65700, Loss: 0.7109375, Token per second per gpu: 19769.849471265963
Epoch: 0, Global Step: 13150, Data Step: 65750, Loss: 0.6796875, Token per second per gpu: 19759.037535151572
Epoch: 0, Global Step: 13160, Data Step: 65800, Loss: 0.7421875, Token per second per gpu: 19737.340984292652
Epoch: 0, Global Step: 13170, Data Step: 65850, Loss: 0.7734375, Token per second per gpu: 20084.416243890773
Epoch: 0, Global Step: 13180, Data Step: 65900, Loss: 0.6484375, Token per second per gpu: 19650.7545951798
Epoch: 0, Global Step: 13190, Data Step: 65950, Loss: 0.66015625, Token per second per gpu: 19700.423655447084
Epoch: 0, Global Step: 13200, Data Step: 66000, Loss: 0.80859375, Token per second per gpu: 19690.37137084208
Epoch: 0, Global Step: 13210, Data Step: 66050, Loss: 0.75, Token per second per gpu: 19816.542067267997
Epoch: 0, Global Step: 13220, Data Step: 66100, Loss: 0.56640625, Token per second per gpu: 20213.785903192766
Epoch: 0, Global Step: 13230, Data Step: 66150, Loss: 0.5859375, Token per second per gpu: 19733.215625988218
Epoch: 0, Global Step: 13240, Data Step: 66200, Loss: 0.80078125, Token per second per gpu: 19824.811107576825
Epoch: 0, Global Step: 13250, Data Step: 66250, Loss: 0.765625, Token per second per gpu: 19851.614068224513
Epoch: 0, Global Step: 13260, Data Step: 66300, Loss: 0.8203125, Token per second per gpu: 20171.71883383714
Epoch: 0, Global Step: 13270, Data Step: 66350, Loss: 0.6484375, Token per second per gpu: 19582.9183964777
Epoch: 0, Global Step: 13280, Data Step: 66400, Loss: 0.64453125, Token per second per gpu: 19521.454182420912
Epoch: 0, Global Step: 13290, Data Step: 66450, Loss: 0.76171875, Token per second per gpu: 20214.409049678143
Epoch: 0, Global Step: 13300, Data Step: 66500, Loss: 0.76171875, Token per second per gpu: 19860.704586032993
Epoch: 0, Global Step: 13310, Data Step: 66550, Loss: 0.9296875, Token per second per gpu: 20104.20929364898
Epoch: 0, Global Step: 13320, Data Step: 66600, Loss: 0.90234375, Token per second per gpu: 20038.844656023077
Epoch: 0, Global Step: 13330, Data Step: 66650, Loss: 0.5390625, Token per second per gpu: 19951.018580415923
Epoch: 0, Global Step: 13340, Data Step: 66700, Loss: 0.71875, Token per second per gpu: 19776.418842231276
Epoch: 0, Global Step: 13350, Data Step: 66750, Loss: 0.78125, Token per second per gpu: 19746.489810577354
Epoch: 0, Global Step: 13360, Data Step: 66800, Loss: 0.57421875, Token per second per gpu: 19962.929074356784
Epoch: 0, Global Step: 13370, Data Step: 66850, Loss: 0.703125, Token per second per gpu: 19788.46452279098
Epoch: 0, Global Step: 13380, Data Step: 66900, Loss: 0.78125, Token per second per gpu: 19671.427481492985
Epoch: 0, Global Step: 13390, Data Step: 66950, Loss: 0.640625, Token per second per gpu: 19973.158363351507
Epoch: 0, Global Step: 13400, Data Step: 67000, Loss: 0.78125, Token per second per gpu: 19782.5194854925
Epoch: 0, Global Step: 13410, Data Step: 67050, Loss: 0.875, Token per second per gpu: 20048.958182848302
Epoch: 0, Global Step: 13420, Data Step: 67100, Loss: 0.62109375, Token per second per gpu: 20262.64100263576
Epoch: 0, Global Step: 13430, Data Step: 67150, Loss: 0.671875, Token per second per gpu: 19830.545731447375
Epoch: 0, Global Step: 13440, Data Step: 67200, Loss: 0.69921875, Token per second per gpu: 19707.572345672404
Epoch: 0, Global Step: 13450, Data Step: 67250, Loss: 0.5703125, Token per second per gpu: 19759.26442824654
Epoch: 0, Global Step: 13460, Data Step: 67300, Loss: 0.74609375, Token per second per gpu: 19521.6758294096
Epoch: 0, Global Step: 13470, Data Step: 67350, Loss: 0.7890625, Token per second per gpu: 20135.201080368886
Epoch: 0, Global Step: 13480, Data Step: 67400, Loss: 0.67578125, Token per second per gpu: 19670.93682370621
Epoch: 0, Global Step: 13490, Data Step: 67450, Loss: 0.84375, Token per second per gpu: 19638.255620559565
Epoch: 0, Global Step: 13500, Data Step: 67500, Loss: 0.8046875, Token per second per gpu: 19659.034478556412
Epoch: 0, Global Step: 13510, Data Step: 67550, Loss: 0.59375, Token per second per gpu: 19944.68447542884
Epoch: 0, Global Step: 13520, Data Step: 67600, Loss: 0.9296875, Token per second per gpu: 19757.216943097614
Epoch: 0, Global Step: 13530, Data Step: 67650, Loss: 0.80078125, Token per second per gpu: 19695.078680993982
Epoch: 0, Global Step: 13540, Data Step: 67700, Loss: 0.7421875, Token per second per gpu: 20110.094856598294
Epoch: 0, Global Step: 13550, Data Step: 67750, Loss: 0.90234375, Token per second per gpu: 20523.673011389077
Epoch: 0, Global Step: 13560, Data Step: 67800, Loss: 0.703125, Token per second per gpu: 20940.897414048126
Epoch: 0, Global Step: 13570, Data Step: 67850, Loss: 0.67578125, Token per second per gpu: 19524.49468109166
Epoch: 0, Global Step: 13580, Data Step: 67900, Loss: 0.8359375, Token per second per gpu: 20132.74520457092
Epoch: 0, Global Step: 13590, Data Step: 67950, Loss: 0.83984375, Token per second per gpu: 19879.953677827765
Epoch: 0, Global Step: 13600, Data Step: 68000, Loss: 0.7890625, Token per second per gpu: 19711.938488076474
Epoch: 0, Global Step: 13610, Data Step: 68050, Loss: 0.51171875, Token per second per gpu: 19694.171427296948
Epoch: 0, Global Step: 13620, Data Step: 68100, Loss: 0.609375, Token per second per gpu: 20124.25694539548
Epoch: 0, Global Step: 13630, Data Step: 68150, Loss: 0.7734375, Token per second per gpu: 19830.345215372672
Epoch: 0, Global Step: 13640, Data Step: 68200, Loss: 0.58203125, Token per second per gpu: 19852.65114229604
Epoch: 0, Global Step: 13650, Data Step: 68250, Loss: 0.73046875, Token per second per gpu: 19848.033866521357
Epoch: 0, Global Step: 13660, Data Step: 68300, Loss: 0.72265625, Token per second per gpu: 20257.156327698518
Epoch: 0, Global Step: 13670, Data Step: 68350, Loss: 0.79296875, Token per second per gpu: 19845.339430015887
Epoch: 0, Global Step: 13680, Data Step: 68400, Loss: 0.609375, Token per second per gpu: 19826.983558472177
Epoch: 0, Global Step: 13690, Data Step: 68450, Loss: 0.71875, Token per second per gpu: 19793.772921029453
Epoch: 0, Global Step: 13700, Data Step: 68500, Loss: 0.65234375, Token per second per gpu: 19845.475509946893
Epoch: 0, Global Step: 13710, Data Step: 68550, Loss: 0.8515625, Token per second per gpu: 20270.651916303384
Epoch: 0, Global Step: 13720, Data Step: 68600, Loss: 0.6875, Token per second per gpu: 19661.426539965607
Epoch: 0, Global Step: 13730, Data Step: 68650, Loss: 0.75, Token per second per gpu: 19554.531967796505
Epoch: 0, Global Step: 13740, Data Step: 68700, Loss: 0.57421875, Token per second per gpu: 19823.440598056386
Epoch: 0, Global Step: 13750, Data Step: 68750, Loss: 0.55078125, Token per second per gpu: 19909.475103992223
Epoch: 0, Global Step: 13760, Data Step: 68800, Loss: 0.66796875, Token per second per gpu: 19549.240583370894
Epoch: 0, Global Step: 13770, Data Step: 68850, Loss: 0.76171875, Token per second per gpu: 19773.94828451152
Epoch: 0, Global Step: 13780, Data Step: 68900, Loss: 0.7265625, Token per second per gpu: 19808.10799305904
Epoch: 0, Global Step: 13790, Data Step: 68950, Loss: 0.94921875, Token per second per gpu: 19633.447446511065
Epoch: 0, Global Step: 13800, Data Step: 69000, Loss: 1.7109375, Token per second per gpu: 20165.57729933114
Epoch: 0, Global Step: 13810, Data Step: 69050, Loss: 2.125, Token per second per gpu: 19910.31369612584
Epoch: 0, Global Step: 13820, Data Step: 69100, Loss: 1.625, Token per second per gpu: 19532.265378847384
Epoch: 0, Global Step: 13830, Data Step: 69150, Loss: 1.3125, Token per second per gpu: 19621.132175997205
Epoch: 0, Global Step: 13840, Data Step: 69200, Loss: 1.21875, Token per second per gpu: 20075.45838463209
Epoch: 0, Global Step: 13850, Data Step: 69250, Loss: 1.421875, Token per second per gpu: 19957.069419347546
Epoch: 0, Global Step: 13860, Data Step: 69300, Loss: 1.640625, Token per second per gpu: 19739.177147361006
Epoch: 0, Global Step: 13870, Data Step: 69350, Loss: 1.75, Token per second per gpu: 20026.931209816496
Epoch: 0, Global Step: 13880, Data Step: 69400, Loss: 1.4140625, Token per second per gpu: 20160.80027504892
Epoch: 0, Global Step: 13890, Data Step: 69450, Loss: 1.84375, Token per second per gpu: 20042.42856240758
Epoch: 0, Global Step: 13900, Data Step: 69500, Loss: 1.140625, Token per second per gpu: 19923.05798362812
Epoch: 0, Global Step: 13910, Data Step: 69550, Loss: 1.5078125, Token per second per gpu: 20230.81398854764
Epoch: 0, Global Step: 13920, Data Step: 69600, Loss: 1.84375, Token per second per gpu: 20091.98680275281
Epoch: 0, Global Step: 13930, Data Step: 69650, Loss: 1.4609375, Token per second per gpu: 19728.67910696671
Epoch: 0, Global Step: 13940, Data Step: 69700, Loss: 1.6484375, Token per second per gpu: 19724.12148461355
Epoch: 0, Global Step: 13950, Data Step: 69750, Loss: 1.4921875, Token per second per gpu: 20137.092758541283
Epoch: 0, Global Step: 13960, Data Step: 69800, Loss: 2.109375, Token per second per gpu: 19788.614411677987
Epoch: 0, Global Step: 13970, Data Step: 69850, Loss: 1.28125, Token per second per gpu: 19661.085424143563
Epoch: 0, Global Step: 13980, Data Step: 69900, Loss: 1.4765625, Token per second per gpu: 19765.604269542935
Epoch: 0, Global Step: 13990, Data Step: 69950, Loss: 1.84375, Token per second per gpu: 19927.878699345383
Epoch: 0, Global Step: 14000, Data Step: 70000, Loss: 1.3984375, Token per second per gpu: 20021.383914512153
Epoch: 0, Global Step: 14010, Data Step: 70050, Loss: 1.34375, Token per second per gpu: 19765.417617139683
Epoch: 0, Global Step: 14020, Data Step: 70100, Loss: 1.671875, Token per second per gpu: 19593.459024938664
Epoch: 0, Global Step: 14030, Data Step: 70150, Loss: 2.15625, Token per second per gpu: 20192.403220623983
Epoch: 0, Global Step: 14040, Data Step: 70200, Loss: 1.109375, Token per second per gpu: 19845.88504477201
Epoch: 0, Global Step: 14050, Data Step: 70250, Loss: 1.6875, Token per second per gpu: 19758.53013523577
Epoch: 0, Global Step: 14060, Data Step: 70300, Loss: 2.0, Token per second per gpu: 19825.442714738714
Epoch: 0, Global Step: 14070, Data Step: 70350, Loss: 1.984375, Token per second per gpu: 20311.073510406448
Epoch: 0, Global Step: 14080, Data Step: 70400, Loss: 1.4765625, Token per second per gpu: 20038.62363775163
Epoch: 0, Global Step: 14090, Data Step: 70450, Loss: 1.3359375, Token per second per gpu: 19853.493397375812
Epoch: 0, Global Step: 14100, Data Step: 70500, Loss: 1.4140625, Token per second per gpu: 20016.440618201872
Epoch: 0, Global Step: 14110, Data Step: 70550, Loss: 1.609375, Token per second per gpu: 20232.925167822203
Epoch: 0, Global Step: 14120, Data Step: 70600, Loss: 1.4609375, Token per second per gpu: 19996.81508603622
Epoch: 0, Global Step: 14130, Data Step: 70650, Loss: 1.5078125, Token per second per gpu: 19800.72808592248
Epoch: 0, Global Step: 14140, Data Step: 70700, Loss: 1.5546875, Token per second per gpu: 20323.666223104203
Epoch: 0, Global Step: 14150, Data Step: 70750, Loss: 1.765625, Token per second per gpu: 19752.816913411352
Epoch: 0, Global Step: 14160, Data Step: 70800, Loss: 1.609375, Token per second per gpu: 19933.875725768004
Epoch: 0, Global Step: 14170, Data Step: 70850, Loss: 1.6171875, Token per second per gpu: 20227.37635499375
Epoch: 0, Global Step: 14180, Data Step: 70900, Loss: 1.2109375, Token per second per gpu: 19759.74022980601
Epoch: 0, Global Step: 14190, Data Step: 70950, Loss: 1.4609375, Token per second per gpu: 19815.834595512413
Epoch: 0, Global Step: 14200, Data Step: 71000, Loss: 1.109375, Token per second per gpu: 19996.5121347721
Epoch: 0, Global Step: 14210, Data Step: 71050, Loss: 1.3125, Token per second per gpu: 20049.27582818027
Epoch: 0, Global Step: 14220, Data Step: 71100, Loss: 1.453125, Token per second per gpu: 19974.11286894874
Epoch: 0, Global Step: 14230, Data Step: 71150, Loss: 1.6796875, Token per second per gpu: 19865.88329519224
Epoch: 0, Global Step: 14240, Data Step: 71200, Loss: 1.8515625, Token per second per gpu: 20206.682466170085
Epoch: 0, Global Step: 14250, Data Step: 71250, Loss: 1.3046875, Token per second per gpu: 20109.57171492615
Epoch: 0, Global Step: 14260, Data Step: 71300, Loss: 1.34375, Token per second per gpu: 19814.999737766437
Epoch: 0, Global Step: 14270, Data Step: 71350, Loss: 1.1171875, Token per second per gpu: 19967.521990080953
Epoch: 0, Global Step: 14280, Data Step: 71400, Loss: 1.59375, Token per second per gpu: 20195.62575308628
Epoch: 0, Global Step: 14290, Data Step: 71450, Loss: 1.375, Token per second per gpu: 19893.544760047655
Epoch: 0, Global Step: 14300, Data Step: 71500, Loss: 1.2734375, Token per second per gpu: 19863.570391442685
Epoch: 0, Global Step: 14310, Data Step: 71550, Loss: 1.40625, Token per second per gpu: 20152.921703214975
Epoch: 0, Global Step: 14320, Data Step: 71600, Loss: 1.5546875, Token per second per gpu: 19776.957577536603
Epoch: 0, Global Step: 14330, Data Step: 71650, Loss: 1.0625, Token per second per gpu: 19717.75789196529
Epoch: 0, Global Step: 14340, Data Step: 71700, Loss: 1.3359375, Token per second per gpu: 19821.659895883757
Epoch: 0, Global Step: 14350, Data Step: 71750, Loss: 1.2890625, Token per second per gpu: 19989.96657075009
Epoch: 0, Global Step: 14360, Data Step: 71800, Loss: 1.1796875, Token per second per gpu: 19678.73763057068
Epoch: 0, Global Step: 14370, Data Step: 71850, Loss: 1.265625, Token per second per gpu: 19688.91757446715
Epoch: 0, Global Step: 14380, Data Step: 71900, Loss: 1.125, Token per second per gpu: 20238.012792387442
Epoch: 0, Global Step: 14390, Data Step: 71950, Loss: 1.4765625, Token per second per gpu: 19777.952803376007
Epoch: 0, Global Step: 14400, Data Step: 72000, Loss: 2.5, Token per second per gpu: 19914.04216061556
Epoch: 0, Global Step: 14410, Data Step: 72050, Loss: 2.875, Token per second per gpu: 20217.121278420924
Epoch: 0, Global Step: 14420, Data Step: 72100, Loss: 2.28125, Token per second per gpu: 19679.079359197378
Epoch: 0, Global Step: 14430, Data Step: 72150, Loss: 2.453125, Token per second per gpu: 19831.524747608797
Epoch: 0, Global Step: 14440, Data Step: 72200, Loss: 2.28125, Token per second per gpu: 19873.5147164659
Epoch: 0, Global Step: 14450, Data Step: 72250, Loss: 2.234375, Token per second per gpu: 20205.17576524499
Epoch: 0, Global Step: 14460, Data Step: 72300, Loss: 1.9296875, Token per second per gpu: 19571.11123774838
Epoch: 0, Global Step: 14470, Data Step: 72350, Loss: 1.8359375, Token per second per gpu: 19790.802099623335
Epoch: 0, Global Step: 14480, Data Step: 72400, Loss: 1.8359375, Token per second per gpu: 19749.792984266474
Epoch: 0, Global Step: 14490, Data Step: 72450, Loss: 1.875, Token per second per gpu: 19885.39745670173
Epoch: 0, Global Step: 14500, Data Step: 72500, Loss: 1.984375, Token per second per gpu: 19972.56189011609
Epoch: 0, Global Step: 14510, Data Step: 72550, Loss: 1.9296875, Token per second per gpu: 20158.69409042186
Epoch: 0, Global Step: 14520, Data Step: 72600, Loss: 1.734375, Token per second per gpu: 19882.364094882345
Epoch: 0, Global Step: 14530, Data Step: 72650, Loss: 1.84375, Token per second per gpu: 20343.32584582722
Epoch: 0, Global Step: 14540, Data Step: 72700, Loss: 1.828125, Token per second per gpu: 19934.924928817618
Epoch: 0, Global Step: 14550, Data Step: 72750, Loss: 1.984375, Token per second per gpu: 19919.116806256447
Epoch: 0, Global Step: 14560, Data Step: 72800, Loss: 2.03125, Token per second per gpu: 19653.808347213053
Epoch: 0, Global Step: 14570, Data Step: 72850, Loss: 1.765625, Token per second per gpu: 20147.005189842384
Epoch: 0, Global Step: 14580, Data Step: 72900, Loss: 1.5390625, Token per second per gpu: 19933.627227008055
Epoch: 0, Global Step: 14590, Data Step: 72950, Loss: 1.71875, Token per second per gpu: 19878.858176721682
Epoch: 0, Global Step: 14600, Data Step: 73000, Loss: 1.671875, Token per second per gpu: 19976.482993530073
Epoch: 0, Global Step: 14610, Data Step: 73050, Loss: 1.90625, Token per second per gpu: 20334.25478114759
Epoch: 0, Global Step: 14620, Data Step: 73100, Loss: 1.71875, Token per second per gpu: 19870.13583467077
Epoch: 0, Global Step: 14630, Data Step: 73150, Loss: 1.7421875, Token per second per gpu: 19795.1234565574
Epoch: 0, Global Step: 14640, Data Step: 73200, Loss: 1.7109375, Token per second per gpu: 19873.197834263025
Epoch: 0, Global Step: 14650, Data Step: 73250, Loss: 1.7734375, Token per second per gpu: 19760.696993017376
Epoch: 0, Global Step: 14660, Data Step: 73300, Loss: 1.53125, Token per second per gpu: 20333.6559915375
Epoch: 0, Global Step: 14670, Data Step: 73350, Loss: 1.9453125, Token per second per gpu: 19804.901631970442
Epoch: 0, Global Step: 14680, Data Step: 73400, Loss: 1.765625, Token per second per gpu: 19683.924706943544
Epoch: 0, Global Step: 14690, Data Step: 73450, Loss: 1.6171875, Token per second per gpu: 19846.361909126794
Epoch: 0, Global Step: 14700, Data Step: 73500, Loss: 1.7109375, Token per second per gpu: 20337.541820720577
Epoch: 0, Global Step: 14710, Data Step: 73550, Loss: 1.78125, Token per second per gpu: 19780.609111444403
Epoch: 0, Global Step: 14720, Data Step: 73600, Loss: 1.8984375, Token per second per gpu: 19893.223553152056
Epoch: 0, Global Step: 14730, Data Step: 73650, Loss: 1.4921875, Token per second per gpu: 19920.477109588097
Epoch: 0, Global Step: 14740, Data Step: 73700, Loss: 1.7421875, Token per second per gpu: 20338.81039489993
Epoch: 0, Global Step: 14750, Data Step: 73750, Loss: 1.703125, Token per second per gpu: 19902.293388154783
Epoch: 0, Global Step: 14760, Data Step: 73800, Loss: 1.5703125, Token per second per gpu: 19764.94863669688
Epoch: 0, Global Step: 14770, Data Step: 73850, Loss: 1.7890625, Token per second per gpu: 19990.83652173701
Epoch: 0, Global Step: 14780, Data Step: 73900, Loss: 1.6640625, Token per second per gpu: 19668.402658745003
Epoch: 0, Global Step: 14790, Data Step: 73950, Loss: 2.0, Token per second per gpu: 20221.5927538518
Epoch: 0, Global Step: 14800, Data Step: 74000, Loss: 1.5859375, Token per second per gpu: 19894.979905732005
Epoch: 0, Global Step: 14810, Data Step: 74050, Loss: 1.59375, Token per second per gpu: 19890.080400994484
Epoch: 0, Global Step: 14820, Data Step: 74100, Loss: 1.921875, Token per second per gpu: 19946.046976395886
Epoch: 0, Global Step: 14830, Data Step: 74150, Loss: 2.046875, Token per second per gpu: 19975.99613935238
Epoch: 0, Global Step: 14840, Data Step: 74200, Loss: 1.7109375, Token per second per gpu: 19997.07782501987
Epoch: 0, Global Step: 14850, Data Step: 74250, Loss: 1.515625, Token per second per gpu: 19686.695141499324
Epoch: 0, Global Step: 14860, Data Step: 74300, Loss: 1.5390625, Token per second per gpu: 19841.814289811107
Epoch: 0, Global Step: 14870, Data Step: 74350, Loss: 1.5234375, Token per second per gpu: 20282.41351281629
Epoch: 0, Global Step: 14880, Data Step: 74400, Loss: 1.5390625, Token per second per gpu: 19711.694587221074
Epoch: 0, Global Step: 14890, Data Step: 74450, Loss: 1.8046875, Token per second per gpu: 19608.037218063724
Epoch: 0, Global Step: 14900, Data Step: 74500, Loss: 1.78125, Token per second per gpu: 19721.76195957874
Epoch: 0, Global Step: 14910, Data Step: 74550, Loss: 1.7265625, Token per second per gpu: 20216.780021297305
Epoch: 0, Global Step: 14920, Data Step: 74600, Loss: 1.6171875, Token per second per gpu: 19905.68210933281
Epoch: 0, Global Step: 14930, Data Step: 74650, Loss: 1.3203125, Token per second per gpu: 19724.3664177572
Epoch: 0, Global Step: 14940, Data Step: 74700, Loss: 1.3046875, Token per second per gpu: 20216.269774886663
Epoch: 0, Global Step: 14950, Data Step: 74750, Loss: 1.671875, Token per second per gpu: 19595.304810594997
Epoch: 0, Global Step: 14960, Data Step: 74800, Loss: 1.2109375, Token per second per gpu: 20049.32674221597
Epoch: 0, Global Step: 14970, Data Step: 74850, Loss: 1.4140625, Token per second per gpu: 19639.078883880564
Epoch: 0, Global Step: 14980, Data Step: 74900, Loss: 1.4453125, Token per second per gpu: 20181.98896345521
Epoch: 0, Global Step: 14990, Data Step: 74950, Loss: 1.25, Token per second per gpu: 19702.090996316227
Epoch: 0, Global Step: 15000, Data Step: 75000, Loss: 1.3671875, Token per second per gpu: 19673.442624452648
I0407 08:53:54.506212 140235438175232 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2_instruction
I0407 08:53:54.506633 140235438175232 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-07 08:53:54,507] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-07 08:53:54,513] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2_instruction/pytorch_model/mp_rank_00_model_states.pt
[2024-04-07 08:53:54,513] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-07 08:53:55,160] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-07 08:53:55,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-07 08:53:55,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-07 08:53:55,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-07 08:53:55,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-07 08:53:55,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-07 08:53:55,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-07 08:53:57,371] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-07 08:53:57,371] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-07 08:53:57,372] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 08:53:57,525] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-07 08:53:57,525] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-07 08:53:57,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 08:53:57,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-07 08:53:57,550] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-07 08:53:57,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-07 08:53:57,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 08:53:57,550] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-07 08:53:57,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 08:53:57,568] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-07 08:53:57,568] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-07 08:53:57,568] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-07 08:53:57,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-07 08:53:57,568] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2_instruction/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-07 08:53:57,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0407 08:53:57.569501 140235438175232 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2_instruction/pytorch_model
I0407 08:53:57.570115 140235438175232 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2_instruction/scheduler.bin
I0407 08:53:57.570328 140235438175232 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2_instruction/sampler.bin
I0407 08:53:57.571532 140235438175232 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2_instruction/random_states_0.pkl
Epoch: 0, Global Step: 15010, Data Step: 75050, Loss: 1.328125, Token per second per gpu: 17734.36020258698
Epoch: 0, Global Step: 15020, Data Step: 75100, Loss: 1.4921875, Token per second per gpu: 20140.233438290306
Epoch: 0, Global Step: 15030, Data Step: 75150, Loss: 1.2109375, Token per second per gpu: 19785.00968459414
Epoch: 0, Global Step: 15040, Data Step: 75200, Loss: 1.546875, Token per second per gpu: 19828.679715289938
Epoch: 0, Global Step: 15050, Data Step: 75250, Loss: 1.3515625, Token per second per gpu: 20138.433707067685
Epoch: 0, Global Step: 15060, Data Step: 75300, Loss: 1.5, Token per second per gpu: 19882.090740108895
Epoch: 0, Global Step: 15070, Data Step: 75350, Loss: 1.3359375, Token per second per gpu: 19800.517218342782
Epoch: 0, Global Step: 15080, Data Step: 75400, Loss: 1.5078125, Token per second per gpu: 20123.32009291563
Epoch: 0, Global Step: 15090, Data Step: 75450, Loss: 1.5546875, Token per second per gpu: 19594.31357832218
Epoch: 0, Global Step: 15100, Data Step: 75500, Loss: 1.3828125, Token per second per gpu: 19793.914315395967
Epoch: 0, Global Step: 15110, Data Step: 75550, Loss: 1.21875, Token per second per gpu: 19875.55896906676
Epoch: 0, Global Step: 15120, Data Step: 75600, Loss: 1.28125, Token per second per gpu: 20121.755660770832
Epoch: 0, Global Step: 15130, Data Step: 75650, Loss: 1.7109375, Token per second per gpu: 19825.479869354494
Epoch: 0, Global Step: 15140, Data Step: 75700, Loss: 1.484375, Token per second per gpu: 19695.555189736686
Epoch: 0, Global Step: 15150, Data Step: 75750, Loss: 1.46875, Token per second per gpu: 19873.33631955428
Epoch: 0, Global Step: 15160, Data Step: 75800, Loss: 1.015625, Token per second per gpu: 19885.130647229125
Epoch: 0, Global Step: 15170, Data Step: 75850, Loss: 1.5703125, Token per second per gpu: 19722.09793915043
Epoch: 0, Global Step: 15180, Data Step: 75900, Loss: 1.4453125, Token per second per gpu: 19926.526260123217
Epoch: 0, Global Step: 15190, Data Step: 75950, Loss: 1.625, Token per second per gpu: 19974.12475904488
Epoch: 0, Global Step: 15200, Data Step: 76000, Loss: 1.6796875, Token per second per gpu: 19823.584795505772
Epoch: 0, Global Step: 15210, Data Step: 76050, Loss: 1.7421875, Token per second per gpu: 19647.322145664937
Epoch: 0, Global Step: 15220, Data Step: 76100, Loss: 1.625, Token per second per gpu: 19670.63862096186
Epoch: 0, Global Step: 15230, Data Step: 76150, Loss: 1.2890625, Token per second per gpu: 20297.449818438283
Epoch: 0, Global Step: 15240, Data Step: 76200, Loss: 1.4453125, Token per second per gpu: 19702.88653935517
Epoch: 0, Global Step: 15250, Data Step: 76250, Loss: 1.1640625, Token per second per gpu: 19701.997906975917
Epoch: 0, Global Step: 15260, Data Step: 76300, Loss: 1.1640625, Token per second per gpu: 19963.521260647354
Epoch: 0, Global Step: 15270, Data Step: 76350, Loss: 1.25, Token per second per gpu: 19955.80833208502
Epoch: 0, Global Step: 15280, Data Step: 76400, Loss: 1.0625, Token per second per gpu: 19703.65412484437
Epoch: 0, Global Step: 15290, Data Step: 76450, Loss: 1.3125, Token per second per gpu: 19733.028315516975
Epoch: 0, Global Step: 15300, Data Step: 76500, Loss: 1.3125, Token per second per gpu: 19721.010529722316
Epoch: 0, Global Step: 15310, Data Step: 76550, Loss: 1.15625, Token per second per gpu: 19743.290840820435
Epoch: 0, Global Step: 15320, Data Step: 76600, Loss: 1.8828125, Token per second per gpu: 20061.656296416288
Epoch: 0, Global Step: 15330, Data Step: 76650, Loss: 1.171875, Token per second per gpu: 19867.360770464617
Epoch: 0, Global Step: 15340, Data Step: 76700, Loss: 1.0859375, Token per second per gpu: 19890.886776570234
Epoch: 0, Global Step: 15350, Data Step: 76750, Loss: 1.28125, Token per second per gpu: 19833.54498357525
Epoch: 0, Global Step: 15360, Data Step: 76800, Loss: 1.3359375, Token per second per gpu: 19692.246296188092
Epoch: 0, Global Step: 15370, Data Step: 76850, Loss: 1.3515625, Token per second per gpu: 20267.85586923871
Epoch: 0, Global Step: 15380, Data Step: 76900, Loss: 1.4609375, Token per second per gpu: 19891.60164440839
Epoch: 0, Global Step: 15390, Data Step: 76950, Loss: 1.5234375, Token per second per gpu: 20676.01910226458
Epoch: 0, Global Step: 15400, Data Step: 77000, Loss: 1.1015625, Token per second per gpu: 21068.282878703798
Epoch: 0, Global Step: 15410, Data Step: 77050, Loss: 1.171875, Token per second per gpu: 20624.889724429784
Epoch: 0, Global Step: 15420, Data Step: 77100, Loss: 1.09375, Token per second per gpu: 19705.424174253396
Epoch: 0, Global Step: 15430, Data Step: 77150, Loss: 1.4375, Token per second per gpu: 19777.818376465715
Epoch: 0, Global Step: 15440, Data Step: 77200, Loss: 1.109375, Token per second per gpu: 19775.671254473156
Epoch: 0, Global Step: 15450, Data Step: 77250, Loss: 1.203125, Token per second per gpu: 19835.739136624197
Epoch: 0, Global Step: 15460, Data Step: 77300, Loss: 1.2890625, Token per second per gpu: 20085.250476905574
Epoch: 0, Global Step: 15470, Data Step: 77350, Loss: 1.6015625, Token per second per gpu: 20001.164668071742
Epoch: 0, Global Step: 15480, Data Step: 77400, Loss: 1.171875, Token per second per gpu: 19862.91816432052
Epoch: 0, Global Step: 15490, Data Step: 77450, Loss: 1.2734375, Token per second per gpu: 20081.581196187966
Epoch: 0, Global Step: 15500, Data Step: 77500, Loss: 1.3359375, Token per second per gpu: 20089.82486824927
Epoch: 0, Global Step: 15510, Data Step: 77550, Loss: 1.609375, Token per second per gpu: 19972.782568057904
Epoch: 0, Global Step: 15520, Data Step: 77600, Loss: 1.3828125, Token per second per gpu: 19810.569184726188
Epoch: 0, Global Step: 15530, Data Step: 77650, Loss: 0.98828125, Token per second per gpu: 19668.931021316766
Epoch: 0, Global Step: 15540, Data Step: 77700, Loss: 1.296875, Token per second per gpu: 20122.978978078285
Epoch: 0, Global Step: 15550, Data Step: 77750, Loss: 1.2421875, Token per second per gpu: 19712.735731347377
Epoch: 0, Global Step: 15560, Data Step: 77800, Loss: 1.3515625, Token per second per gpu: 19961.783958850396
Epoch: 0, Global Step: 15570, Data Step: 77850, Loss: 1.4296875, Token per second per gpu: 20073.608853027134
Epoch: 0, Global Step: 15580, Data Step: 77900, Loss: 1.1640625, Token per second per gpu: 20038.70722023027
Epoch: 0, Global Step: 15590, Data Step: 77950, Loss: 1.3125, Token per second per gpu: 19790.444989816024
Epoch: 0, Global Step: 15600, Data Step: 78000, Loss: 1.4296875, Token per second per gpu: 19765.237153450627
Epoch: 0, Global Step: 15610, Data Step: 78050, Loss: 1.3828125, Token per second per gpu: 20247.98590642706
Epoch: 0, Global Step: 15620, Data Step: 78100, Loss: 1.2421875, Token per second per gpu: 19938.879769960677
Epoch: 0, Global Step: 15630, Data Step: 78150, Loss: 1.5, Token per second per gpu: 19756.03732768614
Epoch: 0, Global Step: 15640, Data Step: 78200, Loss: 1.1484375, Token per second per gpu: 19735.27337403153
Epoch: 0, Global Step: 15650, Data Step: 78250, Loss: 1.203125, Token per second per gpu: 19856.749296215043
Epoch: 0, Global Step: 15660, Data Step: 78300, Loss: 1.0625, Token per second per gpu: 20069.172380092816
Epoch: 0, Global Step: 15670, Data Step: 78350, Loss: 1.328125, Token per second per gpu: 19721.37582361133
Epoch: 0, Global Step: 15680, Data Step: 78400, Loss: 1.1875, Token per second per gpu: 19685.087967117634
Epoch: 0, Global Step: 15690, Data Step: 78450, Loss: 1.328125, Token per second per gpu: 19907.387329401663
Epoch: 0, Global Step: 15700, Data Step: 78500, Loss: 1.1640625, Token per second per gpu: 19965.84506363942
Epoch: 0, Global Step: 15710, Data Step: 78550, Loss: 1.03125, Token per second per gpu: 20166.127219829767
Epoch: 0, Global Step: 15720, Data Step: 78600, Loss: 1.28125, Token per second per gpu: 19898.935875568463
Epoch: 0, Global Step: 15730, Data Step: 78650, Loss: 1.234375, Token per second per gpu: 19987.032370499543
Epoch: 0, Global Step: 15740, Data Step: 78700, Loss: 1.34375, Token per second per gpu: 20037.92041323424
Epoch: 0, Global Step: 15750, Data Step: 78750, Loss: 1.421875, Token per second per gpu: 20204.802594423665
Epoch: 0, Global Step: 15760, Data Step: 78800, Loss: 1.40625, Token per second per gpu: 19591.79714994155
Epoch: 0, Global Step: 15770, Data Step: 78850, Loss: 1.2734375, Token per second per gpu: 19894.686851585182
Epoch: 0, Global Step: 15780, Data Step: 78900, Loss: 0.9921875, Token per second per gpu: 20310.285723397545
Epoch: 0, Global Step: 15790, Data Step: 78950, Loss: 1.640625, Token per second per gpu: 19742.69658311215
Epoch: 0, Global Step: 15800, Data Step: 79000, Loss: 1.5390625, Token per second per gpu: 19737.802306045432
Epoch: 0, Global Step: 15810, Data Step: 79050, Loss: 1.5078125, Token per second per gpu: 19894.680953728715
Epoch: 0, Global Step: 15820, Data Step: 79100, Loss: 1.3984375, Token per second per gpu: 19770.87692760498
Epoch: 0, Global Step: 15830, Data Step: 79150, Loss: 1.3515625, Token per second per gpu: 20177.0932578258
Epoch: 0, Global Step: 15840, Data Step: 79200, Loss: 1.3046875, Token per second per gpu: 19989.451707085645
Epoch: 0, Global Step: 15850, Data Step: 79250, Loss: 1.0625, Token per second per gpu: 19938.638921600268
Epoch: 0, Global Step: 15860, Data Step: 79300, Loss: 1.296875, Token per second per gpu: 19746.664666391698
Epoch: 0, Global Step: 15870, Data Step: 79350, Loss: 1.234375, Token per second per gpu: 19826.85615247357
Epoch: 0, Global Step: 15880, Data Step: 79400, Loss: 1.21875, Token per second per gpu: 20223.955697606812
Epoch: 0, Global Step: 15890, Data Step: 79450, Loss: 0.96875, Token per second per gpu: 19772.211779264017
Epoch: 0, Global Step: 15900, Data Step: 79500, Loss: 1.1328125, Token per second per gpu: 20280.845507379338
Epoch: 0, Global Step: 15910, Data Step: 79550, Loss: 1.296875, Token per second per gpu: 19762.39292071922
Epoch: 0, Global Step: 15920, Data Step: 79600, Loss: 1.2578125, Token per second per gpu: 19990.701604591133
Epoch: 0, Global Step: 15930, Data Step: 79650, Loss: 1.3671875, Token per second per gpu: 20275.305414278533
Epoch: 0, Global Step: 15940, Data Step: 79700, Loss: 1.0859375, Token per second per gpu: 19587.121745628527
Epoch: 0, Global Step: 15950, Data Step: 79750, Loss: 1.1171875, Token per second per gpu: 20197.23360593583
Epoch: 0, Global Step: 15960, Data Step: 79800, Loss: 1.09375, Token per second per gpu: 19820.62020305035
Epoch: 0, Global Step: 15970, Data Step: 79850, Loss: 1.0859375, Token per second per gpu: 20128.3612287675
Epoch: 0, Global Step: 15980, Data Step: 79900, Loss: 1.046875, Token per second per gpu: 19938.28478696173
Epoch: 0, Global Step: 15990, Data Step: 79950, Loss: 1.34375, Token per second per gpu: 19788.99643802724
Epoch: 0, Global Step: 16000, Data Step: 80000, Loss: 1.234375, Token per second per gpu: 20176.590889328418
Epoch: 0, Global Step: 16010, Data Step: 80050, Loss: 1.3984375, Token per second per gpu: 20068.73688641741
Epoch: 0, Global Step: 16020, Data Step: 80100, Loss: 1.3671875, Token per second per gpu: 19812.37787155964
Epoch: 0, Global Step: 16030, Data Step: 80150, Loss: 1.625, Token per second per gpu: 20046.737754355832
Epoch: 0, Global Step: 16040, Data Step: 80200, Loss: 1.1875, Token per second per gpu: 19976.466454964433
Epoch: 0, Global Step: 16050, Data Step: 80250, Loss: 1.1796875, Token per second per gpu: 20065.35204580179
Epoch: 0, Global Step: 16060, Data Step: 80300, Loss: 1.0703125, Token per second per gpu: 19836.49108967594
Epoch: 0, Global Step: 16070, Data Step: 80350, Loss: 1.5546875, Token per second per gpu: 19836.226140648494
Epoch: 0, Global Step: 16080, Data Step: 80400, Loss: 1.4375, Token per second per gpu: 20204.558700617294
Epoch: 0, Global Step: 16090, Data Step: 80450, Loss: 1.1328125, Token per second per gpu: 19911.802957342967
Epoch: 0, Global Step: 16100, Data Step: 80500, Loss: 1.171875, Token per second per gpu: 19905.051652310758
Epoch: 0, Global Step: 16110, Data Step: 80550, Loss: 1.203125, Token per second per gpu: 19816.325559820998
Epoch: 0, Global Step: 16120, Data Step: 80600, Loss: 1.2421875, Token per second per gpu: 19973.585445956403
Epoch: 0, Global Step: 16130, Data Step: 80650, Loss: 1.078125, Token per second per gpu: 19947.638679942815
Epoch: 0, Global Step: 16140, Data Step: 80700, Loss: 1.1171875, Token per second per gpu: 19764.664494129247
Epoch: 0, Global Step: 16150, Data Step: 80750, Loss: 1.296875, Token per second per gpu: 19752.65811865679
Epoch: 0, Global Step: 16160, Data Step: 80800, Loss: 1.3359375, Token per second per gpu: 19964.28608847367
Epoch: 0, Global Step: 16170, Data Step: 80850, Loss: 1.03125, Token per second per gpu: 20227.08504829238
Epoch: 0, Global Step: 16180, Data Step: 80900, Loss: 1.1796875, Token per second per gpu: 19888.276103051783
Epoch: 0, Global Step: 16190, Data Step: 80950, Loss: 0.98046875, Token per second per gpu: 19856.677690204102
Epoch: 0, Global Step: 16200, Data Step: 81000, Loss: 1.2421875, Token per second per gpu: 20334.625239708945
Epoch: 0, Global Step: 16210, Data Step: 81050, Loss: 1.125, Token per second per gpu: 19762.1037595556
Epoch: 0, Global Step: 16220, Data Step: 81100, Loss: 1.3671875, Token per second per gpu: 19834.253720523328
Epoch: 0, Global Step: 16230, Data Step: 81150, Loss: 1.1796875, Token per second per gpu: 19967.12282894007
Epoch: 0, Global Step: 16240, Data Step: 81200, Loss: 1.1484375, Token per second per gpu: 19773.529514512556
Epoch: 0, Global Step: 16250, Data Step: 81250, Loss: 1.2734375, Token per second per gpu: 19856.723040617733
Epoch: 0, Global Step: 16260, Data Step: 81300, Loss: 1.34375, Token per second per gpu: 19956.95517311264
Epoch: 0, Global Step: 16270, Data Step: 81350, Loss: 1.15625, Token per second per gpu: 20240.36832001496
Epoch: 0, Global Step: 16280, Data Step: 81400, Loss: 0.9921875, Token per second per gpu: 19464.3039078674
Epoch: 0, Global Step: 16290, Data Step: 81450, Loss: 1.1796875, Token per second per gpu: 19829.03198111571
Epoch: 0, Global Step: 16300, Data Step: 81500, Loss: 1.15625, Token per second per gpu: 20096.856524496256
Epoch: 0, Global Step: 16310, Data Step: 81550, Loss: 1.40625, Token per second per gpu: 19933.589480903007
Epoch: 0, Global Step: 16320, Data Step: 81600, Loss: 1.1171875, Token per second per gpu: 19752.008249568393
Epoch: 0, Global Step: 16330, Data Step: 81650, Loss: 1.203125, Token per second per gpu: 19931.976893581228
Epoch: 0, Global Step: 16340, Data Step: 81700, Loss: 1.34375, Token per second per gpu: 19786.292665203695
Epoch: 0, Global Step: 16350, Data Step: 81750, Loss: 1.375, Token per second per gpu: 20009.60046976098
Epoch: 0, Global Step: 16360, Data Step: 81800, Loss: 1.0859375, Token per second per gpu: 19809.39945311989
Epoch: 0, Global Step: 16370, Data Step: 81850, Loss: 0.9453125, Token per second per gpu: 19782.968342081127
Epoch: 0, Global Step: 16380, Data Step: 81900, Loss: 1.3359375, Token per second per gpu: 19992.386803896447
Epoch: 0, Global Step: 16390, Data Step: 81950, Loss: 1.21875, Token per second per gpu: 20131.532400384927
Epoch: 0, Global Step: 16400, Data Step: 82000, Loss: 1.1328125, Token per second per gpu: 19722.980234346607
Epoch: 0, Global Step: 16410, Data Step: 82050, Loss: 1.296875, Token per second per gpu: 19587.562493579266
Epoch: 0, Global Step: 16420, Data Step: 82100, Loss: 1.53125, Token per second per gpu: 19704.06090070164
Epoch: 0, Global Step: 16430, Data Step: 82150, Loss: 1.3671875, Token per second per gpu: 20111.005240729068
Epoch: 0, Global Step: 16440, Data Step: 82200, Loss: 1.265625, Token per second per gpu: 19738.800852145458
Epoch: 0, Global Step: 16450, Data Step: 82250, Loss: 1.21875, Token per second per gpu: 19803.125902270946
Epoch: 0, Global Step: 16460, Data Step: 82300, Loss: 0.921875, Token per second per gpu: 20070.71757861879
Epoch: 0, Global Step: 16470, Data Step: 82350, Loss: 1.078125, Token per second per gpu: 19859.157578570706
Epoch: 0, Global Step: 16480, Data Step: 82400, Loss: 1.09375, Token per second per gpu: 19851.776109640694
Epoch: 0, Global Step: 16490, Data Step: 82450, Loss: 0.91015625, Token per second per gpu: 20142.040103402658
Epoch: 0, Global Step: 16500, Data Step: 82500, Loss: 1.4765625, Token per second per gpu: 19640.999381534748
Epoch: 0, Global Step: 16510, Data Step: 82550, Loss: 1.1171875, Token per second per gpu: 19875.02588516242
Epoch: 0, Global Step: 16520, Data Step: 82600, Loss: 1.0, Token per second per gpu: 19623.836365035815
Epoch: 0, Global Step: 16530, Data Step: 82650, Loss: 1.40625, Token per second per gpu: 20155.175365204468
Epoch: 0, Global Step: 16540, Data Step: 82700, Loss: 1.171875, Token per second per gpu: 19896.09174689099
Epoch: 0, Global Step: 16550, Data Step: 82750, Loss: 1.40625, Token per second per gpu: 19878.014136184993
Epoch: 0, Global Step: 16560, Data Step: 82800, Loss: 1.421875, Token per second per gpu: 19987.319594219793
Epoch: 0, Global Step: 16570, Data Step: 82850, Loss: 1.359375, Token per second per gpu: 19965.54750499089
Epoch: 0, Global Step: 16580, Data Step: 82900, Loss: 0.91015625, Token per second per gpu: 19742.639954433915
Epoch: 0, Global Step: 16590, Data Step: 82950, Loss: 1.3203125, Token per second per gpu: 19736.75669854769
Epoch: 0, Global Step: 16600, Data Step: 83000, Loss: 1.1875, Token per second per gpu: 20038.655425189307
Epoch: 0, Global Step: 16610, Data Step: 83050, Loss: 1.1328125, Token per second per gpu: 19706.40136329143
Epoch: 0, Global Step: 16620, Data Step: 83100, Loss: 1.4765625, Token per second per gpu: 19889.443379199834
Epoch: 0, Global Step: 16630, Data Step: 83150, Loss: 1.140625, Token per second per gpu: 19913.49463841186
Epoch: 0, Global Step: 16640, Data Step: 83200, Loss: 1.1015625, Token per second per gpu: 19877.44614663632
Epoch: 0, Global Step: 16650, Data Step: 83250, Loss: 1.4296875, Token per second per gpu: 19794.452544690037
Epoch: 0, Global Step: 16660, Data Step: 83300, Loss: 1.2265625, Token per second per gpu: 19739.49339884254
Epoch: 0, Global Step: 16670, Data Step: 83350, Loss: 1.2109375, Token per second per gpu: 19795.06908126131
Epoch: 0, Global Step: 16680, Data Step: 83400, Loss: 1.40625, Token per second per gpu: 19851.370552294295
Epoch: 0, Global Step: 16690, Data Step: 83450, Loss: 1.5, Token per second per gpu: 19957.90757417514
Epoch: 0, Global Step: 16700, Data Step: 83500, Loss: 1.71875, Token per second per gpu: 19634.251637246198
Epoch: 0, Global Step: 16710, Data Step: 83550, Loss: 1.1015625, Token per second per gpu: 19809.984027542574
Epoch: 0, Global Step: 16720, Data Step: 83600, Loss: 1.109375, Token per second per gpu: 20027.612370356244
Epoch: 0, Global Step: 16730, Data Step: 83650, Loss: 1.21875, Token per second per gpu: 19852.946997127612
Epoch: 0, Global Step: 16740, Data Step: 83700, Loss: 0.9921875, Token per second per gpu: 19517.664602534842
Epoch: 0, Global Step: 16750, Data Step: 83750, Loss: 1.03125, Token per second per gpu: 19636.712906090313
Epoch: 0, Global Step: 16760, Data Step: 83800, Loss: 1.15625, Token per second per gpu: 20268.137830389325
Epoch: 0, Global Step: 16770, Data Step: 83850, Loss: 1.125, Token per second per gpu: 19853.581499660457
Epoch: 0, Global Step: 16780, Data Step: 83900, Loss: 1.2109375, Token per second per gpu: 19633.01593917736
Epoch: 0, Global Step: 16790, Data Step: 83950, Loss: 1.1875, Token per second per gpu: 19765.76509178969
Epoch: 0, Global Step: 16800, Data Step: 84000, Loss: 1.15625, Token per second per gpu: 20189.79461524723
Epoch: 0, Global Step: 16810, Data Step: 84050, Loss: 1.3046875, Token per second per gpu: 19710.077901521094
Epoch: 0, Global Step: 16820, Data Step: 84100, Loss: 1.1015625, Token per second per gpu: 19942.568753539195
Epoch: 0, Global Step: 16830, Data Step: 84150, Loss: 1.078125, Token per second per gpu: 20319.722437763787
Epoch: 0, Global Step: 16840, Data Step: 84200, Loss: 1.0703125, Token per second per gpu: 19834.696134343918
Epoch: 0, Global Step: 16850, Data Step: 84250, Loss: 0.90625, Token per second per gpu: 19814.26239344955
Epoch: 0, Global Step: 16860, Data Step: 84300, Loss: 1.015625, Token per second per gpu: 19813.64813411079
Epoch: 0, Global Step: 16870, Data Step: 84350, Loss: 1.15625, Token per second per gpu: 20220.34694613235
Epoch: 0, Global Step: 16880, Data Step: 84400, Loss: 0.96875, Token per second per gpu: 19820.10506221613
Epoch: 0, Global Step: 16890, Data Step: 84450, Loss: 1.046875, Token per second per gpu: 20065.61621392623
Epoch: 0, Global Step: 16900, Data Step: 84500, Loss: 1.296875, Token per second per gpu: 19930.640175830515
Epoch: 0, Global Step: 16910, Data Step: 84550, Loss: 1.078125, Token per second per gpu: 20362.55666451974
Epoch: 0, Global Step: 16920, Data Step: 84600, Loss: 1.1875, Token per second per gpu: 19890.209910416314
Epoch: 0, Global Step: 16930, Data Step: 84650, Loss: 1.2734375, Token per second per gpu: 19876.12795455454
Epoch: 0, Global Step: 16940, Data Step: 84700, Loss: 1.1640625, Token per second per gpu: 19715.627410161997
Epoch: 0, Global Step: 16950, Data Step: 84750, Loss: 1.296875, Token per second per gpu: 20250.684619801465
Epoch: 0, Global Step: 16960, Data Step: 84800, Loss: 1.2109375, Token per second per gpu: 20120.751550600307
Epoch: 0, Global Step: 16970, Data Step: 84850, Loss: 1.3515625, Token per second per gpu: 19796.196243699622
Epoch: 0, Global Step: 16980, Data Step: 84900, Loss: 1.1640625, Token per second per gpu: 20332.594622532662
Epoch: 0, Global Step: 16990, Data Step: 84950, Loss: 0.96875, Token per second per gpu: 19778.294343379483
Epoch: 0, Global Step: 17000, Data Step: 85000, Loss: 1.25, Token per second per gpu: 19924.566159332164
Epoch: 0, Global Step: 17010, Data Step: 85050, Loss: 1.171875, Token per second per gpu: 19975.732281737044
Epoch: 0, Global Step: 17020, Data Step: 85100, Loss: 1.203125, Token per second per gpu: 20167.327908063256
Epoch: 0, Global Step: 17030, Data Step: 85150, Loss: 1.234375, Token per second per gpu: 19799.10789841799
Epoch: 0, Global Step: 17040, Data Step: 85200, Loss: 1.03125, Token per second per gpu: 19828.416988792025
Epoch: 0, Global Step: 17050, Data Step: 85250, Loss: 0.99609375, Token per second per gpu: 19950.987255760432
Epoch: 0, Global Step: 17060, Data Step: 85300, Loss: 1.234375, Token per second per gpu: 19985.360721151384
Epoch: 0, Global Step: 17070, Data Step: 85350, Loss: 1.140625, Token per second per gpu: 19881.061999519963
Epoch: 0, Global Step: 17080, Data Step: 85400, Loss: 1.6875, Token per second per gpu: 19953.941659173546
Epoch: 0, Global Step: 17090, Data Step: 85450, Loss: 1.25, Token per second per gpu: 19918.694636331908
Epoch: 0, Global Step: 17100, Data Step: 85500, Loss: 1.2421875, Token per second per gpu: 20157.958004237888
Epoch: 0, Global Step: 17110, Data Step: 85550, Loss: 1.4453125, Token per second per gpu: 19776.282432843655
Epoch: 0, Global Step: 17120, Data Step: 85600, Loss: 1.15625, Token per second per gpu: 19888.869394509857
Epoch: 0, Global Step: 17130, Data Step: 85650, Loss: 1.3046875, Token per second per gpu: 20225.886378082494
Epoch: 0, Global Step: 17140, Data Step: 85700, Loss: 1.140625, Token per second per gpu: 19739.822361939252
Epoch: 0, Global Step: 17150, Data Step: 85750, Loss: 1.234375, Token per second per gpu: 19713.50788474717
Epoch: 0, Global Step: 17160, Data Step: 85800, Loss: 1.0625, Token per second per gpu: 19674.508030943387
Epoch: 0, Global Step: 17170, Data Step: 85850, Loss: 1.390625, Token per second per gpu: 20103.247583381675
Epoch: 0, Global Step: 17180, Data Step: 85900, Loss: 1.234375, Token per second per gpu: 19993.557398314042
Epoch: 0, Global Step: 17190, Data Step: 85950, Loss: 1.40625, Token per second per gpu: 20007.23590138045
Epoch: 0, Global Step: 17200, Data Step: 86000, Loss: 1.3828125, Token per second per gpu: 19859.749868637966
Epoch: 0, Global Step: 17210, Data Step: 86050, Loss: 1.40625, Token per second per gpu: 20115.329466502182
Epoch: 0, Global Step: 17220, Data Step: 86100, Loss: 1.15625, Token per second per gpu: 19738.79921926595
Epoch: 0, Global Step: 17230, Data Step: 86150, Loss: 1.0078125, Token per second per gpu: 19755.443211108686
Epoch: 0, Global Step: 17240, Data Step: 86200, Loss: 1.203125, Token per second per gpu: 21152.620515424387
Epoch: 0, Global Step: 17250, Data Step: 86250, Loss: 1.46875, Token per second per gpu: 21137.066162719737
Epoch: 0, Global Step: 17260, Data Step: 86300, Loss: 0.8671875, Token per second per gpu: 19797.852823310088
Epoch: 0, Global Step: 17270, Data Step: 86350, Loss: 1.0234375, Token per second per gpu: 20407.43401109749
Epoch: 0, Global Step: 17280, Data Step: 86400, Loss: 0.91015625, Token per second per gpu: 19800.982045826488
Epoch: 0, Global Step: 17290, Data Step: 86450, Loss: 1.1171875, Token per second per gpu: 19723.777102960765
Epoch: 0, Global Step: 17300, Data Step: 86500, Loss: 1.140625, Token per second per gpu: 19776.479671590245
Epoch: 0, Global Step: 17310, Data Step: 86550, Loss: 1.359375, Token per second per gpu: 20180.841906027606
Epoch: 0, Global Step: 17320, Data Step: 86600, Loss: 1.171875, Token per second per gpu: 20077.76308212493
Epoch: 0, Global Step: 17330, Data Step: 86650, Loss: 1.1875, Token per second per gpu: 19864.309021478628
Epoch: 0, Global Step: 17340, Data Step: 86700, Loss: 1.0390625, Token per second per gpu: 20187.287261033813
Epoch: 0, Global Step: 17350, Data Step: 86750, Loss: 1.1171875, Token per second per gpu: 19940.740107667072
Epoch: 0, Global Step: 17360, Data Step: 86800, Loss: 1.015625, Token per second per gpu: 19646.71298062676
Epoch: 0, Global Step: 17370, Data Step: 86850, Loss: 1.2578125, Token per second per gpu: 19712.255495834743
Epoch: 0, Global Step: 17380, Data Step: 86900, Loss: 1.3515625, Token per second per gpu: 19846.926105935057
Epoch: 0, Global Step: 17390, Data Step: 86950, Loss: 1.140625, Token per second per gpu: 19768.332780521545
Epoch: 0, Global Step: 17400, Data Step: 87000, Loss: 1.1484375, Token per second per gpu: 19869.609843846993
Epoch: 0, Global Step: 17410, Data Step: 87050, Loss: 0.890625, Token per second per gpu: 19941.001560331413
Epoch: 0, Global Step: 17420, Data Step: 87100, Loss: 1.0546875, Token per second per gpu: 20009.376740205917
Epoch: 0, Global Step: 17430, Data Step: 87150, Loss: 1.203125, Token per second per gpu: 20283.400105189237
Epoch: 0, Global Step: 17440, Data Step: 87200, Loss: 1.078125, Token per second per gpu: 19952.999833025337
Epoch: 0, Global Step: 17450, Data Step: 87250, Loss: 1.0859375, Token per second per gpu: 19793.77182637127
Epoch: 0, Global Step: 17460, Data Step: 87300, Loss: 1.140625, Token per second per gpu: 19852.444489623416
Epoch: 0, Global Step: 17470, Data Step: 87350, Loss: 1.1953125, Token per second per gpu: 20300.34135177557
Epoch: 0, Global Step: 17480, Data Step: 87400, Loss: 1.390625, Token per second per gpu: 20035.116793865047
Epoch: 0, Global Step: 17490, Data Step: 87450, Loss: 1.421875, Token per second per gpu: 20129.289871395682
Epoch: 0, Global Step: 17500, Data Step: 87500, Loss: 1.1171875, Token per second per gpu: 20324.57123275099
Epoch: 0, Global Step: 17510, Data Step: 87550, Loss: 0.84765625, Token per second per gpu: 19950.443816446863
Epoch: 0, Global Step: 17520, Data Step: 87600, Loss: 1.2890625, Token per second per gpu: 19936.637939245775
Epoch: 0, Global Step: 17530, Data Step: 87650, Loss: 1.359375, Token per second per gpu: 19892.742591480983
Epoch: 0, Global Step: 17540, Data Step: 87700, Loss: 1.078125, Token per second per gpu: 20537.78622324756
Epoch: 0, Global Step: 17550, Data Step: 87750, Loss: 1.5703125, Token per second per gpu: 19821.684961090497
Epoch: 0, Global Step: 17560, Data Step: 87800, Loss: 1.5703125, Token per second per gpu: 19852.825680897477
Epoch: 0, Global Step: 17570, Data Step: 87850, Loss: 1.34375, Token per second per gpu: 20162.274620082815
Epoch: 0, Global Step: 17580, Data Step: 87900, Loss: 1.2578125, Token per second per gpu: 20140.150140030677
Epoch: 0, Global Step: 17590, Data Step: 87950, Loss: 1.0703125, Token per second per gpu: 20112.711161660976
Epoch: 0, Global Step: 17600, Data Step: 88000, Loss: 1.25, Token per second per gpu: 19911.21955836458
Epoch: 0, Global Step: 17610, Data Step: 88050, Loss: 1.1875, Token per second per gpu: 20132.490779131826
Epoch: 0, Global Step: 17620, Data Step: 88100, Loss: 1.46875, Token per second per gpu: 19975.03495076384
Epoch: 0, Global Step: 17630, Data Step: 88150, Loss: 0.96484375, Token per second per gpu: 19835.228889061982
Epoch: 0, Global Step: 17640, Data Step: 88200, Loss: 0.953125, Token per second per gpu: 19867.89399469435
Epoch: 0, Global Step: 17650, Data Step: 88250, Loss: 1.140625, Token per second per gpu: 20053.795081449705
Epoch: 0, Global Step: 17660, Data Step: 88300, Loss: 1.4140625, Token per second per gpu: 20191.387491068006
Epoch: 0, Global Step: 17670, Data Step: 88350, Loss: 1.4765625, Token per second per gpu: 19881.72922385832
Epoch: 0, Global Step: 17680, Data Step: 88400, Loss: 1.1640625, Token per second per gpu: 19932.845874776332
Epoch: 0, Global Step: 17690, Data Step: 88450, Loss: 0.9375, Token per second per gpu: 19930.700662782747
Epoch: 0, Global Step: 17700, Data Step: 88500, Loss: 0.99609375, Token per second per gpu: 20199.981889014536
Epoch: 0, Global Step: 17710, Data Step: 88550, Loss: 1.2578125, Token per second per gpu: 19929.094091189734
Epoch: 0, Global Step: 17720, Data Step: 88600, Loss: 1.40625, Token per second per gpu: 19852.662337634723
Epoch: 0, Global Step: 17730, Data Step: 88650, Loss: 1.0, Token per second per gpu: 19948.401550047656
Epoch: 0, Global Step: 17740, Data Step: 88700, Loss: 1.0078125, Token per second per gpu: 20243.13102679598
Epoch: 0, Global Step: 17750, Data Step: 88750, Loss: 1.046875, Token per second per gpu: 20079.6479254943
Epoch: 0, Global Step: 17760, Data Step: 88800, Loss: 0.98046875, Token per second per gpu: 19952.900093651588
Epoch: 0, Global Step: 17770, Data Step: 88850, Loss: 1.2578125, Token per second per gpu: 20230.5765179786
Epoch: 0, Global Step: 17780, Data Step: 88900, Loss: 1.1171875, Token per second per gpu: 20080.907813932998
Epoch: 0, Global Step: 17790, Data Step: 88950, Loss: 1.1875, Token per second per gpu: 20047.803363896717
Epoch: 0, Global Step: 17800, Data Step: 89000, Loss: 1.234375, Token per second per gpu: 19869.457622408056
Epoch: 0, Global Step: 17810, Data Step: 89050, Loss: 1.5, Token per second per gpu: 20242.059625219772
Epoch: 0, Global Step: 17820, Data Step: 89100, Loss: 1.15625, Token per second per gpu: 20132.016107910455
Epoch: 0, Global Step: 17830, Data Step: 89150, Loss: 1.265625, Token per second per gpu: 20101.98526658819
Epoch: 0, Global Step: 17840, Data Step: 89200, Loss: 0.9921875, Token per second per gpu: 19855.634504275786
Epoch: 0, Global Step: 17850, Data Step: 89250, Loss: 1.0625, Token per second per gpu: 20278.005288185228
Epoch: 0, Global Step: 17860, Data Step: 89300, Loss: 0.91796875, Token per second per gpu: 20104.411057459725
Epoch: 0, Global Step: 17870, Data Step: 89350, Loss: 1.0234375, Token per second per gpu: 19925.21301281916
Epoch: 0, Global Step: 17880, Data Step: 89400, Loss: 1.1640625, Token per second per gpu: 19816.918589374578
Epoch: 0, Global Step: 17890, Data Step: 89450, Loss: 1.0703125, Token per second per gpu: 20346.890143892313
Epoch: 0, Global Step: 17900, Data Step: 89500, Loss: 1.171875, Token per second per gpu: 20078.730240415523
Epoch: 0, Global Step: 17910, Data Step: 89550, Loss: 1.109375, Token per second per gpu: 19741.09803682271
Epoch: 0, Global Step: 17920, Data Step: 89600, Loss: 1.1015625, Token per second per gpu: 20233.241234561036
Epoch: 0, Global Step: 17930, Data Step: 89650, Loss: 1.40625, Token per second per gpu: 20032.746565132067
Epoch: 0, Global Step: 17940, Data Step: 89700, Loss: 1.1015625, Token per second per gpu: 20212.846404116655
Epoch: 0, Global Step: 17950, Data Step: 89750, Loss: 1.140625, Token per second per gpu: 20197.592249972007
Epoch: 0, Global Step: 17960, Data Step: 89800, Loss: 1.09375, Token per second per gpu: 20212.990044026534
Epoch: 0, Global Step: 17970, Data Step: 89850, Loss: 1.109375, Token per second per gpu: 20605.29726262946
Epoch: 0, Global Step: 17980, Data Step: 89900, Loss: 1.421875, Token per second per gpu: 20132.30053048336
Epoch: 0, Global Step: 17990, Data Step: 89950, Loss: 1.0859375, Token per second per gpu: 20548.038963473813
Epoch: 0, Global Step: 18000, Data Step: 90000, Loss: 1.0625, Token per second per gpu: 19817.05043949754
Epoch: 0, Global Step: 18010, Data Step: 90050, Loss: 1.125, Token per second per gpu: 20057.933993716844
Epoch: 0, Global Step: 18020, Data Step: 90100, Loss: 1.078125, Token per second per gpu: 20396.7342523499
Epoch: 0, Global Step: 18030, Data Step: 90150, Loss: 1.09375, Token per second per gpu: 19811.112523704673
Epoch: 0, Global Step: 18040, Data Step: 90200, Loss: 1.25, Token per second per gpu: 19987.289271636786
Epoch: 0, Global Step: 18050, Data Step: 90250, Loss: 0.93359375, Token per second per gpu: 19922.555432346588
Epoch: 0, Global Step: 18060, Data Step: 90300, Loss: 1.1640625, Token per second per gpu: 20498.928415726892
Epoch: 0, Global Step: 18070, Data Step: 90350, Loss: 1.3359375, Token per second per gpu: 20144.241446373366
Epoch: 0, Global Step: 18080, Data Step: 90400, Loss: 1.03125, Token per second per gpu: 19989.536740564257
Epoch: 0, Global Step: 18090, Data Step: 90450, Loss: 1.265625, Token per second per gpu: 20245.54672358001
Epoch: 0, Global Step: 18100, Data Step: 90500, Loss: 1.0546875, Token per second per gpu: 20204.415180513595
Epoch: 0, Global Step: 18110, Data Step: 90550, Loss: 1.0390625, Token per second per gpu: 20317.919513972363
Epoch: 0, Global Step: 18120, Data Step: 90600, Loss: 1.2734375, Token per second per gpu: 19975.539967373996
Epoch: 0, Global Step: 18130, Data Step: 90650, Loss: 1.0546875, Token per second per gpu: 20291.928645570773
Epoch: 0, Global Step: 18140, Data Step: 90700, Loss: 1.140625, Token per second per gpu: 19763.147325653088
Epoch: 0, Global Step: 18150, Data Step: 90750, Loss: 1.3046875, Token per second per gpu: 20028.076527401478
Epoch: 0, Global Step: 18160, Data Step: 90800, Loss: 1.0546875, Token per second per gpu: 20315.769804294312
Epoch: 0, Global Step: 18170, Data Step: 90850, Loss: 1.3359375, Token per second per gpu: 20086.86860937489
Epoch: 0, Global Step: 18180, Data Step: 90900, Loss: 1.1875, Token per second per gpu: 19888.6203588266
Epoch: 0, Global Step: 18190, Data Step: 90950, Loss: 1.0078125, Token per second per gpu: 20540.81717665002
Epoch: 0, Global Step: 18200, Data Step: 91000, Loss: 1.390625, Token per second per gpu: 19993.168177676016
Epoch: 0, Global Step: 18210, Data Step: 91050, Loss: 1.1171875, Token per second per gpu: 20070.538062277836
Epoch: 0, Global Step: 18220, Data Step: 91100, Loss: 1.453125, Token per second per gpu: 20467.71733176135
Epoch: 0, Global Step: 18230, Data Step: 91150, Loss: 1.078125, Token per second per gpu: 20084.242681038148
Epoch: 0, Global Step: 18240, Data Step: 91200, Loss: 1.234375, Token per second per gpu: 19825.98613774958
Epoch: 0, Global Step: 18250, Data Step: 91250, Loss: 1.2734375, Token per second per gpu: 20258.982502662344
Epoch: 0, Global Step: 18260, Data Step: 91300, Loss: 1.0703125, Token per second per gpu: 19896.78044292896
Epoch: 0, Global Step: 18270, Data Step: 91350, Loss: 1.21875, Token per second per gpu: 19911.802588092294
Epoch: 0, Global Step: 18280, Data Step: 91400, Loss: 1.3828125, Token per second per gpu: 19927.774402995987
Epoch: 0, Global Step: 18290, Data Step: 91450, Loss: 1.3671875, Token per second per gpu: 20441.98553646404
Epoch: 0, Global Step: 18300, Data Step: 91500, Loss: 1.4140625, Token per second per gpu: 20060.393200712653
Epoch: 0, Global Step: 18310, Data Step: 91550, Loss: 0.92578125, Token per second per gpu: 19942.770434360933
Epoch: 0, Global Step: 18320, Data Step: 91600, Loss: 1.2265625, Token per second per gpu: 20089.006228911585
Epoch: 0, Global Step: 18330, Data Step: 91650, Loss: 1.3125, Token per second per gpu: 20314.702038236166
Epoch: 0, Global Step: 18340, Data Step: 91700, Loss: 1.1796875, Token per second per gpu: 19894.339621243165
Epoch: 0, Global Step: 18350, Data Step: 91750, Loss: 1.21875, Token per second per gpu: 20174.95864877047
Epoch: 0, Global Step: 18360, Data Step: 91800, Loss: 0.92578125, Token per second per gpu: 20019.85264810131
Epoch: 0, Global Step: 18370, Data Step: 91850, Loss: 1.0078125, Token per second per gpu: 20068.36948849341
Epoch: 0, Global Step: 18380, Data Step: 91900, Loss: 1.21875, Token per second per gpu: 19978.620043738767
Epoch: 0, Global Step: 18390, Data Step: 91950, Loss: 1.3046875, Token per second per gpu: 20247.748987098053
Epoch: 0, Global Step: 18400, Data Step: 92000, Loss: 0.6875, Token per second per gpu: 19955.860812391205
Epoch: 0, Global Step: 18410, Data Step: 92050, Loss: 1.453125, Token per second per gpu: 20028.248747278296
Epoch: 0, Global Step: 18420, Data Step: 92100, Loss: 0.859375, Token per second per gpu: 20409.49124315438
Epoch: 0, Global Step: 18430, Data Step: 92150, Loss: 1.4140625, Token per second per gpu: 19861.472393813703
Epoch: 0, Global Step: 18440, Data Step: 92200, Loss: 1.421875, Token per second per gpu: 20119.197884802732
Epoch: 0, Global Step: 18450, Data Step: 92250, Loss: 1.125, Token per second per gpu: 19872.43482043913
Epoch: 0, Global Step: 18460, Data Step: 92300, Loss: 1.171875, Token per second per gpu: 20362.513994179113
Epoch: 0, Global Step: 18470, Data Step: 92350, Loss: 1.3359375, Token per second per gpu: 19797.262759366167
Epoch: 0, Global Step: 18480, Data Step: 92400, Loss: 1.3984375, Token per second per gpu: 20039.81592269423
Epoch: 0, Global Step: 18490, Data Step: 92450, Loss: 0.81640625, Token per second per gpu: 20188.454789568987
Epoch: 0, Global Step: 18500, Data Step: 92500, Loss: 1.0625, Token per second per gpu: 20206.757189174354
Epoch: 0, Global Step: 18510, Data Step: 92550, Loss: 1.0546875, Token per second per gpu: 19904.547979613213
Epoch: 0, Global Step: 18520, Data Step: 92600, Loss: 1.375, Token per second per gpu: 20082.061942883356
Epoch: 0, Global Step: 18530, Data Step: 92650, Loss: 0.82421875, Token per second per gpu: 20134.549771303056
Epoch: 0, Global Step: 18540, Data Step: 92700, Loss: 1.6796875, Token per second per gpu: 20130.80829799455
Epoch: 0, Global Step: 18550, Data Step: 92750, Loss: 1.1796875, Token per second per gpu: 19947.051141117485
Epoch: 0, Global Step: 18560, Data Step: 92800, Loss: 1.171875, Token per second per gpu: 19785.815401773736
Epoch: 0, Global Step: 18570, Data Step: 92850, Loss: 1.21875, Token per second per gpu: 20030.606701615794
Epoch: 0, Global Step: 18580, Data Step: 92900, Loss: 1.421875, Token per second per gpu: 20133.012848953826
Epoch: 0, Global Step: 18590, Data Step: 92950, Loss: 0.859375, Token per second per gpu: 19891.00026762051
Epoch: 0, Global Step: 18600, Data Step: 93000, Loss: 1.03125, Token per second per gpu: 20294.966482052416
Epoch: 0, Global Step: 18610, Data Step: 93050, Loss: 0.80078125, Token per second per gpu: 20064.54477312293
Epoch: 0, Global Step: 18620, Data Step: 93100, Loss: 1.0625, Token per second per gpu: 20278.52688973443
Epoch: 0, Global Step: 18630, Data Step: 93150, Loss: 1.0234375, Token per second per gpu: 20083.873024700977
Epoch: 0, Global Step: 18640, Data Step: 93200, Loss: 1.40625, Token per second per gpu: 19757.666833537034
Epoch: 0, Global Step: 18650, Data Step: 93250, Loss: 1.265625, Token per second per gpu: 20240.694539626962
Epoch: 0, Global Step: 18660, Data Step: 93300, Loss: 1.078125, Token per second per gpu: 20139.819787051412
Epoch: 0, Global Step: 18670, Data Step: 93350, Loss: 1.359375, Token per second per gpu: 19971.788812763116
Epoch: 0, Global Step: 18680, Data Step: 93400, Loss: 1.09375, Token per second per gpu: 20289.714652917388
Epoch: 0, Global Step: 18690, Data Step: 93450, Loss: 1.296875, Token per second per gpu: 19766.46371654099
Epoch: 0, Global Step: 18700, Data Step: 93500, Loss: 0.97265625, Token per second per gpu: 19975.315698199713
Epoch: 0, Global Step: 18710, Data Step: 93550, Loss: 1.2890625, Token per second per gpu: 19973.680190549338
Epoch: 0, Global Step: 18720, Data Step: 93600, Loss: 1.0859375, Token per second per gpu: 20009.763236156177
Epoch: 0, Global Step: 18730, Data Step: 93650, Loss: 1.1796875, Token per second per gpu: 20342.574094944644
Epoch: 0, Global Step: 18740, Data Step: 93700, Loss: 1.1484375, Token per second per gpu: 20080.38112002664
Epoch: 0, Global Step: 18750, Data Step: 93750, Loss: 1.3046875, Token per second per gpu: 19989.42268050584
Epoch: 0, Global Step: 18760, Data Step: 93800, Loss: 1.1796875, Token per second per gpu: 20513.47924955266
Epoch: 0, Global Step: 18770, Data Step: 93850, Loss: 0.94140625, Token per second per gpu: 19953.116258685746
Epoch: 0, Global Step: 18780, Data Step: 93900, Loss: 1.359375, Token per second per gpu: 19841.52298314927
Epoch: 0, Global Step: 18790, Data Step: 93950, Loss: 1.1484375, Token per second per gpu: 19914.04806995703
Epoch: 0, Global Step: 18800, Data Step: 94000, Loss: 1.046875, Token per second per gpu: 19982.499272971123
Epoch: 0, Global Step: 18810, Data Step: 94050, Loss: 1.0, Token per second per gpu: 20025.337471468338
Epoch: 0, Global Step: 18820, Data Step: 94100, Loss: 1.140625, Token per second per gpu: 19949.114813180993
Epoch: 0, Global Step: 18830, Data Step: 94150, Loss: 1.078125, Token per second per gpu: 20084.082457387893
Epoch: 0, Global Step: 18840, Data Step: 94200, Loss: 1.0625, Token per second per gpu: 19914.735608844054
Epoch: 0, Global Step: 18850, Data Step: 94250, Loss: 1.0546875, Token per second per gpu: 20027.983507485165
Epoch: 0, Global Step: 18860, Data Step: 94300, Loss: 0.89453125, Token per second per gpu: 19971.400624938946
Epoch: 0, Global Step: 18870, Data Step: 94350, Loss: 1.6171875, Token per second per gpu: 20253.495987513754
Epoch: 0, Global Step: 18880, Data Step: 94400, Loss: 0.9609375, Token per second per gpu: 20150.318372293776
Epoch: 0, Global Step: 18890, Data Step: 94450, Loss: 0.984375, Token per second per gpu: 20223.013919877212
Epoch: 0, Global Step: 18900, Data Step: 94500, Loss: 1.59375, Token per second per gpu: 20171.7741612024
Epoch: 0, Global Step: 18910, Data Step: 94550, Loss: 1.3671875, Token per second per gpu: 20101.49058250866
Epoch: 0, Global Step: 18920, Data Step: 94600, Loss: 1.1953125, Token per second per gpu: 19801.876891047537
Epoch: 0, Global Step: 18930, Data Step: 94650, Loss: 0.9296875, Token per second per gpu: 20337.20399755482
Epoch: 0, Global Step: 18940, Data Step: 94700, Loss: 0.96484375, Token per second per gpu: 20099.079416779892
Epoch: 0, Global Step: 18950, Data Step: 94750, Loss: 1.28125, Token per second per gpu: 20056.842580073597
Epoch: 0, Global Step: 18960, Data Step: 94800, Loss: 1.109375, Token per second per gpu: 19961.715118789096
Epoch: 0, Global Step: 18970, Data Step: 94850, Loss: 1.1640625, Token per second per gpu: 20384.346123004532
Epoch: 0, Global Step: 18980, Data Step: 94900, Loss: 1.0703125, Token per second per gpu: 20320.523070805946
Epoch: 0, Global Step: 18990, Data Step: 94950, Loss: 1.171875, Token per second per gpu: 20001.711246745635
Epoch: 0, Global Step: 19000, Data Step: 95000, Loss: 1.296875, Token per second per gpu: 20377.960749272057
Epoch: 0, Global Step: 19010, Data Step: 95050, Loss: 1.1171875, Token per second per gpu: 20136.324640344686
Epoch: 0, Global Step: 19020, Data Step: 95100, Loss: 1.2421875, Token per second per gpu: 20026.716430661556
Epoch: 0, Global Step: 19030, Data Step: 95150, Loss: 1.21875, Token per second per gpu: 20276.136627738862
Epoch: 0, Global Step: 19040, Data Step: 95200, Loss: 1.2265625, Token per second per gpu: 19938.85940592331
Epoch: 0, Global Step: 19050, Data Step: 95250, Loss: 1.4375, Token per second per gpu: 20111.351788411946
Epoch: 0, Global Step: 19060, Data Step: 95300, Loss: 1.5625, Token per second per gpu: 20531.043111417373
Epoch: 0, Global Step: 19070, Data Step: 95350, Loss: 1.0703125, Token per second per gpu: 20154.43499630915
Epoch: 0, Global Step: 19080, Data Step: 95400, Loss: 1.2421875, Token per second per gpu: 19954.117983358574
Epoch: 0, Global Step: 19090, Data Step: 95450, Loss: 1.0625, Token per second per gpu: 20960.9983566745
Epoch: 0, Global Step: 19100, Data Step: 95500, Loss: 1.1796875, Token per second per gpu: 21386.797088182582
Epoch: 0, Global Step: 19110, Data Step: 95550, Loss: 1.1484375, Token per second per gpu: 20777.079205721868
Epoch: 0, Global Step: 19120, Data Step: 95600, Loss: 1.046875, Token per second per gpu: 20130.848304377218
Epoch: 0, Global Step: 19130, Data Step: 95650, Loss: 1.25, Token per second per gpu: 19850.44517284707
Epoch: 0, Global Step: 19140, Data Step: 95700, Loss: 1.4140625, Token per second per gpu: 20043.303272785077
Epoch: 0, Global Step: 19150, Data Step: 95750, Loss: 1.171875, Token per second per gpu: 20452.36351598187
Epoch: 0, Global Step: 19160, Data Step: 95800, Loss: 1.40625, Token per second per gpu: 20079.5033582442
Epoch: 0, Global Step: 19170, Data Step: 95850, Loss: 1.09375, Token per second per gpu: 20377.91453368296
