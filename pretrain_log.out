[2024-03-26 15:47:02,475] torch.distributed.run: [WARNING] 
[2024-03-26 15:47:02,475] torch.distributed.run: [WARNING] *****************************************
[2024-03-26 15:47:02,475] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-26 15:47:02,475] torch.distributed.run: [WARNING] *****************************************
[2024-03-26 15:47:05,994] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-26 15:47:06,003] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-26 15:47:06,109] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-26 15:47:06,122] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-26 15:47:06,124] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-26 15:47:06,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-26 15:47:06,346] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-26 15:47:06,348] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-26 15:47:06,391] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-26 15:47:06,391] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-26 15:47:06,408] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-26 15:47:06,414] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-26 15:47:06,423] [INFO] [comm.py:637:init_distributed] cdb=None
W0326 15:47:06.793065 140366881915904 logging.py:61] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
W0326 15:47:09.839736 140268300952576 train.py:68] #parameters: 110418432
W0326 15:47:09.872777 140366881915904 train.py:68] #parameters: 110418432
W0326 15:47:09.921232 140669469496320 train.py:68] #parameters: 110418432
W0326 15:47:09.925404 140136197284864 train.py:68] #parameters: 110418432
W0326 15:47:09.932140 140018372211712 train.py:68] #parameters: 110418432
W0326 15:47:10.049007 140718519165952 train.py:68] #parameters: 110418432
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /data/yslan/.cache/torch_extensions/py39_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.18861699104309082 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10159063339233398 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.2019212245941162 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.20173931121826172 seconds
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /data/yslan/.cache/torch_extensions/py39_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.21205496788024902 seconds
wandb: Tracking run with wandb version 0.16.4
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Using /data/yslan/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /data/yslan/.cache/torch_extensions/py39_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.15477490425109863 seconds
[2024-03-26 15:47:13,783] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-03-26 15:47:14,406] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-26 15:47:14,407] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-03-26 15:47:14,407] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-03-26 15:47:14,410] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-03-26 15:47:14,410] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-03-26 15:47:14,411] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-03-26 15:47:14,411] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500,000,000
[2024-03-26 15:47:14,411] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500,000,000
[2024-03-26 15:47:14,411] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2024-03-26 15:47:14,411] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
W0326 15:47:14.717489 140136197284864 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2
W0326 15:47:14.746320 140268300952576 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2
W0326 15:47:14.746365 140669469496320 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2
W0326 15:47:14.749163 140718519165952 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2
W0326 15:47:14.752691 140018372211712 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2
[2024-03-26 15:47:14,858] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-03-26 15:47:14,859] [INFO] [utils.py:801:see_memory_usage] MA 0.29 GB         Max_MA 0.29 GB         CA 0.29 GB         Max_CA 0 GB 
[2024-03-26 15:47:14,859] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.5 GB, percent = 2.7%
[2024-03-26 15:47:14,979] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-03-26 15:47:14,980] [INFO] [utils.py:801:see_memory_usage] MA 0.29 GB         Max_MA 0.35 GB         CA 0.36 GB         Max_CA 0 GB 
[2024-03-26 15:47:14,981] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.5 GB, percent = 2.7%
[2024-03-26 15:47:14,981] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-03-26 15:47:15,097] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-03-26 15:47:15,098] [INFO] [utils.py:801:see_memory_usage] MA 0.29 GB         Max_MA 0.29 GB         CA 0.36 GB         Max_CA 0 GB 
[2024-03-26 15:47:15,098] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.5 GB, percent = 2.7%
[2024-03-26 15:47:15,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2024-03-26 15:47:15,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-03-26 15:47:15,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-03-26 15:47:15,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-03-26 15:47:15,100] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-03-26 15:47:15,100] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-03-26 15:47:15,101] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa8d0059850>
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-03-26 15:47:15,102] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-03-26 15:47:15,103] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-03-26 15:47:15,104] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   train_batch_size ............. 288
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  24
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   world_size ................... 6
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-03-26 15:47:15,105] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-03-26 15:47:15,106] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 288, 
    "train_micro_batch_size_per_gpu": 24, 
    "gradient_accumulation_steps": 2, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
W0326 15:47:15.106247 140366881915904 trainer.py:108] No ckpt found in ckpt/vocab_32k_gpt2
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0326 15:47:16.796000 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
Epoch: 0, Global Step: 100, Data Step: 200, Loss: 8.75, Token per second per gpu: 24235.510407229114
Epoch: 0, Global Step: 200, Data Step: 400, Loss: 7.875, Token per second per gpu: 25500.77820932488
Epoch: 0, Global Step: 300, Data Step: 600, Loss: 7.34375, Token per second per gpu: 25370.316574101536
Epoch: 0, Global Step: 400, Data Step: 800, Loss: 7.0, Token per second per gpu: 25296.879490876552
Epoch: 0, Global Step: 500, Data Step: 1000, Loss: 6.875, Token per second per gpu: 25230.524157441094
Epoch: 0, Global Step: 600, Data Step: 1200, Loss: 6.46875, Token per second per gpu: 25121.44816555161
Epoch: 0, Global Step: 700, Data Step: 1400, Loss: 6.1875, Token per second per gpu: 25087.34050264506
Epoch: 0, Global Step: 800, Data Step: 1600, Loss: 6.0625, Token per second per gpu: 25061.32324213386
Epoch: 0, Global Step: 900, Data Step: 1800, Loss: 5.5625, Token per second per gpu: 25059.05005025991
Epoch: 0, Global Step: 1000, Data Step: 2000, Loss: 5.625, Token per second per gpu: 25139.279120935214
Epoch: 0, Global Step: 1100, Data Step: 2200, Loss: 5.34375, Token per second per gpu: 25244.32322156638
Epoch: 0, Global Step: 1200, Data Step: 2400, Loss: 5.15625, Token per second per gpu: 25269.4400226964
Epoch: 0, Global Step: 1300, Data Step: 2600, Loss: 5.03125, Token per second per gpu: 25273.5773177079
Epoch: 0, Global Step: 1400, Data Step: 2800, Loss: 4.78125, Token per second per gpu: 25191.528018166286
Epoch: 0, Global Step: 1500, Data Step: 3000, Loss: 4.84375, Token per second per gpu: 25110.333719776572
Epoch: 0, Global Step: 1600, Data Step: 3200, Loss: 4.59375, Token per second per gpu: 25055.752565196042
Epoch: 0, Global Step: 1700, Data Step: 3400, Loss: 4.3125, Token per second per gpu: 25041.260215922095
Epoch: 0, Global Step: 1800, Data Step: 3600, Loss: 4.46875, Token per second per gpu: 24951.20764294631
Epoch: 0, Global Step: 1900, Data Step: 3800, Loss: 4.3125, Token per second per gpu: 24558.57586230375
Epoch: 0, Global Step: 2000, Data Step: 4000, Loss: 4.21875, Token per second per gpu: 23860.901661094165
Epoch: 0, Global Step: 2100, Data Step: 4200, Loss: 4.25, Token per second per gpu: 24189.16460791894
Epoch: 0, Global Step: 2200, Data Step: 4400, Loss: 4.34375, Token per second per gpu: 24330.026118815193
Epoch: 0, Global Step: 2300, Data Step: 4600, Loss: 4.21875, Token per second per gpu: 24143.25359543658
Epoch: 0, Global Step: 2400, Data Step: 4800, Loss: 4.53125, Token per second per gpu: 23982.065452122853
Epoch: 0, Global Step: 2500, Data Step: 5000, Loss: 4.25, Token per second per gpu: 24054.99619853614
Epoch: 0, Global Step: 2600, Data Step: 5200, Loss: 4.125, Token per second per gpu: 24233.91024375907
Epoch: 0, Global Step: 2700, Data Step: 5400, Loss: 4.34375, Token per second per gpu: 24435.77107805572
Epoch: 0, Global Step: 2800, Data Step: 5600, Loss: 4.3125, Token per second per gpu: 24273.412300200773
Epoch: 0, Global Step: 2900, Data Step: 5800, Loss: 4.15625, Token per second per gpu: 24281.040732393616
Epoch: 0, Global Step: 3000, Data Step: 6000, Loss: 4.03125, Token per second per gpu: 24323.365095275134
Epoch: 0, Global Step: 3100, Data Step: 6200, Loss: 4.1875, Token per second per gpu: 24313.045988686175
Epoch: 0, Global Step: 3200, Data Step: 6400, Loss: 4.21875, Token per second per gpu: 25107.880973834122
Epoch: 0, Global Step: 3300, Data Step: 6600, Loss: 3.9375, Token per second per gpu: 24366.508900072593
Epoch: 0, Global Step: 3400, Data Step: 6800, Loss: 4.03125, Token per second per gpu: 23981.151898716485
Epoch: 0, Global Step: 3500, Data Step: 7000, Loss: 3.875, Token per second per gpu: 24176.279511294408
Epoch: 0, Global Step: 3600, Data Step: 7200, Loss: 3.734375, Token per second per gpu: 24774.64306998969
Epoch: 0, Global Step: 3700, Data Step: 7400, Loss: 4.0, Token per second per gpu: 24304.76256824052
Epoch: 0, Global Step: 3800, Data Step: 7600, Loss: 3.765625, Token per second per gpu: 24028.649586189826
Epoch: 0, Global Step: 3900, Data Step: 7800, Loss: 4.1875, Token per second per gpu: 24210.32605703027
Epoch: 0, Global Step: 4000, Data Step: 8000, Loss: 3.984375, Token per second per gpu: 24483.218603588542
Epoch: 0, Global Step: 4100, Data Step: 8200, Loss: 3.90625, Token per second per gpu: 24220.45299434033
Epoch: 0, Global Step: 4200, Data Step: 8400, Loss: 4.0625, Token per second per gpu: 24027.065533871406
Epoch: 0, Global Step: 4300, Data Step: 8600, Loss: 3.765625, Token per second per gpu: 24035.508017211665
Epoch: 0, Global Step: 4400, Data Step: 8800, Loss: 4.03125, Token per second per gpu: 24379.399182082117
Epoch: 0, Global Step: 4500, Data Step: 9000, Loss: 4.09375, Token per second per gpu: 24265.645104511554
Epoch: 0, Global Step: 4600, Data Step: 9200, Loss: 3.765625, Token per second per gpu: 24072.26364062282
Epoch: 0, Global Step: 4700, Data Step: 9400, Loss: 3.59375, Token per second per gpu: 23955.523526618
Epoch: 0, Global Step: 4800, Data Step: 9600, Loss: 4.15625, Token per second per gpu: 24030.35914214212
Epoch: 0, Global Step: 4900, Data Step: 9800, Loss: 3.640625, Token per second per gpu: 24069.851405794438
Epoch: 0, Global Step: 5000, Data Step: 10000, Loss: 3.953125, Token per second per gpu: 24078.845374803335
Epoch: 0, Global Step: 5100, Data Step: 10200, Loss: 3.75, Token per second per gpu: 24049.33564681089
Epoch: 0, Global Step: 5200, Data Step: 10400, Loss: 3.734375, Token per second per gpu: 23939.47738925867
Epoch: 0, Global Step: 5300, Data Step: 10600, Loss: 3.8125, Token per second per gpu: 24018.225289660848
Epoch: 0, Global Step: 5400, Data Step: 10800, Loss: 3.875, Token per second per gpu: 23989.9740158433
Epoch: 0, Global Step: 5500, Data Step: 11000, Loss: 3.671875, Token per second per gpu: 24107.322015239875
Epoch: 0, Global Step: 5600, Data Step: 11200, Loss: 3.859375, Token per second per gpu: 24151.069663200076
Epoch: 0, Global Step: 5700, Data Step: 11400, Loss: 3.703125, Token per second per gpu: 24061.821510602334
Epoch: 0, Global Step: 5800, Data Step: 11600, Loss: 3.65625, Token per second per gpu: 24041.6475304766
Epoch: 0, Global Step: 5900, Data Step: 11800, Loss: 3.75, Token per second per gpu: 24019.673200568614
Epoch: 0, Global Step: 6000, Data Step: 12000, Loss: 3.296875, Token per second per gpu: 23960.027623187227
Epoch: 0, Global Step: 6100, Data Step: 12200, Loss: 3.765625, Token per second per gpu: 23999.98540879183
Epoch: 0, Global Step: 6200, Data Step: 12400, Loss: 3.453125, Token per second per gpu: 24087.711430785064
Epoch: 0, Global Step: 6300, Data Step: 12600, Loss: 3.84375, Token per second per gpu: 24043.294330864923
Epoch: 0, Global Step: 6400, Data Step: 12800, Loss: 3.5, Token per second per gpu: 24005.211387937718
Epoch: 0, Global Step: 6500, Data Step: 13000, Loss: 3.671875, Token per second per gpu: 24027.099274594402
Epoch: 0, Global Step: 6600, Data Step: 13200, Loss: 3.578125, Token per second per gpu: 24003.4232253216
Epoch: 0, Global Step: 6700, Data Step: 13400, Loss: 4.03125, Token per second per gpu: 23978.6550269425
Epoch: 0, Global Step: 6800, Data Step: 13600, Loss: 3.46875, Token per second per gpu: 24020.984050294133
Epoch: 0, Global Step: 6900, Data Step: 13800, Loss: 3.578125, Token per second per gpu: 24038.09868518759
Epoch: 0, Global Step: 7000, Data Step: 14000, Loss: 3.40625, Token per second per gpu: 24147.50862021459
Epoch: 0, Global Step: 7100, Data Step: 14200, Loss: 3.75, Token per second per gpu: 24092.892939994512
Epoch: 0, Global Step: 7200, Data Step: 14400, Loss: 3.9375, Token per second per gpu: 23998.956916368996
Epoch: 0, Global Step: 7300, Data Step: 14600, Loss: 3.484375, Token per second per gpu: 24015.43402751004
Epoch: 0, Global Step: 7400, Data Step: 14800, Loss: 3.640625, Token per second per gpu: 24068.332555101177
Epoch: 0, Global Step: 7500, Data Step: 15000, Loss: 3.75, Token per second per gpu: 24012.899605000617
Epoch: 0, Global Step: 7600, Data Step: 15200, Loss: 3.375, Token per second per gpu: 24036.071264450675
Epoch: 0, Global Step: 7700, Data Step: 15400, Loss: 3.515625, Token per second per gpu: 24008.089454767553
Epoch: 0, Global Step: 7800, Data Step: 15600, Loss: 3.703125, Token per second per gpu: 24025.606314382614
Epoch: 0, Global Step: 7900, Data Step: 15800, Loss: 3.65625, Token per second per gpu: 24069.662150476302
Epoch: 0, Global Step: 8000, Data Step: 16000, Loss: 3.796875, Token per second per gpu: 24032.834088867643
Epoch: 0, Global Step: 8100, Data Step: 16200, Loss: 3.390625, Token per second per gpu: 23983.0303577893
Epoch: 0, Global Step: 8200, Data Step: 16400, Loss: 3.484375, Token per second per gpu: 23972.079752255024
Epoch: 0, Global Step: 8300, Data Step: 16600, Loss: 3.53125, Token per second per gpu: 24022.265089878296
Epoch: 0, Global Step: 8400, Data Step: 16800, Loss: 3.75, Token per second per gpu: 24024.806602532273
Epoch: 0, Global Step: 8500, Data Step: 17000, Loss: 3.296875, Token per second per gpu: 23987.6732751153
Epoch: 0, Global Step: 8600, Data Step: 17200, Loss: 3.296875, Token per second per gpu: 24032.552466082077
Epoch: 0, Global Step: 8700, Data Step: 17400, Loss: 3.640625, Token per second per gpu: 24004.579654846144
Epoch: 0, Global Step: 8800, Data Step: 17600, Loss: 3.46875, Token per second per gpu: 23974.980285922287
Epoch: 0, Global Step: 8900, Data Step: 17800, Loss: 3.734375, Token per second per gpu: 24008.92780408822
Epoch: 0, Global Step: 9000, Data Step: 18000, Loss: 3.640625, Token per second per gpu: 23971.55484508473
Epoch: 0, Global Step: 9100, Data Step: 18200, Loss: 3.671875, Token per second per gpu: 23961.498232369722
Epoch: 0, Global Step: 9200, Data Step: 18400, Loss: 3.609375, Token per second per gpu: 23992.32337526282
Epoch: 0, Global Step: 9300, Data Step: 18600, Loss: 3.640625, Token per second per gpu: 24009.075925393638
Epoch: 0, Global Step: 9400, Data Step: 18800, Loss: 3.546875, Token per second per gpu: 24001.546291788534
Epoch: 0, Global Step: 9500, Data Step: 19000, Loss: 3.203125, Token per second per gpu: 23969.670668139657
Epoch: 0, Global Step: 9600, Data Step: 19200, Loss: 3.625, Token per second per gpu: 23989.0387075703
Epoch: 0, Global Step: 9700, Data Step: 19400, Loss: 3.28125, Token per second per gpu: 24046.179335375546
Epoch: 0, Global Step: 9800, Data Step: 19600, Loss: 3.625, Token per second per gpu: 23990.87990322454
Epoch: 0, Global Step: 9900, Data Step: 19800, Loss: 3.90625, Token per second per gpu: 23460.17404031226
Epoch: 0, Global Step: 10000, Data Step: 20000, Loss: 3.21875, Token per second per gpu: 22864.954756367584
I0326 19:05:05.741079 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0326 19:05:05.741453 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-26 19:05:05,741] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/data/yslan/miniconda3/envs/vocab_32k_gpt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 19:05:05,750] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-26 19:05:05,750] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-26 19:05:05,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-26 19:05:05,992] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 19:05:05,992] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-26 19:05:05,992] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-26 19:05:05,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-26 19:05:05,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-26 19:05:05,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-26 19:05:06,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 19:05:06,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-26 19:05:06,253] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-26 19:05:06,253] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 19:05:06,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-26 19:05:06,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-26 19:05:06,253] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-26 19:05:06,253] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-26 19:05:06,253] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 19:05:06,253] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 19:05:06,255] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-26 19:05:06,255] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-26 19:05:06,255] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 19:05:06,256] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-26 19:05:06,256] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-26 19:05:06,256] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 19:05:06,258] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 19:05:06,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0326 19:05:06.259240 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0326 19:05:06.259902 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0326 19:05:06.260178 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0326 19:05:06.261446 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0326 19:06:07.853391 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 1, Global Step: 10100, Data Step: 20200, Loss: 3.515625, Token per second per gpu: 23337.549933548158
Epoch: 1, Global Step: 10200, Data Step: 20400, Loss: 3.3125, Token per second per gpu: 23963.82509322974
Epoch: 1, Global Step: 10300, Data Step: 20600, Loss: 3.6875, Token per second per gpu: 23916.49065430944
Epoch: 1, Global Step: 10400, Data Step: 20800, Loss: 3.203125, Token per second per gpu: 24376.98128001684
Epoch: 1, Global Step: 10500, Data Step: 21000, Loss: 3.796875, Token per second per gpu: 25178.32042006315
Epoch: 1, Global Step: 10600, Data Step: 21200, Loss: 3.5625, Token per second per gpu: 25141.01144539372
Epoch: 1, Global Step: 10700, Data Step: 21400, Loss: 3.25, Token per second per gpu: 25072.021783733624
Epoch: 1, Global Step: 10800, Data Step: 21600, Loss: 3.75, Token per second per gpu: 25044.669939579722
Epoch: 1, Global Step: 10900, Data Step: 21800, Loss: 3.640625, Token per second per gpu: 24961.210354404706
Epoch: 1, Global Step: 11000, Data Step: 22000, Loss: 3.21875, Token per second per gpu: 23945.585593104817
Epoch: 1, Global Step: 11100, Data Step: 22200, Loss: 3.515625, Token per second per gpu: 21076.30711786514
Epoch: 1, Global Step: 11200, Data Step: 22400, Loss: 3.46875, Token per second per gpu: 22120.994140216142
Epoch: 1, Global Step: 11300, Data Step: 22600, Loss: 3.296875, Token per second per gpu: 23006.802306947982
Epoch: 1, Global Step: 11400, Data Step: 22800, Loss: 3.6875, Token per second per gpu: 22856.603405940565
Epoch: 1, Global Step: 11500, Data Step: 23000, Loss: 3.265625, Token per second per gpu: 22227.54259294442
Epoch: 1, Global Step: 11600, Data Step: 23200, Loss: 3.625, Token per second per gpu: 22145.85729462106
Epoch: 1, Global Step: 11700, Data Step: 23400, Loss: 3.46875, Token per second per gpu: 22284.137072575384
Epoch: 1, Global Step: 11800, Data Step: 23600, Loss: 3.421875, Token per second per gpu: 22379.69459964815
Epoch: 1, Global Step: 11900, Data Step: 23800, Loss: 3.46875, Token per second per gpu: 22374.51712628631
Epoch: 1, Global Step: 12000, Data Step: 24000, Loss: 3.6875, Token per second per gpu: 22169.12182495979
Epoch: 1, Global Step: 12100, Data Step: 24200, Loss: 3.734375, Token per second per gpu: 22463.05566460172
Epoch: 1, Global Step: 12200, Data Step: 24400, Loss: 3.5625, Token per second per gpu: 22394.643366024935
Epoch: 1, Global Step: 12300, Data Step: 24600, Loss: 3.359375, Token per second per gpu: 22505.648138015575
Epoch: 1, Global Step: 12400, Data Step: 24800, Loss: 3.640625, Token per second per gpu: 22341.647893859918
Epoch: 1, Global Step: 12500, Data Step: 25000, Loss: 3.46875, Token per second per gpu: 22603.417606723073
Epoch: 1, Global Step: 12600, Data Step: 25200, Loss: 3.5625, Token per second per gpu: 22441.688702291147
Epoch: 1, Global Step: 12700, Data Step: 25400, Loss: 3.3125, Token per second per gpu: 22241.2851639312
Epoch: 1, Global Step: 12800, Data Step: 25600, Loss: 3.546875, Token per second per gpu: 22804.600764779403
Epoch: 1, Global Step: 12900, Data Step: 25800, Loss: 3.515625, Token per second per gpu: 22360.077389782564
Epoch: 1, Global Step: 13000, Data Step: 26000, Loss: 3.546875, Token per second per gpu: 22452.13096044602
Epoch: 1, Global Step: 13100, Data Step: 26200, Loss: 3.703125, Token per second per gpu: 22446.950662682
Epoch: 1, Global Step: 13200, Data Step: 26400, Loss: 3.609375, Token per second per gpu: 22458.947262690588
Epoch: 1, Global Step: 13300, Data Step: 26600, Loss: 3.578125, Token per second per gpu: 22296.86838599072
Epoch: 1, Global Step: 13400, Data Step: 26800, Loss: 3.40625, Token per second per gpu: 22384.7875658022
Epoch: 1, Global Step: 13500, Data Step: 27000, Loss: 3.25, Token per second per gpu: 22412.740748703935
Epoch: 1, Global Step: 13600, Data Step: 27200, Loss: 3.765625, Token per second per gpu: 22969.868665188216
Epoch: 1, Global Step: 13700, Data Step: 27400, Loss: 3.40625, Token per second per gpu: 22277.79506741741
Epoch: 1, Global Step: 13800, Data Step: 27600, Loss: 3.171875, Token per second per gpu: 22198.188790968627
Epoch: 1, Global Step: 13900, Data Step: 27800, Loss: 3.6875, Token per second per gpu: 22372.30165936269
Epoch: 1, Global Step: 14000, Data Step: 28000, Loss: 3.421875, Token per second per gpu: 22410.452014881008
Epoch: 1, Global Step: 14100, Data Step: 28200, Loss: 3.28125, Token per second per gpu: 22167.526765225728
Epoch: 1, Global Step: 14200, Data Step: 28400, Loss: 3.25, Token per second per gpu: 22631.34097309804
Epoch: 1, Global Step: 14300, Data Step: 28600, Loss: 3.4375, Token per second per gpu: 22295.752367818048
Epoch: 1, Global Step: 14400, Data Step: 28800, Loss: 3.46875, Token per second per gpu: 22496.0872537725
Epoch: 1, Global Step: 14500, Data Step: 29000, Loss: 3.03125, Token per second per gpu: 22399.64997223945
Epoch: 1, Global Step: 14600, Data Step: 29200, Loss: 3.65625, Token per second per gpu: 22412.643024191686
Epoch: 1, Global Step: 14700, Data Step: 29400, Loss: 3.59375, Token per second per gpu: 22385.39997268519
Epoch: 1, Global Step: 14800, Data Step: 29600, Loss: 3.671875, Token per second per gpu: 22418.79084816572
Epoch: 1, Global Step: 14900, Data Step: 29800, Loss: 3.46875, Token per second per gpu: 22449.48170606862
Epoch: 1, Global Step: 15000, Data Step: 30000, Loss: 3.375, Token per second per gpu: 22671.425745958473
Epoch: 1, Global Step: 15100, Data Step: 30200, Loss: 3.4375, Token per second per gpu: 22331.644552156908
Epoch: 1, Global Step: 15200, Data Step: 30400, Loss: 3.59375, Token per second per gpu: 22766.399541014016
Epoch: 1, Global Step: 15300, Data Step: 30600, Loss: 3.4375, Token per second per gpu: 22530.680190008858
Epoch: 1, Global Step: 15400, Data Step: 30800, Loss: 3.71875, Token per second per gpu: 22099.529357018113
Epoch: 1, Global Step: 15500, Data Step: 31000, Loss: 3.0625, Token per second per gpu: 22800.05099832498
Epoch: 1, Global Step: 15600, Data Step: 31200, Loss: 3.453125, Token per second per gpu: 22360.686127382865
Epoch: 1, Global Step: 15700, Data Step: 31400, Loss: 3.15625, Token per second per gpu: 22694.124109663542
Epoch: 1, Global Step: 15800, Data Step: 31600, Loss: 3.328125, Token per second per gpu: 22828.452859297286
Epoch: 1, Global Step: 15900, Data Step: 31800, Loss: 3.34375, Token per second per gpu: 22516.604011776377
Epoch: 1, Global Step: 16000, Data Step: 32000, Loss: 3.578125, Token per second per gpu: 22316.496871204585
Epoch: 1, Global Step: 16100, Data Step: 32200, Loss: 3.484375, Token per second per gpu: 22454.422372206096
Epoch: 1, Global Step: 16200, Data Step: 32400, Loss: 3.34375, Token per second per gpu: 22330.16385110412
Epoch: 1, Global Step: 16300, Data Step: 32600, Loss: 3.234375, Token per second per gpu: 22684.11099160324
Epoch: 1, Global Step: 16400, Data Step: 32800, Loss: 3.390625, Token per second per gpu: 22548.284671217698
Epoch: 1, Global Step: 16500, Data Step: 33000, Loss: 3.203125, Token per second per gpu: 22506.766188062702
Epoch: 1, Global Step: 16600, Data Step: 33200, Loss: 3.328125, Token per second per gpu: 22576.190086193492
Epoch: 1, Global Step: 16700, Data Step: 33400, Loss: 3.546875, Token per second per gpu: 22613.685565110944
Epoch: 1, Global Step: 16800, Data Step: 33600, Loss: 3.046875, Token per second per gpu: 22126.479716893642
Epoch: 1, Global Step: 16900, Data Step: 33800, Loss: 3.375, Token per second per gpu: 22711.985760251253
Epoch: 1, Global Step: 17000, Data Step: 34000, Loss: 3.453125, Token per second per gpu: 22622.568036982844
Epoch: 1, Global Step: 17100, Data Step: 34200, Loss: 3.5, Token per second per gpu: 22735.61612397258
Epoch: 1, Global Step: 17200, Data Step: 34400, Loss: 3.453125, Token per second per gpu: 22593.24711765789
Epoch: 1, Global Step: 17300, Data Step: 34600, Loss: 3.5625, Token per second per gpu: 22564.882490644643
Epoch: 1, Global Step: 17400, Data Step: 34800, Loss: 3.1875, Token per second per gpu: 22478.026569957998
Epoch: 1, Global Step: 17500, Data Step: 35000, Loss: 3.8125, Token per second per gpu: 22489.537086025786
Epoch: 1, Global Step: 17600, Data Step: 35200, Loss: 3.46875, Token per second per gpu: 22442.590715622224
Epoch: 1, Global Step: 17700, Data Step: 35400, Loss: 3.28125, Token per second per gpu: 22707.901018915454
Epoch: 1, Global Step: 17800, Data Step: 35600, Loss: 3.546875, Token per second per gpu: 22387.98014039602
Epoch: 1, Global Step: 17900, Data Step: 35800, Loss: 3.453125, Token per second per gpu: 22539.11775274576
Epoch: 1, Global Step: 18000, Data Step: 36000, Loss: 3.59375, Token per second per gpu: 22192.76815028893
Epoch: 1, Global Step: 18100, Data Step: 36200, Loss: 3.53125, Token per second per gpu: 22706.278451668157
Epoch: 1, Global Step: 18200, Data Step: 36400, Loss: 3.375, Token per second per gpu: 22161.28636338469
Epoch: 1, Global Step: 18300, Data Step: 36600, Loss: 3.28125, Token per second per gpu: 22436.014152606425
Epoch: 1, Global Step: 18400, Data Step: 36800, Loss: 3.296875, Token per second per gpu: 22742.25995909521
Epoch: 1, Global Step: 18500, Data Step: 37000, Loss: 3.203125, Token per second per gpu: 22508.93324784199
Epoch: 1, Global Step: 18600, Data Step: 37200, Loss: 3.390625, Token per second per gpu: 22395.130382177762
Epoch: 1, Global Step: 18700, Data Step: 37400, Loss: 3.046875, Token per second per gpu: 22562.998388471668
Epoch: 1, Global Step: 18800, Data Step: 37600, Loss: 3.453125, Token per second per gpu: 22349.855493632356
Epoch: 1, Global Step: 18900, Data Step: 37800, Loss: 3.734375, Token per second per gpu: 22369.36375425218
Epoch: 1, Global Step: 19000, Data Step: 38000, Loss: 3.125, Token per second per gpu: 22736.630764017125
Epoch: 1, Global Step: 19100, Data Step: 38200, Loss: 3.265625, Token per second per gpu: 22581.60776530625
Epoch: 1, Global Step: 19200, Data Step: 38400, Loss: 3.28125, Token per second per gpu: 22318.607196894343
Epoch: 1, Global Step: 19300, Data Step: 38600, Loss: 3.265625, Token per second per gpu: 22497.98219587521
Epoch: 1, Global Step: 19400, Data Step: 38800, Loss: 3.4375, Token per second per gpu: 22503.338630171926
Epoch: 1, Global Step: 19500, Data Step: 39000, Loss: 3.203125, Token per second per gpu: 22604.46866049609
Epoch: 1, Global Step: 19600, Data Step: 39200, Loss: 3.578125, Token per second per gpu: 22399.75937995835
Epoch: 1, Global Step: 19700, Data Step: 39400, Loss: 2.96875, Token per second per gpu: 22280.26854436302
Epoch: 1, Global Step: 19800, Data Step: 39600, Loss: 3.1875, Token per second per gpu: 22614.102690667918
Epoch: 1, Global Step: 19900, Data Step: 39800, Loss: 3.4375, Token per second per gpu: 21477.80014914129
Epoch: 1, Global Step: 20000, Data Step: 40000, Loss: 3.28125, Token per second per gpu: 21332.786404538623
I0326 22:37:18.875101 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0326 22:37:18.876779 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-26 22:37:18,883] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-26 22:37:18,898] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-26 22:37:18,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-26 22:37:19,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-26 22:37:19,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 22:37:19,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-26 22:37:19,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-26 22:37:19,815] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-26 22:37:19,815] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-26 22:37:19,856] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-26 22:37:20,476] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-26 22:37:20,477] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-26 22:37:20,477] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 22:37:21,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 22:37:21,931] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 22:37:21,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 22:37:21,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-26 22:37:21,966] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-26 22:37:21,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 22:37:21,985] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-26 22:37:21,986] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-26 22:37:21,986] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 22:37:22,005] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-26 22:37:22,005] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-26 22:37:22,005] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-26 22:37:22,036] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-26 22:37:22,036] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-26 22:37:22,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0326 22:37:22.036706 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0326 22:37:22.037392 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0326 22:37:22.037645 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0326 22:37:22.038880 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 1, Global Step: 20100, Data Step: 40200, Loss: 3.625, Token per second per gpu: 21904.75672025532
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0326 22:39:33.967743 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 2, Global Step: 20200, Data Step: 40400, Loss: 3.203125, Token per second per gpu: 22184.680266354295
Epoch: 2, Global Step: 20300, Data Step: 40600, Loss: 3.34375, Token per second per gpu: 22529.598925281258
Epoch: 2, Global Step: 20400, Data Step: 40800, Loss: 3.625, Token per second per gpu: 22254.909596986738
Epoch: 2, Global Step: 20500, Data Step: 41000, Loss: 3.40625, Token per second per gpu: 22379.48977656781
Epoch: 2, Global Step: 20600, Data Step: 41200, Loss: 3.578125, Token per second per gpu: 22591.034345710443
Epoch: 2, Global Step: 20700, Data Step: 41400, Loss: 3.6875, Token per second per gpu: 22437.602990629926
Epoch: 2, Global Step: 20800, Data Step: 41600, Loss: 3.4375, Token per second per gpu: 22474.72283473526
Epoch: 2, Global Step: 20900, Data Step: 41800, Loss: 3.28125, Token per second per gpu: 22670.163341162326
Epoch: 2, Global Step: 21000, Data Step: 42000, Loss: 3.3125, Token per second per gpu: 22360.056984588227
Epoch: 2, Global Step: 21100, Data Step: 42200, Loss: 3.4375, Token per second per gpu: 22348.20699169491
Epoch: 2, Global Step: 21200, Data Step: 42400, Loss: 3.125, Token per second per gpu: 22537.456267661524
Epoch: 2, Global Step: 21300, Data Step: 42600, Loss: 3.359375, Token per second per gpu: 22412.960902280724
Epoch: 2, Global Step: 21400, Data Step: 42800, Loss: 3.390625, Token per second per gpu: 22676.25690196952
Epoch: 2, Global Step: 21500, Data Step: 43000, Loss: 3.515625, Token per second per gpu: 22904.311803712153
Epoch: 2, Global Step: 21600, Data Step: 43200, Loss: 3.359375, Token per second per gpu: 22419.685737122425
Epoch: 2, Global Step: 21700, Data Step: 43400, Loss: 3.515625, Token per second per gpu: 22298.196575920476
Epoch: 2, Global Step: 21800, Data Step: 43600, Loss: 3.4375, Token per second per gpu: 22765.07042058161
Epoch: 2, Global Step: 21900, Data Step: 43800, Loss: 3.515625, Token per second per gpu: 22393.919191828896
Epoch: 2, Global Step: 22000, Data Step: 44000, Loss: 3.140625, Token per second per gpu: 22702.391303399956
Epoch: 2, Global Step: 22100, Data Step: 44200, Loss: 3.625, Token per second per gpu: 22734.30201942355
Epoch: 2, Global Step: 22200, Data Step: 44400, Loss: 3.515625, Token per second per gpu: 21541.427446796442
Epoch: 2, Global Step: 22300, Data Step: 44600, Loss: 3.375, Token per second per gpu: 22402.34040784296
Epoch: 2, Global Step: 22400, Data Step: 44800, Loss: 3.09375, Token per second per gpu: 22740.66355878738
Epoch: 2, Global Step: 22500, Data Step: 45000, Loss: 3.640625, Token per second per gpu: 22285.219160930636
Epoch: 2, Global Step: 22600, Data Step: 45200, Loss: 3.40625, Token per second per gpu: 22706.578506476333
Epoch: 2, Global Step: 22700, Data Step: 45400, Loss: 3.359375, Token per second per gpu: 22790.227817764353
Epoch: 2, Global Step: 22800, Data Step: 45600, Loss: 3.234375, Token per second per gpu: 22573.975549484054
Epoch: 2, Global Step: 22900, Data Step: 45800, Loss: 3.59375, Token per second per gpu: 22799.655946201397
Epoch: 2, Global Step: 23000, Data Step: 46000, Loss: 3.171875, Token per second per gpu: 22554.935902878166
Epoch: 2, Global Step: 23100, Data Step: 46200, Loss: 3.375, Token per second per gpu: 22632.82961578689
Epoch: 2, Global Step: 23200, Data Step: 46400, Loss: 3.609375, Token per second per gpu: 22705.72364859438
Epoch: 2, Global Step: 23300, Data Step: 46600, Loss: 3.421875, Token per second per gpu: 22457.508626993156
Epoch: 2, Global Step: 23400, Data Step: 46800, Loss: 3.546875, Token per second per gpu: 22353.280009377442
Epoch: 2, Global Step: 23500, Data Step: 47000, Loss: 3.03125, Token per second per gpu: 22350.033474273823
Epoch: 2, Global Step: 23600, Data Step: 47200, Loss: 3.328125, Token per second per gpu: 22539.7369088743
Epoch: 2, Global Step: 23700, Data Step: 47400, Loss: 3.359375, Token per second per gpu: 22640.796144372725
Epoch: 2, Global Step: 23800, Data Step: 47600, Loss: 3.53125, Token per second per gpu: 22926.681920265994
Epoch: 2, Global Step: 23900, Data Step: 47800, Loss: 3.421875, Token per second per gpu: 22696.17034375382
Epoch: 2, Global Step: 24000, Data Step: 48000, Loss: 3.453125, Token per second per gpu: 22510.27973425203
Epoch: 2, Global Step: 24100, Data Step: 48200, Loss: 3.25, Token per second per gpu: 22384.26175973729
Epoch: 2, Global Step: 24200, Data Step: 48400, Loss: 3.46875, Token per second per gpu: 22501.86755920705
Epoch: 2, Global Step: 24300, Data Step: 48600, Loss: 3.46875, Token per second per gpu: 22541.431750444695
Epoch: 2, Global Step: 24400, Data Step: 48800, Loss: 3.390625, Token per second per gpu: 22639.17462407043
Epoch: 2, Global Step: 24500, Data Step: 49000, Loss: 3.203125, Token per second per gpu: 22415.9126004884
Epoch: 2, Global Step: 24600, Data Step: 49200, Loss: 3.25, Token per second per gpu: 22535.42518158737
Epoch: 2, Global Step: 24700, Data Step: 49400, Loss: 3.484375, Token per second per gpu: 22460.929921593743
Epoch: 2, Global Step: 24800, Data Step: 49600, Loss: 3.65625, Token per second per gpu: 22505.23517151338
Epoch: 2, Global Step: 24900, Data Step: 49800, Loss: 3.671875, Token per second per gpu: 22421.756151606423
Epoch: 2, Global Step: 25000, Data Step: 50000, Loss: 3.5625, Token per second per gpu: 22314.191559177874
Epoch: 2, Global Step: 25100, Data Step: 50200, Loss: 3.71875, Token per second per gpu: 22589.893248913573
Epoch: 2, Global Step: 25200, Data Step: 50400, Loss: 3.1875, Token per second per gpu: 22252.502825835505
Epoch: 2, Global Step: 25300, Data Step: 50600, Loss: 3.015625, Token per second per gpu: 22409.783608020836
Epoch: 2, Global Step: 25400, Data Step: 50800, Loss: 3.046875, Token per second per gpu: 22409.178430082902
Epoch: 2, Global Step: 25500, Data Step: 51000, Loss: 3.578125, Token per second per gpu: 22480.08463931056
Epoch: 2, Global Step: 25600, Data Step: 51200, Loss: 3.546875, Token per second per gpu: 22553.40354755371
Epoch: 2, Global Step: 25700, Data Step: 51400, Loss: 3.53125, Token per second per gpu: 22144.796776169864
Epoch: 2, Global Step: 25800, Data Step: 51600, Loss: 3.140625, Token per second per gpu: 22507.772121285045
Epoch: 2, Global Step: 25900, Data Step: 51800, Loss: 2.921875, Token per second per gpu: 22504.296755304378
Epoch: 2, Global Step: 26000, Data Step: 52000, Loss: 3.75, Token per second per gpu: 22656.396093399362
Epoch: 2, Global Step: 26100, Data Step: 52200, Loss: 3.34375, Token per second per gpu: 22353.736726619587
Epoch: 2, Global Step: 26200, Data Step: 52400, Loss: 3.484375, Token per second per gpu: 22286.61775577945
Epoch: 2, Global Step: 26300, Data Step: 52600, Loss: 3.359375, Token per second per gpu: 22514.079002133065
Epoch: 2, Global Step: 26400, Data Step: 52800, Loss: 3.140625, Token per second per gpu: 22349.943118970317
Epoch: 2, Global Step: 26500, Data Step: 53000, Loss: 3.40625, Token per second per gpu: 22642.797629615594
Epoch: 2, Global Step: 26600, Data Step: 53200, Loss: 3.484375, Token per second per gpu: 22697.157924738007
Epoch: 2, Global Step: 26700, Data Step: 53400, Loss: 3.1875, Token per second per gpu: 22330.865537245078
Epoch: 2, Global Step: 26800, Data Step: 53600, Loss: 3.28125, Token per second per gpu: 22505.137016104116
Epoch: 2, Global Step: 26900, Data Step: 53800, Loss: 3.25, Token per second per gpu: 22398.98166962831
Epoch: 2, Global Step: 27000, Data Step: 54000, Loss: 3.328125, Token per second per gpu: 22711.133868462664
Epoch: 2, Global Step: 27100, Data Step: 54200, Loss: 3.421875, Token per second per gpu: 22184.175552447305
Epoch: 2, Global Step: 27200, Data Step: 54400, Loss: 2.921875, Token per second per gpu: 22636.43621047905
Epoch: 2, Global Step: 27300, Data Step: 54600, Loss: 3.3125, Token per second per gpu: 22435.16033629188
Epoch: 2, Global Step: 27400, Data Step: 54800, Loss: 3.453125, Token per second per gpu: 22598.837787790424
Epoch: 2, Global Step: 27500, Data Step: 55000, Loss: 3.15625, Token per second per gpu: 22659.32440455276
Epoch: 2, Global Step: 27600, Data Step: 55200, Loss: 3.4375, Token per second per gpu: 22429.6477227144
Epoch: 2, Global Step: 27700, Data Step: 55400, Loss: 3.484375, Token per second per gpu: 22522.47683742528
Epoch: 2, Global Step: 27800, Data Step: 55600, Loss: 3.03125, Token per second per gpu: 22794.09663096613
Epoch: 2, Global Step: 27900, Data Step: 55800, Loss: 3.359375, Token per second per gpu: 22474.675959765173
Epoch: 2, Global Step: 28000, Data Step: 56000, Loss: 3.5, Token per second per gpu: 22712.28408694012
Epoch: 2, Global Step: 28100, Data Step: 56200, Loss: 3.25, Token per second per gpu: 22395.86588729954
Epoch: 2, Global Step: 28200, Data Step: 56400, Loss: 3.296875, Token per second per gpu: 22341.152953692068
Epoch: 2, Global Step: 28300, Data Step: 56600, Loss: 3.078125, Token per second per gpu: 22367.368566529207
Epoch: 2, Global Step: 28400, Data Step: 56800, Loss: 3.09375, Token per second per gpu: 22254.987705034073
Epoch: 2, Global Step: 28500, Data Step: 57000, Loss: 3.375, Token per second per gpu: 22703.74221698866
Epoch: 2, Global Step: 28600, Data Step: 57200, Loss: 3.34375, Token per second per gpu: 22292.167445235613
Epoch: 2, Global Step: 28700, Data Step: 57400, Loss: 3.046875, Token per second per gpu: 22356.880974898366
Epoch: 2, Global Step: 28800, Data Step: 57600, Loss: 3.3125, Token per second per gpu: 22498.51021536038
Epoch: 2, Global Step: 28900, Data Step: 57800, Loss: 3.40625, Token per second per gpu: 22468.690700456653
Epoch: 2, Global Step: 29000, Data Step: 58000, Loss: 3.03125, Token per second per gpu: 22357.270845699342
Epoch: 2, Global Step: 29100, Data Step: 58200, Loss: 2.921875, Token per second per gpu: 22464.694081955004
Epoch: 2, Global Step: 29200, Data Step: 58400, Loss: 3.5, Token per second per gpu: 22592.98085560624
Epoch: 2, Global Step: 29300, Data Step: 58600, Loss: 3.421875, Token per second per gpu: 22778.314749691766
Epoch: 2, Global Step: 29400, Data Step: 58800, Loss: 3.421875, Token per second per gpu: 22217.281507206695
Epoch: 2, Global Step: 29500, Data Step: 59000, Loss: 3.25, Token per second per gpu: 22394.264728716305
Epoch: 2, Global Step: 29600, Data Step: 59200, Loss: 3.328125, Token per second per gpu: 21967.112120728496
Epoch: 2, Global Step: 29700, Data Step: 59400, Loss: 2.984375, Token per second per gpu: 22370.027846450477
Epoch: 2, Global Step: 29800, Data Step: 59600, Loss: 3.625, Token per second per gpu: 21482.898238324506
Epoch: 2, Global Step: 29900, Data Step: 59800, Loss: 3.25, Token per second per gpu: 21489.89491737076
Epoch: 2, Global Step: 30000, Data Step: 60000, Loss: 3.546875, Token per second per gpu: 22033.85942733935
I0327 02:11:06.271662 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0327 02:11:06.273326 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-27 02:11:06,319] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-27 02:11:06,325] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-27 02:11:06,325] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-27 02:11:07,677] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-27 02:11:07,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-27 02:11:07,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-27 02:11:07,750] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-27 02:11:07,750] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-27 02:11:07,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-27 02:11:07,758] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-27 02:11:08,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-27 02:11:08,305] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-27 02:11:08,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 02:11:09,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-27 02:11:09,358] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-27 02:11:09,358] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 02:11:09,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-27 02:11:09,661] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-27 02:11:09,661] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 02:11:09,705] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-27 02:11:09,706] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-27 02:11:09,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 02:11:09,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-27 02:11:09,707] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-27 02:11:09,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 02:11:09,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-27 02:11:09,748] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-27 02:11:09,748] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0327 02:11:09.748679 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0327 02:11:09.749439 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0327 02:11:09.749718 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0327 02:11:09.751394 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 2, Global Step: 30100, Data Step: 60200, Loss: 3.453125, Token per second per gpu: 21805.411637426652
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0327 02:14:28.627535 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 3, Global Step: 30200, Data Step: 60400, Loss: 3.40625, Token per second per gpu: 21863.927279692238
Epoch: 3, Global Step: 30300, Data Step: 60600, Loss: 3.03125, Token per second per gpu: 22339.30569580068
Epoch: 3, Global Step: 30400, Data Step: 60800, Loss: 3.34375, Token per second per gpu: 22183.44577870463
Epoch: 3, Global Step: 30500, Data Step: 61000, Loss: 3.171875, Token per second per gpu: 22165.686265230615
Epoch: 3, Global Step: 30600, Data Step: 61200, Loss: 3.296875, Token per second per gpu: 22523.71574963222
Epoch: 3, Global Step: 30700, Data Step: 61400, Loss: 3.75, Token per second per gpu: 22303.09524780789
Epoch: 3, Global Step: 30800, Data Step: 61600, Loss: 3.53125, Token per second per gpu: 22300.64971869336
Epoch: 3, Global Step: 30900, Data Step: 61800, Loss: 3.53125, Token per second per gpu: 22423.860249556295
Epoch: 3, Global Step: 31000, Data Step: 62000, Loss: 3.0, Token per second per gpu: 22298.60185558417
Epoch: 3, Global Step: 31100, Data Step: 62200, Loss: 3.015625, Token per second per gpu: 22292.417942538945
Epoch: 3, Global Step: 31200, Data Step: 62400, Loss: 3.21875, Token per second per gpu: 22048.512191413894
Epoch: 3, Global Step: 31300, Data Step: 62600, Loss: 3.203125, Token per second per gpu: 22252.185465745613
Epoch: 3, Global Step: 31400, Data Step: 62800, Loss: 3.265625, Token per second per gpu: 22434.59303649668
Epoch: 3, Global Step: 31500, Data Step: 63000, Loss: 3.234375, Token per second per gpu: 22922.18435844787
Epoch: 3, Global Step: 31600, Data Step: 63200, Loss: 3.265625, Token per second per gpu: 22212.518442800007
Epoch: 3, Global Step: 31700, Data Step: 63400, Loss: 3.328125, Token per second per gpu: 22158.337888510076
Epoch: 3, Global Step: 31800, Data Step: 63600, Loss: 2.953125, Token per second per gpu: 22650.63913027769
Epoch: 3, Global Step: 31900, Data Step: 63800, Loss: 3.046875, Token per second per gpu: 22414.230217327527
Epoch: 3, Global Step: 32000, Data Step: 64000, Loss: 3.203125, Token per second per gpu: 22411.816599989867
Epoch: 3, Global Step: 32100, Data Step: 64200, Loss: 3.125, Token per second per gpu: 22241.419566808294
Epoch: 3, Global Step: 32200, Data Step: 64400, Loss: 3.34375, Token per second per gpu: 22730.228666004543
Epoch: 3, Global Step: 32300, Data Step: 64600, Loss: 3.53125, Token per second per gpu: 22070.76200335334
Epoch: 3, Global Step: 32400, Data Step: 64800, Loss: 3.21875, Token per second per gpu: 22322.412984921182
Epoch: 3, Global Step: 32500, Data Step: 65000, Loss: 3.28125, Token per second per gpu: 22728.747796792297
Epoch: 3, Global Step: 32600, Data Step: 65200, Loss: 3.125, Token per second per gpu: 22398.117504240083
Epoch: 3, Global Step: 32700, Data Step: 65400, Loss: 3.296875, Token per second per gpu: 22047.534334467153
Epoch: 3, Global Step: 32800, Data Step: 65600, Loss: 3.53125, Token per second per gpu: 22521.356890069354
Epoch: 3, Global Step: 32900, Data Step: 65800, Loss: 3.125, Token per second per gpu: 22563.063333381946
Epoch: 3, Global Step: 33000, Data Step: 66000, Loss: 2.890625, Token per second per gpu: 22288.77182664393
Epoch: 3, Global Step: 33100, Data Step: 66200, Loss: 3.109375, Token per second per gpu: 22503.66185308089
Epoch: 3, Global Step: 33200, Data Step: 66400, Loss: 3.203125, Token per second per gpu: 22277.182535687873
Epoch: 3, Global Step: 33300, Data Step: 66600, Loss: 3.15625, Token per second per gpu: 21728.741244447363
Epoch: 3, Global Step: 33400, Data Step: 66800, Loss: 3.234375, Token per second per gpu: 22392.21997640426
Epoch: 3, Global Step: 33500, Data Step: 67000, Loss: 3.03125, Token per second per gpu: 22309.82806080114
Epoch: 3, Global Step: 33600, Data Step: 67200, Loss: 3.09375, Token per second per gpu: 22360.274490148957
Epoch: 3, Global Step: 33700, Data Step: 67400, Loss: 3.40625, Token per second per gpu: 22908.535814337003
Epoch: 3, Global Step: 33800, Data Step: 67600, Loss: 3.1875, Token per second per gpu: 22480.673027188077
Epoch: 3, Global Step: 33900, Data Step: 67800, Loss: 3.375, Token per second per gpu: 22223.39342118909
Epoch: 3, Global Step: 34000, Data Step: 68000, Loss: 3.28125, Token per second per gpu: 22637.37252846781
Epoch: 3, Global Step: 34100, Data Step: 68200, Loss: 2.90625, Token per second per gpu: 21998.47572047006
Epoch: 3, Global Step: 34200, Data Step: 68400, Loss: 3.046875, Token per second per gpu: 22311.347522940163
Epoch: 3, Global Step: 34300, Data Step: 68600, Loss: 3.03125, Token per second per gpu: 22166.836526656552
Epoch: 3, Global Step: 34400, Data Step: 68800, Loss: 3.375, Token per second per gpu: 22151.098693692402
Epoch: 3, Global Step: 34500, Data Step: 69000, Loss: 3.265625, Token per second per gpu: 22449.438607808985
Epoch: 3, Global Step: 34600, Data Step: 69200, Loss: 2.921875, Token per second per gpu: 22144.848537115944
Epoch: 3, Global Step: 34700, Data Step: 69400, Loss: 3.359375, Token per second per gpu: 21898.277654937963
Epoch: 3, Global Step: 34800, Data Step: 69600, Loss: 3.15625, Token per second per gpu: 22298.25593040035
Epoch: 3, Global Step: 34900, Data Step: 69800, Loss: 3.21875, Token per second per gpu: 22454.070635596076
Epoch: 3, Global Step: 35000, Data Step: 70000, Loss: 2.71875, Token per second per gpu: 22418.363130875045
Epoch: 3, Global Step: 35100, Data Step: 70200, Loss: 3.109375, Token per second per gpu: 22269.37311094132
Epoch: 3, Global Step: 35200, Data Step: 70400, Loss: 3.25, Token per second per gpu: 22423.900002793984
Epoch: 3, Global Step: 35300, Data Step: 70600, Loss: 3.4375, Token per second per gpu: 22556.89970069409
Epoch: 3, Global Step: 35400, Data Step: 70800, Loss: 3.140625, Token per second per gpu: 22264.52933274217
Epoch: 3, Global Step: 35500, Data Step: 71000, Loss: 3.25, Token per second per gpu: 22265.50309911436
Epoch: 3, Global Step: 35600, Data Step: 71200, Loss: 2.6875, Token per second per gpu: 22463.29690049512
Epoch: 3, Global Step: 35700, Data Step: 71400, Loss: 3.15625, Token per second per gpu: 22463.500419896493
Epoch: 3, Global Step: 35800, Data Step: 71600, Loss: 3.703125, Token per second per gpu: 22340.77393310762
Epoch: 3, Global Step: 35900, Data Step: 71800, Loss: 3.28125, Token per second per gpu: 22524.141238500833
Epoch: 3, Global Step: 36000, Data Step: 72000, Loss: 3.21875, Token per second per gpu: 22350.567226433162
Epoch: 3, Global Step: 36100, Data Step: 72200, Loss: 3.421875, Token per second per gpu: 22472.59918923514
Epoch: 3, Global Step: 36200, Data Step: 72400, Loss: 3.5, Token per second per gpu: 22334.87170591144
Epoch: 3, Global Step: 36300, Data Step: 72600, Loss: 3.28125, Token per second per gpu: 22666.124377744796
Epoch: 3, Global Step: 36400, Data Step: 72800, Loss: 3.390625, Token per second per gpu: 22445.625755248362
Epoch: 3, Global Step: 36500, Data Step: 73000, Loss: 3.65625, Token per second per gpu: 22700.260975674024
Epoch: 3, Global Step: 36600, Data Step: 73200, Loss: 3.25, Token per second per gpu: 21874.53434927191
Epoch: 3, Global Step: 36700, Data Step: 73400, Loss: 3.109375, Token per second per gpu: 22167.37747023712
Epoch: 3, Global Step: 36800, Data Step: 73600, Loss: 3.140625, Token per second per gpu: 22335.419890730544
Epoch: 3, Global Step: 36900, Data Step: 73800, Loss: 3.078125, Token per second per gpu: 22127.497542881683
Epoch: 3, Global Step: 37000, Data Step: 74000, Loss: 3.1875, Token per second per gpu: 22447.51149620991
Epoch: 3, Global Step: 37100, Data Step: 74200, Loss: 3.015625, Token per second per gpu: 22218.125685571915
Epoch: 3, Global Step: 37200, Data Step: 74400, Loss: 3.171875, Token per second per gpu: 22388.835763872343
Epoch: 3, Global Step: 37300, Data Step: 74600, Loss: 3.59375, Token per second per gpu: 22255.374891234496
Epoch: 3, Global Step: 37400, Data Step: 74800, Loss: 3.15625, Token per second per gpu: 22138.288192793003
Epoch: 3, Global Step: 37500, Data Step: 75000, Loss: 3.359375, Token per second per gpu: 22530.137296502264
Epoch: 3, Global Step: 37600, Data Step: 75200, Loss: 3.078125, Token per second per gpu: 22189.069287450042
Epoch: 3, Global Step: 37700, Data Step: 75400, Loss: 3.171875, Token per second per gpu: 22160.528457656183
Epoch: 3, Global Step: 37800, Data Step: 75600, Loss: 3.171875, Token per second per gpu: 22282.646510811686
Epoch: 3, Global Step: 37900, Data Step: 75800, Loss: 3.125, Token per second per gpu: 22330.421560215982
Epoch: 3, Global Step: 38000, Data Step: 76000, Loss: 3.40625, Token per second per gpu: 22074.2323765808
Epoch: 3, Global Step: 38100, Data Step: 76200, Loss: 3.234375, Token per second per gpu: 22077.258696935984
Epoch: 3, Global Step: 38200, Data Step: 76400, Loss: 3.46875, Token per second per gpu: 22196.230382786038
Epoch: 3, Global Step: 38300, Data Step: 76600, Loss: 2.984375, Token per second per gpu: 22644.075368357255
Epoch: 3, Global Step: 38400, Data Step: 76800, Loss: 3.140625, Token per second per gpu: 22196.311383133972
Epoch: 3, Global Step: 38500, Data Step: 77000, Loss: 3.171875, Token per second per gpu: 22510.94546583722
Epoch: 3, Global Step: 38600, Data Step: 77200, Loss: 3.4375, Token per second per gpu: 22721.657695835143
Epoch: 3, Global Step: 38700, Data Step: 77400, Loss: 3.171875, Token per second per gpu: 22502.28061171741
Epoch: 3, Global Step: 38800, Data Step: 77600, Loss: 3.59375, Token per second per gpu: 22129.951499707997
Epoch: 3, Global Step: 38900, Data Step: 77800, Loss: 3.484375, Token per second per gpu: 22609.416469454045
Epoch: 3, Global Step: 39000, Data Step: 78000, Loss: 3.34375, Token per second per gpu: 22308.863597764634
Epoch: 3, Global Step: 39100, Data Step: 78200, Loss: 3.1875, Token per second per gpu: 22449.43376813092
Epoch: 3, Global Step: 39200, Data Step: 78400, Loss: 3.078125, Token per second per gpu: 22481.465249080185
Epoch: 3, Global Step: 39300, Data Step: 78600, Loss: 3.71875, Token per second per gpu: 22546.18455454274
Epoch: 3, Global Step: 39400, Data Step: 78800, Loss: 3.21875, Token per second per gpu: 21308.87816367261
Epoch: 3, Global Step: 39500, Data Step: 79000, Loss: 3.265625, Token per second per gpu: 22584.5481899406
Epoch: 3, Global Step: 39600, Data Step: 79200, Loss: 2.90625, Token per second per gpu: 22235.844767540657
Epoch: 3, Global Step: 39700, Data Step: 79400, Loss: 3.359375, Token per second per gpu: 22327.185664060617
Epoch: 3, Global Step: 39800, Data Step: 79600, Loss: 3.6875, Token per second per gpu: 21696.856068780617
Epoch: 3, Global Step: 39900, Data Step: 79800, Loss: 3.140625, Token per second per gpu: 21989.79334323293
Epoch: 3, Global Step: 40000, Data Step: 80000, Loss: 3.265625, Token per second per gpu: 22294.29238735749
I0327 05:46:11.670767 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0327 05:46:11.672704 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-27 05:46:11,679] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-27 05:46:11,695] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-27 05:46:11,714] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-27 05:46:12,455] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-27 05:46:12,499] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-27 05:46:12,499] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-27 05:46:12,499] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-27 05:46:12,499] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-27 05:46:12,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-27 05:46:12,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-27 05:46:14,834] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-27 05:46:14,834] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-27 05:46:14,834] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 05:46:15,032] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-27 05:46:15,032] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-27 05:46:15,032] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 05:46:15,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-27 05:46:15,234] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-27 05:46:15,234] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 05:46:15,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-27 05:46:15,303] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-27 05:46:15,303] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 05:46:15,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-27 05:46:15,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-27 05:46:15,336] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-27 05:46:15,336] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-27 05:46:15,337] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 05:46:15,337] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0327 05:46:15.337433 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0327 05:46:15.352500 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0327 05:46:15.352764 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0327 05:46:15.354159 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 3, Global Step: 40100, Data Step: 80200, Loss: 3.0, Token per second per gpu: 21726.88573604673
Epoch: 3, Global Step: 40200, Data Step: 80400, Loss: 3.34375, Token per second per gpu: 22335.1474055653
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0327 05:50:41.359879 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 4, Global Step: 40300, Data Step: 80600, Loss: 3.296875, Token per second per gpu: 21341.59100588175
Epoch: 4, Global Step: 40400, Data Step: 80800, Loss: 3.234375, Token per second per gpu: 22093.63290805519
Epoch: 4, Global Step: 40500, Data Step: 81000, Loss: 3.0, Token per second per gpu: 22522.328140165617
Epoch: 4, Global Step: 40600, Data Step: 81200, Loss: 3.265625, Token per second per gpu: 22340.937596692405
Epoch: 4, Global Step: 40700, Data Step: 81400, Loss: 3.09375, Token per second per gpu: 22609.687689506947
Epoch: 4, Global Step: 40800, Data Step: 81600, Loss: 3.453125, Token per second per gpu: 22431.185049958356
Epoch: 4, Global Step: 40900, Data Step: 81800, Loss: 3.734375, Token per second per gpu: 22087.616220727446
Epoch: 4, Global Step: 41000, Data Step: 82000, Loss: 3.09375, Token per second per gpu: 22211.67264835272
Epoch: 4, Global Step: 41100, Data Step: 82200, Loss: 3.3125, Token per second per gpu: 22836.95301832276
Epoch: 4, Global Step: 41200, Data Step: 82400, Loss: 3.59375, Token per second per gpu: 22499.910858812935
Epoch: 4, Global Step: 41300, Data Step: 82600, Loss: 3.25, Token per second per gpu: 22399.42168987118
Epoch: 4, Global Step: 41400, Data Step: 82800, Loss: 3.421875, Token per second per gpu: 22376.892290096104
Epoch: 4, Global Step: 41500, Data Step: 83000, Loss: 3.453125, Token per second per gpu: 22578.08407278583
Epoch: 4, Global Step: 41600, Data Step: 83200, Loss: 3.53125, Token per second per gpu: 22139.044821447205
Epoch: 4, Global Step: 41700, Data Step: 83400, Loss: 3.296875, Token per second per gpu: 22367.14814805783
Epoch: 4, Global Step: 41800, Data Step: 83600, Loss: 3.203125, Token per second per gpu: 22278.991263279528
Epoch: 4, Global Step: 41900, Data Step: 83800, Loss: 3.40625, Token per second per gpu: 22405.079404544525
Epoch: 4, Global Step: 42000, Data Step: 84000, Loss: 3.109375, Token per second per gpu: 22365.915011287434
Epoch: 4, Global Step: 42100, Data Step: 84200, Loss: 3.359375, Token per second per gpu: 22330.76361292375
Epoch: 4, Global Step: 42200, Data Step: 84400, Loss: 3.09375, Token per second per gpu: 22230.172981399533
Epoch: 4, Global Step: 42300, Data Step: 84600, Loss: 3.375, Token per second per gpu: 22387.675376636926
Epoch: 4, Global Step: 42400, Data Step: 84800, Loss: 3.5, Token per second per gpu: 22098.121156841353
Epoch: 4, Global Step: 42500, Data Step: 85000, Loss: 3.03125, Token per second per gpu: 22245.611319233325
Epoch: 4, Global Step: 42600, Data Step: 85200, Loss: 3.28125, Token per second per gpu: 22214.61507195656
Epoch: 4, Global Step: 42700, Data Step: 85400, Loss: 2.96875, Token per second per gpu: 22153.7006184585
Epoch: 4, Global Step: 42800, Data Step: 85600, Loss: 3.484375, Token per second per gpu: 22349.727468418238
Epoch: 4, Global Step: 42900, Data Step: 85800, Loss: 3.109375, Token per second per gpu: 22299.895753726916
Epoch: 4, Global Step: 43000, Data Step: 86000, Loss: 3.1875, Token per second per gpu: 22437.279036945474
Epoch: 4, Global Step: 43100, Data Step: 86200, Loss: 3.203125, Token per second per gpu: 22230.206446123673
Epoch: 4, Global Step: 43200, Data Step: 86400, Loss: 3.171875, Token per second per gpu: 22503.11569142843
Epoch: 4, Global Step: 43300, Data Step: 86600, Loss: 3.28125, Token per second per gpu: 22370.45454991178
Epoch: 4, Global Step: 43400, Data Step: 86800, Loss: 3.171875, Token per second per gpu: 22149.359813843213
Epoch: 4, Global Step: 43500, Data Step: 87000, Loss: 3.46875, Token per second per gpu: 22125.03946992858
Epoch: 4, Global Step: 43600, Data Step: 87200, Loss: 2.75, Token per second per gpu: 22418.178734836678
Epoch: 4, Global Step: 43700, Data Step: 87400, Loss: 2.6875, Token per second per gpu: 22403.766125003793
Epoch: 4, Global Step: 43800, Data Step: 87600, Loss: 3.140625, Token per second per gpu: 22534.01334532459
Epoch: 4, Global Step: 43900, Data Step: 87800, Loss: 2.984375, Token per second per gpu: 22045.65938880772
Epoch: 4, Global Step: 44000, Data Step: 88000, Loss: 3.484375, Token per second per gpu: 22573.958548720497
Epoch: 4, Global Step: 44100, Data Step: 88200, Loss: 3.40625, Token per second per gpu: 22162.312761448826
Epoch: 4, Global Step: 44200, Data Step: 88400, Loss: 3.203125, Token per second per gpu: 22210.06521196459
Epoch: 4, Global Step: 44300, Data Step: 88600, Loss: 2.59375, Token per second per gpu: 21439.98824708523
Epoch: 4, Global Step: 44400, Data Step: 88800, Loss: 3.375, Token per second per gpu: 22546.859818419616
Epoch: 4, Global Step: 44500, Data Step: 89000, Loss: 3.578125, Token per second per gpu: 22271.93300567027
Epoch: 4, Global Step: 44600, Data Step: 89200, Loss: 3.40625, Token per second per gpu: 22234.096772743844
Epoch: 4, Global Step: 44700, Data Step: 89400, Loss: 3.15625, Token per second per gpu: 22138.88909324061
Epoch: 4, Global Step: 44800, Data Step: 89600, Loss: 3.59375, Token per second per gpu: 22559.167216144582
Epoch: 4, Global Step: 44900, Data Step: 89800, Loss: 3.421875, Token per second per gpu: 22450.645797165846
Epoch: 4, Global Step: 45000, Data Step: 90000, Loss: 3.390625, Token per second per gpu: 22291.07703094414
Epoch: 4, Global Step: 45100, Data Step: 90200, Loss: 3.4375, Token per second per gpu: 22327.58296013599
Epoch: 4, Global Step: 45200, Data Step: 90400, Loss: 3.15625, Token per second per gpu: 22102.104257748117
Epoch: 4, Global Step: 45300, Data Step: 90600, Loss: 3.21875, Token per second per gpu: 22420.74969397529
Epoch: 4, Global Step: 45400, Data Step: 90800, Loss: 2.953125, Token per second per gpu: 22350.85658890317
Epoch: 4, Global Step: 45500, Data Step: 91000, Loss: 3.578125, Token per second per gpu: 22146.763618617366
Epoch: 4, Global Step: 45600, Data Step: 91200, Loss: 3.109375, Token per second per gpu: 22492.613861637976
Epoch: 4, Global Step: 45700, Data Step: 91400, Loss: 3.21875, Token per second per gpu: 22632.155765197207
Epoch: 4, Global Step: 45800, Data Step: 91600, Loss: 3.328125, Token per second per gpu: 22194.73491913863
Epoch: 4, Global Step: 45900, Data Step: 91800, Loss: 3.0, Token per second per gpu: 22360.59725881589
Epoch: 4, Global Step: 46000, Data Step: 92000, Loss: 3.359375, Token per second per gpu: 22377.572457787126
Epoch: 4, Global Step: 46100, Data Step: 92200, Loss: 3.59375, Token per second per gpu: 22379.659107768926
Epoch: 4, Global Step: 46200, Data Step: 92400, Loss: 3.5, Token per second per gpu: 22170.003972399423
Epoch: 4, Global Step: 46300, Data Step: 92600, Loss: 3.109375, Token per second per gpu: 22336.309457770447
Epoch: 4, Global Step: 46400, Data Step: 92800, Loss: 3.359375, Token per second per gpu: 22110.05259147279
Epoch: 4, Global Step: 46500, Data Step: 93000, Loss: 3.265625, Token per second per gpu: 22574.854814948587
Epoch: 4, Global Step: 46600, Data Step: 93200, Loss: 3.28125, Token per second per gpu: 22290.083833579636
Epoch: 4, Global Step: 46700, Data Step: 93400, Loss: 3.015625, Token per second per gpu: 22022.981962672267
Epoch: 4, Global Step: 46800, Data Step: 93600, Loss: 3.28125, Token per second per gpu: 22217.54205226758
Epoch: 4, Global Step: 46900, Data Step: 93800, Loss: 2.9375, Token per second per gpu: 22003.740047008996
Epoch: 4, Global Step: 47000, Data Step: 94000, Loss: 2.90625, Token per second per gpu: 22331.5051346314
Epoch: 4, Global Step: 47100, Data Step: 94200, Loss: 3.296875, Token per second per gpu: 22505.58281039358
Epoch: 4, Global Step: 47200, Data Step: 94400, Loss: 3.28125, Token per second per gpu: 22315.86464698521
Epoch: 4, Global Step: 47300, Data Step: 94600, Loss: 2.953125, Token per second per gpu: 22163.104987503542
Epoch: 4, Global Step: 47400, Data Step: 94800, Loss: 3.359375, Token per second per gpu: 22542.0203253461
Epoch: 4, Global Step: 47500, Data Step: 95000, Loss: 3.40625, Token per second per gpu: 22453.043871489997
Epoch: 4, Global Step: 47600, Data Step: 95200, Loss: 3.328125, Token per second per gpu: 22039.404446743236
Epoch: 4, Global Step: 47700, Data Step: 95400, Loss: 3.34375, Token per second per gpu: 22514.177109671306
Epoch: 4, Global Step: 47800, Data Step: 95600, Loss: 3.40625, Token per second per gpu: 21953.865851921248
Epoch: 4, Global Step: 47900, Data Step: 95800, Loss: 3.296875, Token per second per gpu: 22395.697431849996
Epoch: 4, Global Step: 48000, Data Step: 96000, Loss: 3.09375, Token per second per gpu: 22184.659324426186
Epoch: 4, Global Step: 48100, Data Step: 96200, Loss: 3.203125, Token per second per gpu: 22377.988877781936
Epoch: 4, Global Step: 48200, Data Step: 96400, Loss: 3.3125, Token per second per gpu: 22389.16570676855
Epoch: 4, Global Step: 48300, Data Step: 96600, Loss: 3.140625, Token per second per gpu: 22728.005360526873
Epoch: 4, Global Step: 48400, Data Step: 96800, Loss: 3.28125, Token per second per gpu: 22250.404572133804
Epoch: 4, Global Step: 48500, Data Step: 97000, Loss: 2.84375, Token per second per gpu: 21987.13152394939
Epoch: 4, Global Step: 48600, Data Step: 97200, Loss: 3.421875, Token per second per gpu: 22291.564695029116
Epoch: 4, Global Step: 48700, Data Step: 97400, Loss: 3.375, Token per second per gpu: 22436.572981191814
Epoch: 4, Global Step: 48800, Data Step: 97600, Loss: 3.171875, Token per second per gpu: 22162.406932662372
Epoch: 4, Global Step: 48900, Data Step: 97800, Loss: 3.546875, Token per second per gpu: 22269.697940995353
Epoch: 4, Global Step: 49000, Data Step: 98000, Loss: 3.015625, Token per second per gpu: 22224.832476371765
Epoch: 4, Global Step: 49100, Data Step: 98200, Loss: 3.25, Token per second per gpu: 22213.656985138576
Epoch: 4, Global Step: 49200, Data Step: 98400, Loss: 3.359375, Token per second per gpu: 21311.07419188881
Epoch: 4, Global Step: 49300, Data Step: 98600, Loss: 3.640625, Token per second per gpu: 22456.40552641198
Epoch: 4, Global Step: 49400, Data Step: 98800, Loss: 3.21875, Token per second per gpu: 22144.24398435732
Epoch: 4, Global Step: 49500, Data Step: 99000, Loss: 3.078125, Token per second per gpu: 22688.00931050916
Epoch: 4, Global Step: 49600, Data Step: 99200, Loss: 2.90625, Token per second per gpu: 22342.24331387461
Epoch: 4, Global Step: 49700, Data Step: 99400, Loss: 3.140625, Token per second per gpu: 21588.559253196705
Epoch: 4, Global Step: 49800, Data Step: 99600, Loss: 3.28125, Token per second per gpu: 21897.313792676337
Epoch: 4, Global Step: 49900, Data Step: 99800, Loss: 3.03125, Token per second per gpu: 22397.071225303123
Epoch: 4, Global Step: 50000, Data Step: 100000, Loss: 3.40625, Token per second per gpu: 22496.668686791138
I0327 09:21:40.259249 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0327 09:21:40.259662 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-27 09:21:40,260] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-27 09:21:40,264] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-27 09:21:40,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-27 09:21:40,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-27 09:21:40,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-27 09:21:40,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-27 09:21:40,841] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-27 09:21:40,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-27 09:21:40,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-27 09:21:40,841] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-27 09:21:42,866] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-27 09:21:42,879] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-27 09:21:42,879] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 09:21:42,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-27 09:21:42,907] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-27 09:21:42,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 09:21:42,955] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-27 09:21:42,955] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-27 09:21:42,955] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 09:21:42,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-27 09:21:42,964] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-27 09:21:42,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 09:21:42,976] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-27 09:21:42,976] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-27 09:21:42,976] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 09:21:42,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-27 09:21:42,994] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-27 09:21:42,994] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0327 09:21:42.994637 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0327 09:21:42.995333 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0327 09:21:42.995588 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0327 09:21:42.997052 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 4, Global Step: 50100, Data Step: 100200, Loss: 3.078125, Token per second per gpu: 21832.22254988923
Epoch: 4, Global Step: 50200, Data Step: 100400, Loss: 3.140625, Token per second per gpu: 22103.83661526523
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0327 09:27:17.614661 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 5, Global Step: 50300, Data Step: 100600, Loss: 3.1875, Token per second per gpu: 21704.30079173598
Epoch: 5, Global Step: 50400, Data Step: 100800, Loss: 3.265625, Token per second per gpu: 22315.63971820448
Epoch: 5, Global Step: 50500, Data Step: 101000, Loss: 3.328125, Token per second per gpu: 22181.002142000023
Epoch: 5, Global Step: 50600, Data Step: 101200, Loss: 3.265625, Token per second per gpu: 22175.12397796333
Epoch: 5, Global Step: 50700, Data Step: 101400, Loss: 3.015625, Token per second per gpu: 22275.758300593356
Epoch: 5, Global Step: 50800, Data Step: 101600, Loss: 3.140625, Token per second per gpu: 22274.505153915918
Epoch: 5, Global Step: 50900, Data Step: 101800, Loss: 3.21875, Token per second per gpu: 22279.788853346825
Epoch: 5, Global Step: 51000, Data Step: 102000, Loss: 3.0, Token per second per gpu: 22314.542472119083
Epoch: 5, Global Step: 51100, Data Step: 102200, Loss: 3.515625, Token per second per gpu: 22519.845677291964
Epoch: 5, Global Step: 51200, Data Step: 102400, Loss: 3.109375, Token per second per gpu: 22373.520582213914
Epoch: 5, Global Step: 51300, Data Step: 102600, Loss: 3.0625, Token per second per gpu: 22380.646454233105
Epoch: 5, Global Step: 51400, Data Step: 102800, Loss: 3.015625, Token per second per gpu: 22637.233764239387
Epoch: 5, Global Step: 51500, Data Step: 103000, Loss: 3.375, Token per second per gpu: 22048.621817888554
Epoch: 5, Global Step: 51600, Data Step: 103200, Loss: 3.15625, Token per second per gpu: 22414.458468905916
Epoch: 5, Global Step: 51700, Data Step: 103400, Loss: 2.859375, Token per second per gpu: 22410.78779243217
Epoch: 5, Global Step: 51800, Data Step: 103600, Loss: 2.609375, Token per second per gpu: 22065.73650505793
Epoch: 5, Global Step: 51900, Data Step: 103800, Loss: 3.078125, Token per second per gpu: 22427.951784200708
Epoch: 5, Global Step: 52000, Data Step: 104000, Loss: 3.25, Token per second per gpu: 22565.741698820562
Epoch: 5, Global Step: 52100, Data Step: 104200, Loss: 3.015625, Token per second per gpu: 22122.015553414803
Epoch: 5, Global Step: 52200, Data Step: 104400, Loss: 3.359375, Token per second per gpu: 22386.35065317645
Epoch: 5, Global Step: 52300, Data Step: 104600, Loss: 3.171875, Token per second per gpu: 22392.01384477842
Epoch: 5, Global Step: 52400, Data Step: 104800, Loss: 3.359375, Token per second per gpu: 22444.089158636565
Epoch: 5, Global Step: 52500, Data Step: 105000, Loss: 2.640625, Token per second per gpu: 22231.45886836359
Epoch: 5, Global Step: 52600, Data Step: 105200, Loss: 3.296875, Token per second per gpu: 22557.036892074037
Epoch: 5, Global Step: 52700, Data Step: 105400, Loss: 3.046875, Token per second per gpu: 22477.395993266655
Epoch: 5, Global Step: 52800, Data Step: 105600, Loss: 3.078125, Token per second per gpu: 22389.674313433283
Epoch: 5, Global Step: 52900, Data Step: 105800, Loss: 3.546875, Token per second per gpu: 22547.46142902659
Epoch: 5, Global Step: 53000, Data Step: 106000, Loss: 3.140625, Token per second per gpu: 22272.612926486443
Epoch: 5, Global Step: 53100, Data Step: 106200, Loss: 3.28125, Token per second per gpu: 22377.602512469428
Epoch: 5, Global Step: 53200, Data Step: 106400, Loss: 3.1875, Token per second per gpu: 22294.40377202641
Epoch: 5, Global Step: 53300, Data Step: 106600, Loss: 3.296875, Token per second per gpu: 22449.552173957058
Epoch: 5, Global Step: 53400, Data Step: 106800, Loss: 3.15625, Token per second per gpu: 22156.625547832427
Epoch: 5, Global Step: 53500, Data Step: 107000, Loss: 3.265625, Token per second per gpu: 22288.91034096128
Epoch: 5, Global Step: 53600, Data Step: 107200, Loss: 3.375, Token per second per gpu: 22612.95244697785
Epoch: 5, Global Step: 53700, Data Step: 107400, Loss: 3.34375, Token per second per gpu: 22411.875812417115
Epoch: 5, Global Step: 53800, Data Step: 107600, Loss: 3.359375, Token per second per gpu: 22430.677679515265
Epoch: 5, Global Step: 53900, Data Step: 107800, Loss: 3.34375, Token per second per gpu: 22440.993666969258
Epoch: 5, Global Step: 54000, Data Step: 108000, Loss: 3.0, Token per second per gpu: 22329.134147297453
Epoch: 5, Global Step: 54100, Data Step: 108200, Loss: 3.515625, Token per second per gpu: 22297.946689616747
Epoch: 5, Global Step: 54200, Data Step: 108400, Loss: 3.234375, Token per second per gpu: 22045.948434041005
Epoch: 5, Global Step: 54300, Data Step: 108600, Loss: 3.0, Token per second per gpu: 22323.92566308151
Epoch: 5, Global Step: 54400, Data Step: 108800, Loss: 3.359375, Token per second per gpu: 22590.12437401941
Epoch: 5, Global Step: 54500, Data Step: 109000, Loss: 3.0625, Token per second per gpu: 22229.71062213741
Epoch: 5, Global Step: 54600, Data Step: 109200, Loss: 3.28125, Token per second per gpu: 22317.381509280644
Epoch: 5, Global Step: 54700, Data Step: 109400, Loss: 3.046875, Token per second per gpu: 22050.57499383695
Epoch: 5, Global Step: 54800, Data Step: 109600, Loss: 3.078125, Token per second per gpu: 21803.496966626797
Epoch: 5, Global Step: 54900, Data Step: 109800, Loss: 3.15625, Token per second per gpu: 22474.357792187824
Epoch: 5, Global Step: 55000, Data Step: 110000, Loss: 3.359375, Token per second per gpu: 22098.75735551034
Epoch: 5, Global Step: 55100, Data Step: 110200, Loss: 3.203125, Token per second per gpu: 22332.952527719513
Epoch: 5, Global Step: 55200, Data Step: 110400, Loss: 3.484375, Token per second per gpu: 22099.873227369822
Epoch: 5, Global Step: 55300, Data Step: 110600, Loss: 3.359375, Token per second per gpu: 22449.78748734028
Epoch: 5, Global Step: 55400, Data Step: 110800, Loss: 3.15625, Token per second per gpu: 21909.80003540075
Epoch: 5, Global Step: 55500, Data Step: 111000, Loss: 3.109375, Token per second per gpu: 22265.8313458647
Epoch: 5, Global Step: 55600, Data Step: 111200, Loss: 3.03125, Token per second per gpu: 22620.89791181578
Epoch: 5, Global Step: 55700, Data Step: 111400, Loss: 3.171875, Token per second per gpu: 22434.547078761098
Epoch: 5, Global Step: 55800, Data Step: 111600, Loss: 3.015625, Token per second per gpu: 22098.626207537356
Epoch: 5, Global Step: 55900, Data Step: 111800, Loss: 3.015625, Token per second per gpu: 22614.766279263684
Epoch: 5, Global Step: 56000, Data Step: 112000, Loss: 3.09375, Token per second per gpu: 22325.933987051794
Epoch: 5, Global Step: 56100, Data Step: 112200, Loss: 3.28125, Token per second per gpu: 22344.440721027306
Epoch: 5, Global Step: 56200, Data Step: 112400, Loss: 3.53125, Token per second per gpu: 22258.915340062907
Epoch: 5, Global Step: 56300, Data Step: 112600, Loss: 3.140625, Token per second per gpu: 22467.83736240065
Epoch: 5, Global Step: 56400, Data Step: 112800, Loss: 3.25, Token per second per gpu: 22420.300970509514
Epoch: 5, Global Step: 56500, Data Step: 113000, Loss: 3.5625, Token per second per gpu: 22349.378053059878
Epoch: 5, Global Step: 56600, Data Step: 113200, Loss: 3.34375, Token per second per gpu: 22369.694034838947
Epoch: 5, Global Step: 56700, Data Step: 113400, Loss: 3.203125, Token per second per gpu: 22133.451179095755
Epoch: 5, Global Step: 56800, Data Step: 113600, Loss: 3.203125, Token per second per gpu: 22308.211124503792
Epoch: 5, Global Step: 56900, Data Step: 113800, Loss: 3.328125, Token per second per gpu: 22685.49734918693
Epoch: 5, Global Step: 57000, Data Step: 114000, Loss: 3.328125, Token per second per gpu: 22480.093424739214
Epoch: 5, Global Step: 57100, Data Step: 114200, Loss: 3.125, Token per second per gpu: 22137.09265615918
Epoch: 5, Global Step: 57200, Data Step: 114400, Loss: 2.96875, Token per second per gpu: 22168.247274025434
Epoch: 5, Global Step: 57300, Data Step: 114600, Loss: 3.125, Token per second per gpu: 22421.524463579317
Epoch: 5, Global Step: 57400, Data Step: 114800, Loss: 3.25, Token per second per gpu: 22232.469762594013
Epoch: 5, Global Step: 57500, Data Step: 115000, Loss: 3.28125, Token per second per gpu: 22499.665315584818
Epoch: 5, Global Step: 57600, Data Step: 115200, Loss: 3.328125, Token per second per gpu: 22096.550808102038
Epoch: 5, Global Step: 57700, Data Step: 115400, Loss: 3.234375, Token per second per gpu: 22313.90248577531
Epoch: 5, Global Step: 57800, Data Step: 115600, Loss: 3.125, Token per second per gpu: 22704.212813020757
Epoch: 5, Global Step: 57900, Data Step: 115800, Loss: 3.1875, Token per second per gpu: 22664.099461188584
Epoch: 5, Global Step: 58000, Data Step: 116000, Loss: 3.4375, Token per second per gpu: 22636.578273677864
Epoch: 5, Global Step: 58100, Data Step: 116200, Loss: 3.1875, Token per second per gpu: 22280.708515579063
Epoch: 5, Global Step: 58200, Data Step: 116400, Loss: 2.984375, Token per second per gpu: 22761.092138942262
Epoch: 5, Global Step: 58300, Data Step: 116600, Loss: 3.171875, Token per second per gpu: 22465.90153377465
Epoch: 5, Global Step: 58400, Data Step: 116800, Loss: 3.484375, Token per second per gpu: 22217.76406796103
Epoch: 5, Global Step: 58500, Data Step: 117000, Loss: 3.1875, Token per second per gpu: 22349.19069649596
Epoch: 5, Global Step: 58600, Data Step: 117200, Loss: 3.375, Token per second per gpu: 22591.661638949692
Epoch: 5, Global Step: 58700, Data Step: 117400, Loss: 2.9375, Token per second per gpu: 22795.819961877583
Epoch: 5, Global Step: 58800, Data Step: 117600, Loss: 3.140625, Token per second per gpu: 22707.630254788088
Epoch: 5, Global Step: 58900, Data Step: 117800, Loss: 2.796875, Token per second per gpu: 22392.619050712172
Epoch: 5, Global Step: 59000, Data Step: 118000, Loss: 3.265625, Token per second per gpu: 22578.286807760775
Epoch: 5, Global Step: 59100, Data Step: 118200, Loss: 3.34375, Token per second per gpu: 21225.542091920277
Epoch: 5, Global Step: 59200, Data Step: 118400, Loss: 3.171875, Token per second per gpu: 22460.311036525425
Epoch: 5, Global Step: 59300, Data Step: 118600, Loss: 3.21875, Token per second per gpu: 22284.27980470584
Epoch: 5, Global Step: 59400, Data Step: 118800, Loss: 3.328125, Token per second per gpu: 22406.44050923309
Epoch: 5, Global Step: 59500, Data Step: 119000, Loss: 3.28125, Token per second per gpu: 22712.57230034116
Epoch: 5, Global Step: 59600, Data Step: 119200, Loss: 3.265625, Token per second per gpu: 21903.675239550546
Epoch: 5, Global Step: 59700, Data Step: 119400, Loss: 3.046875, Token per second per gpu: 22715.44761771194
Epoch: 5, Global Step: 59800, Data Step: 119600, Loss: 3.078125, Token per second per gpu: 21904.641926029697
Epoch: 5, Global Step: 59900, Data Step: 119800, Loss: 3.046875, Token per second per gpu: 22315.512785507242
Epoch: 5, Global Step: 60000, Data Step: 120000, Loss: 2.9375, Token per second per gpu: 22240.05632519682
I0327 12:56:37.516922 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0327 12:56:37.517382 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-27 12:56:37,524] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-27 12:56:37,564] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-27 12:56:37,566] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-27 12:56:38,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-27 12:56:38,937] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-27 12:56:38,937] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-27 12:56:38,939] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-27 12:56:38,947] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-27 12:56:38,948] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-27 12:56:38,953] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-27 12:56:39,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-27 12:56:39,445] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-27 12:56:39,445] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 12:56:40,544] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-27 12:56:40,544] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-27 12:56:40,544] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 12:56:40,675] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-27 12:56:40,675] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-27 12:56:40,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 12:56:40,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-27 12:56:40,685] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-27 12:56:40,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 12:56:40,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-27 12:56:40,752] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-27 12:56:40,752] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 12:56:40,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-27 12:56:40,784] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-27 12:56:40,784] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0327 12:56:40.785034 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0327 12:56:40.795604 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0327 12:56:40.796424 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0327 12:56:40.800277 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 5, Global Step: 60100, Data Step: 120200, Loss: 3.21875, Token per second per gpu: 21576.527594329724
Epoch: 5, Global Step: 60200, Data Step: 120400, Loss: 3.328125, Token per second per gpu: 22263.775099899456
Epoch: 5, Global Step: 60300, Data Step: 120600, Loss: 3.3125, Token per second per gpu: 22202.05367675811
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0327 13:03:20.219426 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 6, Global Step: 60400, Data Step: 120800, Loss: 3.203125, Token per second per gpu: 21951.835982833898
Epoch: 6, Global Step: 60500, Data Step: 121000, Loss: 2.8125, Token per second per gpu: 22214.497660720357
Epoch: 6, Global Step: 60600, Data Step: 121200, Loss: 3.203125, Token per second per gpu: 22499.773397529945
Epoch: 6, Global Step: 60700, Data Step: 121400, Loss: 3.109375, Token per second per gpu: 22627.220251854415
Epoch: 6, Global Step: 60800, Data Step: 121600, Loss: 3.265625, Token per second per gpu: 22705.032860846328
Epoch: 6, Global Step: 60900, Data Step: 121800, Loss: 3.21875, Token per second per gpu: 22463.007083870238
Epoch: 6, Global Step: 61000, Data Step: 122000, Loss: 3.125, Token per second per gpu: 22528.05778452322
Epoch: 6, Global Step: 61100, Data Step: 122200, Loss: 3.09375, Token per second per gpu: 22362.720736075975
Epoch: 6, Global Step: 61200, Data Step: 122400, Loss: 3.28125, Token per second per gpu: 22247.7689432453
Epoch: 6, Global Step: 61300, Data Step: 122600, Loss: 3.125, Token per second per gpu: 22224.83022738321
Epoch: 6, Global Step: 61400, Data Step: 122800, Loss: 3.328125, Token per second per gpu: 21939.52199137034
Epoch: 6, Global Step: 61500, Data Step: 123000, Loss: 3.25, Token per second per gpu: 22393.982876427486
Epoch: 6, Global Step: 61600, Data Step: 123200, Loss: 3.15625, Token per second per gpu: 22545.952139470286
Epoch: 6, Global Step: 61700, Data Step: 123400, Loss: 3.421875, Token per second per gpu: 22558.211616828557
Epoch: 6, Global Step: 61800, Data Step: 123600, Loss: 3.171875, Token per second per gpu: 22622.454620125958
Epoch: 6, Global Step: 61900, Data Step: 123800, Loss: 3.234375, Token per second per gpu: 22339.775971814368
Epoch: 6, Global Step: 62000, Data Step: 124000, Loss: 3.421875, Token per second per gpu: 22418.558098036116
Epoch: 6, Global Step: 62100, Data Step: 124200, Loss: 3.34375, Token per second per gpu: 22320.79830716945
Epoch: 6, Global Step: 62200, Data Step: 124400, Loss: 2.84375, Token per second per gpu: 22330.858189085488
Epoch: 6, Global Step: 62300, Data Step: 124600, Loss: 3.40625, Token per second per gpu: 22312.080090208692
Epoch: 6, Global Step: 62400, Data Step: 124800, Loss: 3.1875, Token per second per gpu: 22261.954523444663
Epoch: 6, Global Step: 62500, Data Step: 125000, Loss: 3.578125, Token per second per gpu: 22118.971321256922
Epoch: 6, Global Step: 62600, Data Step: 125200, Loss: 2.765625, Token per second per gpu: 22153.61785672493
Epoch: 6, Global Step: 62700, Data Step: 125400, Loss: 3.109375, Token per second per gpu: 22783.585410083302
Epoch: 6, Global Step: 62800, Data Step: 125600, Loss: 3.546875, Token per second per gpu: 22407.3693271385
Epoch: 6, Global Step: 62900, Data Step: 125800, Loss: 2.6875, Token per second per gpu: 22245.67363052125
Epoch: 6, Global Step: 63000, Data Step: 126000, Loss: 3.359375, Token per second per gpu: 22255.269062555344
Epoch: 6, Global Step: 63100, Data Step: 126200, Loss: 3.46875, Token per second per gpu: 22295.070045934568
Epoch: 6, Global Step: 63200, Data Step: 126400, Loss: 3.390625, Token per second per gpu: 21822.47076704931
Epoch: 6, Global Step: 63300, Data Step: 126600, Loss: 3.578125, Token per second per gpu: 22267.295030464964
Epoch: 6, Global Step: 63400, Data Step: 126800, Loss: 3.390625, Token per second per gpu: 22375.131458067455
Epoch: 6, Global Step: 63500, Data Step: 127000, Loss: 3.125, Token per second per gpu: 21986.80864162715
Epoch: 6, Global Step: 63600, Data Step: 127200, Loss: 3.203125, Token per second per gpu: 22322.31390150758
Epoch: 6, Global Step: 63700, Data Step: 127400, Loss: 3.25, Token per second per gpu: 22222.88379624071
Epoch: 6, Global Step: 63800, Data Step: 127600, Loss: 3.046875, Token per second per gpu: 22032.475657913757
Epoch: 6, Global Step: 63900, Data Step: 127800, Loss: 2.953125, Token per second per gpu: 22397.13592448332
Epoch: 6, Global Step: 64000, Data Step: 128000, Loss: 3.265625, Token per second per gpu: 22556.791111340754
Epoch: 6, Global Step: 64100, Data Step: 128200, Loss: 3.234375, Token per second per gpu: 22279.811865551317
Epoch: 6, Global Step: 64200, Data Step: 128400, Loss: 3.078125, Token per second per gpu: 22297.636675388505
Epoch: 6, Global Step: 64300, Data Step: 128600, Loss: 3.234375, Token per second per gpu: 22556.49997235336
Epoch: 6, Global Step: 64400, Data Step: 128800, Loss: 3.3125, Token per second per gpu: 22218.44460864531
Epoch: 6, Global Step: 64500, Data Step: 129000, Loss: 3.25, Token per second per gpu: 22260.485305480484
Epoch: 6, Global Step: 64600, Data Step: 129200, Loss: 3.484375, Token per second per gpu: 22379.380152151756
Epoch: 6, Global Step: 64700, Data Step: 129400, Loss: 3.140625, Token per second per gpu: 22397.263081470985
Epoch: 6, Global Step: 64800, Data Step: 129600, Loss: 3.171875, Token per second per gpu: 22249.96112481136
Epoch: 6, Global Step: 64900, Data Step: 129800, Loss: 3.328125, Token per second per gpu: 22349.39620584908
Epoch: 6, Global Step: 65000, Data Step: 130000, Loss: 3.03125, Token per second per gpu: 22298.058234285123
Epoch: 6, Global Step: 65100, Data Step: 130200, Loss: 3.359375, Token per second per gpu: 22297.480683703743
Epoch: 6, Global Step: 65200, Data Step: 130400, Loss: 3.40625, Token per second per gpu: 22161.024737347518
Epoch: 6, Global Step: 65300, Data Step: 130600, Loss: 3.015625, Token per second per gpu: 22297.346302115835
Epoch: 6, Global Step: 65400, Data Step: 130800, Loss: 3.125, Token per second per gpu: 22638.285844012575
Epoch: 6, Global Step: 65500, Data Step: 131000, Loss: 3.40625, Token per second per gpu: 22349.184328660882
Epoch: 6, Global Step: 65600, Data Step: 131200, Loss: 3.21875, Token per second per gpu: 22231.380107090303
Epoch: 6, Global Step: 65700, Data Step: 131400, Loss: 3.296875, Token per second per gpu: 22365.15174059187
Epoch: 6, Global Step: 65800, Data Step: 131600, Loss: 2.78125, Token per second per gpu: 22427.094126095573
Epoch: 6, Global Step: 65900, Data Step: 131800, Loss: 3.109375, Token per second per gpu: 22553.09956897679
Epoch: 6, Global Step: 66000, Data Step: 132000, Loss: 3.296875, Token per second per gpu: 22288.273427877943
Epoch: 6, Global Step: 66100, Data Step: 132200, Loss: 3.125, Token per second per gpu: 22394.67890544715
Epoch: 6, Global Step: 66200, Data Step: 132400, Loss: 2.953125, Token per second per gpu: 22336.94457574876
Epoch: 6, Global Step: 66300, Data Step: 132600, Loss: 3.453125, Token per second per gpu: 22380.512146443114
Epoch: 6, Global Step: 66400, Data Step: 132800, Loss: 3.484375, Token per second per gpu: 22410.62909096612
Epoch: 6, Global Step: 66500, Data Step: 133000, Loss: 3.34375, Token per second per gpu: 21795.118438359663
Epoch: 6, Global Step: 66600, Data Step: 133200, Loss: 3.015625, Token per second per gpu: 22373.539685916992
Epoch: 6, Global Step: 66700, Data Step: 133400, Loss: 3.25, Token per second per gpu: 22284.231706404626
Epoch: 6, Global Step: 66800, Data Step: 133600, Loss: 2.921875, Token per second per gpu: 22598.840535894822
Epoch: 6, Global Step: 66900, Data Step: 133800, Loss: 3.203125, Token per second per gpu: 22455.284459414732
Epoch: 6, Global Step: 67000, Data Step: 134000, Loss: 3.28125, Token per second per gpu: 22273.86216383696
Epoch: 6, Global Step: 67100, Data Step: 134200, Loss: 3.140625, Token per second per gpu: 22338.432496616493
Epoch: 6, Global Step: 67200, Data Step: 134400, Loss: 3.46875, Token per second per gpu: 22259.659111979247
Epoch: 6, Global Step: 67300, Data Step: 134600, Loss: 3.21875, Token per second per gpu: 22435.299134315585
Epoch: 6, Global Step: 67400, Data Step: 134800, Loss: 3.296875, Token per second per gpu: 22239.762372406967
Epoch: 6, Global Step: 67500, Data Step: 135000, Loss: 3.46875, Token per second per gpu: 22078.141861249023
Epoch: 6, Global Step: 67600, Data Step: 135200, Loss: 3.125, Token per second per gpu: 22239.741612975915
Epoch: 6, Global Step: 67700, Data Step: 135400, Loss: 3.484375, Token per second per gpu: 22356.73147699756
Epoch: 6, Global Step: 67800, Data Step: 135600, Loss: 3.296875, Token per second per gpu: 22116.853750602502
Epoch: 6, Global Step: 67900, Data Step: 135800, Loss: 3.40625, Token per second per gpu: 22390.806436307503
Epoch: 6, Global Step: 68000, Data Step: 136000, Loss: 2.78125, Token per second per gpu: 22546.961746967972
Epoch: 6, Global Step: 68100, Data Step: 136200, Loss: 3.09375, Token per second per gpu: 22302.728553077344
Epoch: 6, Global Step: 68200, Data Step: 136400, Loss: 3.171875, Token per second per gpu: 22344.607414289992
Epoch: 6, Global Step: 68300, Data Step: 136600, Loss: 2.9375, Token per second per gpu: 22330.283107392443
Epoch: 6, Global Step: 68400, Data Step: 136800, Loss: 3.3125, Token per second per gpu: 22364.928135997063
Epoch: 6, Global Step: 68500, Data Step: 137000, Loss: 3.15625, Token per second per gpu: 22631.040741072396
Epoch: 6, Global Step: 68600, Data Step: 137200, Loss: 2.96875, Token per second per gpu: 22462.917149502286
Epoch: 6, Global Step: 68700, Data Step: 137400, Loss: 3.234375, Token per second per gpu: 22441.34324296721
Epoch: 6, Global Step: 68800, Data Step: 137600, Loss: 3.21875, Token per second per gpu: 22547.240056157796
Epoch: 6, Global Step: 68900, Data Step: 137800, Loss: 3.359375, Token per second per gpu: 21489.336414521105
Epoch: 6, Global Step: 69000, Data Step: 138000, Loss: 3.265625, Token per second per gpu: 22472.108547103602
Epoch: 6, Global Step: 69100, Data Step: 138200, Loss: 2.875, Token per second per gpu: 22526.417295228523
Epoch: 6, Global Step: 69200, Data Step: 138400, Loss: 3.359375, Token per second per gpu: 22424.692682640325
Epoch: 6, Global Step: 69300, Data Step: 138600, Loss: 3.0, Token per second per gpu: 22470.373118474105
Epoch: 6, Global Step: 69400, Data Step: 138800, Loss: 3.0, Token per second per gpu: 22530.56264981181
Epoch: 6, Global Step: 69500, Data Step: 139000, Loss: 3.34375, Token per second per gpu: 21693.97875598437
Epoch: 6, Global Step: 69600, Data Step: 139200, Loss: 3.28125, Token per second per gpu: 22619.57636489109
Epoch: 6, Global Step: 69700, Data Step: 139400, Loss: 3.359375, Token per second per gpu: 21681.32685447509
Epoch: 6, Global Step: 69800, Data Step: 139600, Loss: 2.984375, Token per second per gpu: 22150.591647902475
Epoch: 6, Global Step: 69900, Data Step: 139800, Loss: 3.09375, Token per second per gpu: 22202.60246189339
Epoch: 6, Global Step: 70000, Data Step: 140000, Loss: 3.171875, Token per second per gpu: 22077.18788383244
I0327 16:31:47.180870 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0327 16:31:47.181299 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-27 16:31:47,181] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-27 16:31:47,186] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-27 16:31:47,186] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-27 16:31:47,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-27 16:31:47,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-27 16:31:47,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-27 16:31:47,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-27 16:31:47,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-27 16:31:47,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-27 16:31:47,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-27 16:31:49,712] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-27 16:31:49,712] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-27 16:31:49,712] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 16:31:49,801] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-27 16:31:49,801] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-27 16:31:49,801] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-27 16:31:49,801] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 16:31:49,817] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-27 16:31:49,817] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 16:31:49,828] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-27 16:31:49,828] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-27 16:31:49,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 16:31:49,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-27 16:31:49,838] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-27 16:31:49,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 16:31:49,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-27 16:31:49,849] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-27 16:31:49,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0327 16:31:49.849901 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0327 16:31:49.850625 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0327 16:31:49.850883 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0327 16:31:49.852268 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 6, Global Step: 70100, Data Step: 140200, Loss: 3.359375, Token per second per gpu: 21885.559209636278
Epoch: 6, Global Step: 70200, Data Step: 140400, Loss: 3.359375, Token per second per gpu: 22309.925055589727
Epoch: 6, Global Step: 70300, Data Step: 140600, Loss: 3.109375, Token per second per gpu: 22162.135074421567
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0327 16:39:36.361302 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 7, Global Step: 70400, Data Step: 140800, Loss: 3.015625, Token per second per gpu: 21570.553327379886
Epoch: 7, Global Step: 70500, Data Step: 141000, Loss: 2.890625, Token per second per gpu: 22595.16319784288
Epoch: 7, Global Step: 70600, Data Step: 141200, Loss: 3.296875, Token per second per gpu: 22122.06335914324
Epoch: 7, Global Step: 70700, Data Step: 141400, Loss: 3.40625, Token per second per gpu: 22298.546862476665
Epoch: 7, Global Step: 70800, Data Step: 141600, Loss: 2.953125, Token per second per gpu: 22222.424683629735
Epoch: 7, Global Step: 70900, Data Step: 141800, Loss: 3.203125, Token per second per gpu: 22385.50733297611
Epoch: 7, Global Step: 71000, Data Step: 142000, Loss: 3.125, Token per second per gpu: 22426.768685393763
Epoch: 7, Global Step: 71100, Data Step: 142200, Loss: 3.421875, Token per second per gpu: 22354.535168288578
Epoch: 7, Global Step: 71200, Data Step: 142400, Loss: 2.984375, Token per second per gpu: 22004.870915769905
Epoch: 7, Global Step: 71300, Data Step: 142600, Loss: 3.34375, Token per second per gpu: 22183.30539453246
Epoch: 7, Global Step: 71400, Data Step: 142800, Loss: 3.359375, Token per second per gpu: 22208.529584267795
Epoch: 7, Global Step: 71500, Data Step: 143000, Loss: 3.34375, Token per second per gpu: 21763.929404644856
Epoch: 7, Global Step: 71600, Data Step: 143200, Loss: 3.171875, Token per second per gpu: 22399.699193119686
Epoch: 7, Global Step: 71700, Data Step: 143400, Loss: 3.3125, Token per second per gpu: 22203.804472838554
Epoch: 7, Global Step: 71800, Data Step: 143600, Loss: 3.40625, Token per second per gpu: 22223.344604116817
Epoch: 7, Global Step: 71900, Data Step: 143800, Loss: 3.34375, Token per second per gpu: 22405.926444307715
Epoch: 7, Global Step: 72000, Data Step: 144000, Loss: 3.046875, Token per second per gpu: 22038.8156494879
Epoch: 7, Global Step: 72100, Data Step: 144200, Loss: 3.1875, Token per second per gpu: 22428.31448792377
Epoch: 7, Global Step: 72200, Data Step: 144400, Loss: 2.890625, Token per second per gpu: 22236.763629927947
Epoch: 7, Global Step: 72300, Data Step: 144600, Loss: 3.03125, Token per second per gpu: 21870.595223489996
Epoch: 7, Global Step: 72400, Data Step: 144800, Loss: 3.125, Token per second per gpu: 22151.309309537897
Epoch: 7, Global Step: 72500, Data Step: 145000, Loss: 3.15625, Token per second per gpu: 22320.088510244404
Epoch: 7, Global Step: 72600, Data Step: 145200, Loss: 3.4375, Token per second per gpu: 22080.247666199615
Epoch: 7, Global Step: 72700, Data Step: 145400, Loss: 3.203125, Token per second per gpu: 22159.45750921713
Epoch: 7, Global Step: 72800, Data Step: 145600, Loss: 2.953125, Token per second per gpu: 22420.069270942287
Epoch: 7, Global Step: 72900, Data Step: 145800, Loss: 3.015625, Token per second per gpu: 22987.11474651056
Epoch: 7, Global Step: 73000, Data Step: 146000, Loss: 3.328125, Token per second per gpu: 22271.171865559096
Epoch: 7, Global Step: 73100, Data Step: 146200, Loss: 3.375, Token per second per gpu: 22281.971073823508
Epoch: 7, Global Step: 73200, Data Step: 146400, Loss: 3.25, Token per second per gpu: 22404.113420192996
Epoch: 7, Global Step: 73300, Data Step: 146600, Loss: 3.046875, Token per second per gpu: 22532.734630962856
Epoch: 7, Global Step: 73400, Data Step: 146800, Loss: 3.453125, Token per second per gpu: 22342.738756938197
Epoch: 7, Global Step: 73500, Data Step: 147000, Loss: 3.25, Token per second per gpu: 22376.59321676951
Epoch: 7, Global Step: 73600, Data Step: 147200, Loss: 3.109375, Token per second per gpu: 22436.364781993867
Epoch: 7, Global Step: 73700, Data Step: 147400, Loss: 3.375, Token per second per gpu: 22279.808906836646
Epoch: 7, Global Step: 73800, Data Step: 147600, Loss: 3.359375, Token per second per gpu: 23048.950873460177
Epoch: 7, Global Step: 73900, Data Step: 147800, Loss: 3.4375, Token per second per gpu: 22344.255141818336
Epoch: 7, Global Step: 74000, Data Step: 148000, Loss: 3.25, Token per second per gpu: 22096.48973361248
Epoch: 7, Global Step: 74100, Data Step: 148200, Loss: 3.171875, Token per second per gpu: 22389.756441300997
Epoch: 7, Global Step: 74200, Data Step: 148400, Loss: 3.21875, Token per second per gpu: 22458.390617626235
Epoch: 7, Global Step: 74300, Data Step: 148600, Loss: 3.640625, Token per second per gpu: 22389.776278238616
Epoch: 7, Global Step: 74400, Data Step: 148800, Loss: 3.0625, Token per second per gpu: 22411.322745605274
Epoch: 7, Global Step: 74500, Data Step: 149000, Loss: 3.109375, Token per second per gpu: 22388.446202261563
Epoch: 7, Global Step: 74600, Data Step: 149200, Loss: 3.09375, Token per second per gpu: 22293.94832397583
Epoch: 7, Global Step: 74700, Data Step: 149400, Loss: 3.046875, Token per second per gpu: 22314.688932998986
Epoch: 7, Global Step: 74800, Data Step: 149600, Loss: 3.296875, Token per second per gpu: 22418.399993804298
Epoch: 7, Global Step: 74900, Data Step: 149800, Loss: 2.90625, Token per second per gpu: 22281.273936570673
Epoch: 7, Global Step: 75000, Data Step: 150000, Loss: 3.328125, Token per second per gpu: 22209.01931892515
Epoch: 7, Global Step: 75100, Data Step: 150200, Loss: 3.5, Token per second per gpu: 22219.13965890698
Epoch: 7, Global Step: 75200, Data Step: 150400, Loss: 3.234375, Token per second per gpu: 22367.69866394651
Epoch: 7, Global Step: 75300, Data Step: 150600, Loss: 3.125, Token per second per gpu: 22542.533881214622
Epoch: 7, Global Step: 75400, Data Step: 150800, Loss: 3.21875, Token per second per gpu: 22567.936658797767
Epoch: 7, Global Step: 75500, Data Step: 151000, Loss: 3.453125, Token per second per gpu: 22189.627947276207
Epoch: 7, Global Step: 75600, Data Step: 151200, Loss: 3.203125, Token per second per gpu: 22121.93080003801
Epoch: 7, Global Step: 75700, Data Step: 151400, Loss: 3.40625, Token per second per gpu: 22348.486824590273
Epoch: 7, Global Step: 75800, Data Step: 151600, Loss: 2.953125, Token per second per gpu: 22288.293496644194
Epoch: 7, Global Step: 75900, Data Step: 151800, Loss: 2.875, Token per second per gpu: 22449.07150746307
Epoch: 7, Global Step: 76000, Data Step: 152000, Loss: 3.078125, Token per second per gpu: 22425.181048280065
Epoch: 7, Global Step: 76100, Data Step: 152200, Loss: 3.453125, Token per second per gpu: 22490.53147175379
Epoch: 7, Global Step: 76200, Data Step: 152400, Loss: 3.328125, Token per second per gpu: 22023.93130130956
Epoch: 7, Global Step: 76300, Data Step: 152600, Loss: 3.3125, Token per second per gpu: 22582.59354931554
Epoch: 7, Global Step: 76400, Data Step: 152800, Loss: 2.984375, Token per second per gpu: 22287.589301962023
Epoch: 7, Global Step: 76500, Data Step: 153000, Loss: 2.6875, Token per second per gpu: 22443.883030062007
Epoch: 7, Global Step: 76600, Data Step: 153200, Loss: 2.890625, Token per second per gpu: 22455.467921851374
Epoch: 7, Global Step: 76700, Data Step: 153400, Loss: 3.171875, Token per second per gpu: 22289.664550369907
Epoch: 7, Global Step: 76800, Data Step: 153600, Loss: 3.65625, Token per second per gpu: 22255.875797235894
Epoch: 7, Global Step: 76900, Data Step: 153800, Loss: 3.21875, Token per second per gpu: 22145.391371777157
Epoch: 7, Global Step: 77000, Data Step: 154000, Loss: 3.265625, Token per second per gpu: 22152.816233467103
Epoch: 7, Global Step: 77100, Data Step: 154200, Loss: 2.859375, Token per second per gpu: 22502.673683784185
Epoch: 7, Global Step: 77200, Data Step: 154400, Loss: 3.203125, Token per second per gpu: 22355.569864092868
Epoch: 7, Global Step: 77300, Data Step: 154600, Loss: 2.9375, Token per second per gpu: 22295.700392852505
Epoch: 7, Global Step: 77400, Data Step: 154800, Loss: 3.21875, Token per second per gpu: 22660.50649184751
Epoch: 7, Global Step: 77500, Data Step: 155000, Loss: 3.171875, Token per second per gpu: 22128.250474976452
Epoch: 7, Global Step: 77600, Data Step: 155200, Loss: 3.28125, Token per second per gpu: 21026.23801012606
Epoch: 7, Global Step: 77700, Data Step: 155400, Loss: 3.203125, Token per second per gpu: 22450.37867139637
Epoch: 7, Global Step: 77800, Data Step: 155600, Loss: 3.40625, Token per second per gpu: 22304.88854017089
Epoch: 7, Global Step: 77900, Data Step: 155800, Loss: 3.34375, Token per second per gpu: 22361.201016042585
Epoch: 7, Global Step: 78000, Data Step: 156000, Loss: 3.25, Token per second per gpu: 22269.22749063522
Epoch: 7, Global Step: 78100, Data Step: 156200, Loss: 3.15625, Token per second per gpu: 22388.956519105293
Epoch: 7, Global Step: 78200, Data Step: 156400, Loss: 2.859375, Token per second per gpu: 22466.841764860314
Epoch: 7, Global Step: 78300, Data Step: 156600, Loss: 3.5, Token per second per gpu: 22364.964823407296
Epoch: 7, Global Step: 78400, Data Step: 156800, Loss: 3.28125, Token per second per gpu: 22113.509909874305
Epoch: 7, Global Step: 78500, Data Step: 157000, Loss: 3.234375, Token per second per gpu: 22230.983445160426
Epoch: 7, Global Step: 78600, Data Step: 157200, Loss: 2.859375, Token per second per gpu: 22372.68510683512
Epoch: 7, Global Step: 78700, Data Step: 157400, Loss: 2.84375, Token per second per gpu: 21946.87725104966
Epoch: 7, Global Step: 78800, Data Step: 157600, Loss: 3.25, Token per second per gpu: 22133.099044436498
Epoch: 7, Global Step: 78900, Data Step: 157800, Loss: 3.28125, Token per second per gpu: 22165.112013043225
Epoch: 7, Global Step: 79000, Data Step: 158000, Loss: 2.9375, Token per second per gpu: 22349.949693999795
Epoch: 7, Global Step: 79100, Data Step: 158200, Loss: 3.234375, Token per second per gpu: 22198.33515622132
Epoch: 7, Global Step: 79200, Data Step: 158400, Loss: 3.296875, Token per second per gpu: 22338.92822510943
Epoch: 7, Global Step: 79300, Data Step: 158600, Loss: 3.25, Token per second per gpu: 22225.435467069274
Epoch: 7, Global Step: 79400, Data Step: 158800, Loss: 3.28125, Token per second per gpu: 22145.74255788027
Epoch: 7, Global Step: 79500, Data Step: 159000, Loss: 3.4375, Token per second per gpu: 21469.529059744225
Epoch: 7, Global Step: 79600, Data Step: 159200, Loss: 3.046875, Token per second per gpu: 22070.632881113885
Epoch: 7, Global Step: 79700, Data Step: 159400, Loss: 3.453125, Token per second per gpu: 21045.947495254735
Epoch: 7, Global Step: 79800, Data Step: 159600, Loss: 3.0, Token per second per gpu: 22524.142918480848
Epoch: 7, Global Step: 79900, Data Step: 159800, Loss: 3.375, Token per second per gpu: 22330.391178054
Epoch: 7, Global Step: 80000, Data Step: 160000, Loss: 3.25, Token per second per gpu: 22330.30560482745
I0327 20:07:23.778556 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0327 20:07:23.780488 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-27 20:07:23,786] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-27 20:07:23,842] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-27 20:07:23,847] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-27 20:07:24,724] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-27 20:07:24,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-27 20:07:24,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-27 20:07:24,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-27 20:07:24,820] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-27 20:07:24,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-27 20:07:24,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-27 20:07:27,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-27 20:07:27,135] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-27 20:07:27,135] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 20:07:27,265] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-27 20:07:27,277] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-27 20:07:27,277] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-27 20:07:27,277] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-27 20:07:27,277] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 20:07:27,277] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 20:07:27,314] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-27 20:07:27,314] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-27 20:07:27,314] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 20:07:27,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-27 20:07:27,349] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-27 20:07:27,349] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 20:07:27,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-27 20:07:27,365] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-27 20:07:27,366] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0327 20:07:27.366421 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0327 20:07:27.367149 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0327 20:07:27.367419 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0327 20:07:27.368840 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 7, Global Step: 80100, Data Step: 160200, Loss: 3.171875, Token per second per gpu: 21844.237289562356
Epoch: 7, Global Step: 80200, Data Step: 160400, Loss: 3.125, Token per second per gpu: 22412.261698820046
Epoch: 7, Global Step: 80300, Data Step: 160600, Loss: 3.21875, Token per second per gpu: 22349.425812739384
Epoch: 7, Global Step: 80400, Data Step: 160800, Loss: 3.265625, Token per second per gpu: 22683.472973875803
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0327 20:16:15.984161 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 8, Global Step: 80500, Data Step: 161000, Loss: 3.0625, Token per second per gpu: 22070.11918957709
Epoch: 8, Global Step: 80600, Data Step: 161200, Loss: 3.15625, Token per second per gpu: 22354.978739462767
Epoch: 8, Global Step: 80700, Data Step: 161400, Loss: 2.921875, Token per second per gpu: 22267.616557082514
Epoch: 8, Global Step: 80800, Data Step: 161600, Loss: 3.28125, Token per second per gpu: 22617.27174712636
Epoch: 8, Global Step: 80900, Data Step: 161800, Loss: 3.5, Token per second per gpu: 22469.490395429548
Epoch: 8, Global Step: 81000, Data Step: 162000, Loss: 3.1875, Token per second per gpu: 22507.37559999653
Epoch: 8, Global Step: 81100, Data Step: 162200, Loss: 3.359375, Token per second per gpu: 22148.667132373877
Epoch: 8, Global Step: 81200, Data Step: 162400, Loss: 3.3125, Token per second per gpu: 22577.795084677462
Epoch: 8, Global Step: 81300, Data Step: 162600, Loss: 3.140625, Token per second per gpu: 22399.77238102257
Epoch: 8, Global Step: 81400, Data Step: 162800, Loss: 3.125, Token per second per gpu: 22111.28807099831
Epoch: 8, Global Step: 81500, Data Step: 163000, Loss: 3.34375, Token per second per gpu: 22416.157109446936
Epoch: 8, Global Step: 81600, Data Step: 163200, Loss: 3.09375, Token per second per gpu: 22680.330393558048
Epoch: 8, Global Step: 81700, Data Step: 163400, Loss: 2.953125, Token per second per gpu: 22524.57673970776
Epoch: 8, Global Step: 81800, Data Step: 163600, Loss: 3.140625, Token per second per gpu: 22530.561431131886
Epoch: 8, Global Step: 81900, Data Step: 163800, Loss: 3.265625, Token per second per gpu: 22421.58455967251
Epoch: 8, Global Step: 82000, Data Step: 164000, Loss: 3.296875, Token per second per gpu: 22251.144045511628
Epoch: 8, Global Step: 82100, Data Step: 164200, Loss: 3.234375, Token per second per gpu: 22243.066932568603
Epoch: 8, Global Step: 82200, Data Step: 164400, Loss: 3.4375, Token per second per gpu: 22142.395334332567
Epoch: 8, Global Step: 82300, Data Step: 164600, Loss: 3.25, Token per second per gpu: 22402.28083032747
Epoch: 8, Global Step: 82400, Data Step: 164800, Loss: 3.265625, Token per second per gpu: 22614.185922880457
Epoch: 8, Global Step: 82500, Data Step: 165000, Loss: 3.234375, Token per second per gpu: 22493.806221757528
Epoch: 8, Global Step: 82600, Data Step: 165200, Loss: 3.3125, Token per second per gpu: 22651.046746856682
Epoch: 8, Global Step: 82700, Data Step: 165400, Loss: 3.203125, Token per second per gpu: 22211.38271268628
Epoch: 8, Global Step: 82800, Data Step: 165600, Loss: 3.046875, Token per second per gpu: 22439.93520809046
Epoch: 8, Global Step: 82900, Data Step: 165800, Loss: 3.3125, Token per second per gpu: 22154.18521564855
Epoch: 8, Global Step: 83000, Data Step: 166000, Loss: 3.1875, Token per second per gpu: 22315.28580260796
Epoch: 8, Global Step: 83100, Data Step: 166200, Loss: 3.15625, Token per second per gpu: 22146.869229961838
Epoch: 8, Global Step: 83200, Data Step: 166400, Loss: 2.734375, Token per second per gpu: 22375.038620230087
Epoch: 8, Global Step: 83300, Data Step: 166600, Loss: 3.078125, Token per second per gpu: 21946.714405614945
Epoch: 8, Global Step: 83400, Data Step: 166800, Loss: 3.203125, Token per second per gpu: 22205.337855197395
Epoch: 8, Global Step: 83500, Data Step: 167000, Loss: 2.96875, Token per second per gpu: 22084.457009117385
Epoch: 8, Global Step: 83600, Data Step: 167200, Loss: 3.375, Token per second per gpu: 22415.401470035664
Epoch: 8, Global Step: 83700, Data Step: 167400, Loss: 3.421875, Token per second per gpu: 22592.439096571874
Epoch: 8, Global Step: 83800, Data Step: 167600, Loss: 3.09375, Token per second per gpu: 22636.894984737424
Epoch: 8, Global Step: 83900, Data Step: 167800, Loss: 2.84375, Token per second per gpu: 22341.567647437547
Epoch: 8, Global Step: 84000, Data Step: 168000, Loss: 3.296875, Token per second per gpu: 22409.421295532768
Epoch: 8, Global Step: 84100, Data Step: 168200, Loss: 2.921875, Token per second per gpu: 22447.782600128463
Epoch: 8, Global Step: 84200, Data Step: 168400, Loss: 3.28125, Token per second per gpu: 22193.70347701563
Epoch: 8, Global Step: 84300, Data Step: 168600, Loss: 3.140625, Token per second per gpu: 22147.953905799703
Epoch: 8, Global Step: 84400, Data Step: 168800, Loss: 3.375, Token per second per gpu: 22378.764094519836
Epoch: 8, Global Step: 84500, Data Step: 169000, Loss: 3.09375, Token per second per gpu: 22283.923841085045
Epoch: 8, Global Step: 84600, Data Step: 169200, Loss: 3.25, Token per second per gpu: 22215.586478983183
Epoch: 8, Global Step: 84700, Data Step: 169400, Loss: 3.390625, Token per second per gpu: 22194.728598235903
Epoch: 8, Global Step: 84800, Data Step: 169600, Loss: 3.25, Token per second per gpu: 22320.388797089006
Epoch: 8, Global Step: 84900, Data Step: 169800, Loss: 3.1875, Token per second per gpu: 22485.82118655788
Epoch: 8, Global Step: 85000, Data Step: 170000, Loss: 2.953125, Token per second per gpu: 22228.162869038824
Epoch: 8, Global Step: 85100, Data Step: 170200, Loss: 3.078125, Token per second per gpu: 22294.51396453028
Epoch: 8, Global Step: 85200, Data Step: 170400, Loss: 3.25, Token per second per gpu: 22421.70204763287
Epoch: 8, Global Step: 85300, Data Step: 170600, Loss: 3.15625, Token per second per gpu: 22514.31575411741
Epoch: 8, Global Step: 85400, Data Step: 170800, Loss: 3.15625, Token per second per gpu: 22321.30385363255
Epoch: 8, Global Step: 85500, Data Step: 171000, Loss: 3.09375, Token per second per gpu: 22292.070028852515
Epoch: 8, Global Step: 85600, Data Step: 171200, Loss: 3.328125, Token per second per gpu: 22158.576932502325
Epoch: 8, Global Step: 85700, Data Step: 171400, Loss: 2.84375, Token per second per gpu: 22563.548767336153
Epoch: 8, Global Step: 85800, Data Step: 171600, Loss: 3.359375, Token per second per gpu: 22352.89751632544
Epoch: 8, Global Step: 85900, Data Step: 171800, Loss: 2.953125, Token per second per gpu: 22374.136641275476
Epoch: 8, Global Step: 86000, Data Step: 172000, Loss: 3.34375, Token per second per gpu: 22389.622480642458
Epoch: 8, Global Step: 86100, Data Step: 172200, Loss: 3.25, Token per second per gpu: 21960.2370489174
Epoch: 8, Global Step: 86200, Data Step: 172400, Loss: 3.25, Token per second per gpu: 22393.633196816456
Epoch: 8, Global Step: 86300, Data Step: 172600, Loss: 3.1875, Token per second per gpu: 22331.921286858826
Epoch: 8, Global Step: 86400, Data Step: 172800, Loss: 3.171875, Token per second per gpu: 22409.390448585014
Epoch: 8, Global Step: 86500, Data Step: 173000, Loss: 3.3125, Token per second per gpu: 22293.216370837836
Epoch: 8, Global Step: 86600, Data Step: 173200, Loss: 2.96875, Token per second per gpu: 22167.366486761515
Epoch: 8, Global Step: 86700, Data Step: 173400, Loss: 3.53125, Token per second per gpu: 22489.006891782137
Epoch: 8, Global Step: 86800, Data Step: 173600, Loss: 3.578125, Token per second per gpu: 22210.15292886305
Epoch: 8, Global Step: 86900, Data Step: 173800, Loss: 2.75, Token per second per gpu: 22263.035525325624
Epoch: 8, Global Step: 87000, Data Step: 174000, Loss: 3.296875, Token per second per gpu: 22272.137835813963
Epoch: 8, Global Step: 87100, Data Step: 174200, Loss: 3.4375, Token per second per gpu: 22620.248746501322
Epoch: 8, Global Step: 87200, Data Step: 174400, Loss: 3.171875, Token per second per gpu: 22147.562164439907
Epoch: 8, Global Step: 87300, Data Step: 174600, Loss: 3.375, Token per second per gpu: 22081.761486875035
Epoch: 8, Global Step: 87400, Data Step: 174800, Loss: 3.203125, Token per second per gpu: 22196.399195357262
Epoch: 8, Global Step: 87500, Data Step: 175000, Loss: 3.328125, Token per second per gpu: 22201.012536809612
Epoch: 8, Global Step: 87600, Data Step: 175200, Loss: 3.171875, Token per second per gpu: 22239.20282307413
Epoch: 8, Global Step: 87700, Data Step: 175400, Loss: 3.546875, Token per second per gpu: 22320.619183326497
Epoch: 8, Global Step: 87800, Data Step: 175600, Loss: 3.109375, Token per second per gpu: 22140.090180413285
Epoch: 8, Global Step: 87900, Data Step: 175800, Loss: 3.609375, Token per second per gpu: 22148.635577736022
Epoch: 8, Global Step: 88000, Data Step: 176000, Loss: 3.15625, Token per second per gpu: 22288.682829635873
Epoch: 8, Global Step: 88100, Data Step: 176200, Loss: 3.21875, Token per second per gpu: 22415.091259159963
Epoch: 8, Global Step: 88200, Data Step: 176400, Loss: 3.15625, Token per second per gpu: 22597.36980368639
Epoch: 8, Global Step: 88300, Data Step: 176600, Loss: 3.171875, Token per second per gpu: 22326.827090389423
Epoch: 8, Global Step: 88400, Data Step: 176800, Loss: 3.21875, Token per second per gpu: 22375.032030430735
Epoch: 8, Global Step: 88500, Data Step: 177000, Loss: 3.140625, Token per second per gpu: 22365.123789773002
Epoch: 8, Global Step: 88600, Data Step: 177200, Loss: 3.453125, Token per second per gpu: 20911.023646210444
Epoch: 8, Global Step: 88700, Data Step: 177400, Loss: 3.03125, Token per second per gpu: 22241.086388247917
Epoch: 8, Global Step: 88800, Data Step: 177600, Loss: 3.21875, Token per second per gpu: 22482.167940349253
Epoch: 8, Global Step: 88900, Data Step: 177800, Loss: 3.296875, Token per second per gpu: 22311.43122010789
Epoch: 8, Global Step: 89000, Data Step: 178000, Loss: 2.78125, Token per second per gpu: 22330.659130107702
Epoch: 8, Global Step: 89100, Data Step: 178200, Loss: 3.234375, Token per second per gpu: 22416.641276217666
Epoch: 8, Global Step: 89200, Data Step: 178400, Loss: 2.671875, Token per second per gpu: 22505.20590509502
Epoch: 8, Global Step: 89300, Data Step: 178600, Loss: 3.21875, Token per second per gpu: 22125.340367082677
Epoch: 8, Global Step: 89400, Data Step: 178800, Loss: 2.90625, Token per second per gpu: 21700.453451906473
Epoch: 8, Global Step: 89500, Data Step: 179000, Loss: 3.265625, Token per second per gpu: 22431.782919328198
Epoch: 8, Global Step: 89600, Data Step: 179200, Loss: 3.4375, Token per second per gpu: 21880.231622179013
Epoch: 8, Global Step: 89700, Data Step: 179400, Loss: 3.390625, Token per second per gpu: 22533.625651004346
Epoch: 8, Global Step: 89800, Data Step: 179600, Loss: 3.328125, Token per second per gpu: 22279.64239821652
Epoch: 8, Global Step: 89900, Data Step: 179800, Loss: 3.5, Token per second per gpu: 22051.307161742974
Epoch: 8, Global Step: 90000, Data Step: 180000, Loss: 3.171875, Token per second per gpu: 22603.542717983346
I0327 23:42:33.697261 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0327 23:42:33.698830 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-27 23:42:33,705] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-27 23:42:33,735] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-27 23:42:33,737] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-27 23:42:34,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-27 23:42:34,861] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-27 23:42:34,861] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-27 23:42:34,861] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-27 23:42:34,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-27 23:42:34,867] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-27 23:42:34,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-27 23:42:35,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-27 23:42:35,482] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-27 23:42:35,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 23:42:36,242] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-27 23:42:36,242] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-27 23:42:36,242] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 23:42:36,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-27 23:42:36,536] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-27 23:42:36,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 23:42:36,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-27 23:42:36,559] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-27 23:42:36,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 23:42:36,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-27 23:42:36,614] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-27 23:42:36,614] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-27 23:42:36,635] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-27 23:42:36,636] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-27 23:42:36,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0327 23:42:36.636708 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0327 23:42:36.637922 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0327 23:42:36.638280 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0327 23:42:36.640019 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 8, Global Step: 90100, Data Step: 180200, Loss: 3.1875, Token per second per gpu: 21803.15552799498
Epoch: 8, Global Step: 90200, Data Step: 180400, Loss: 3.171875, Token per second per gpu: 22439.64649418308
Epoch: 8, Global Step: 90300, Data Step: 180600, Loss: 3.09375, Token per second per gpu: 22515.298693172965
Epoch: 8, Global Step: 90400, Data Step: 180800, Loss: 2.671875, Token per second per gpu: 22407.766406053266
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0327 23:52:32.482378 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 9, Global Step: 90500, Data Step: 181000, Loss: 3.1875, Token per second per gpu: 21308.753892871897
Epoch: 9, Global Step: 90600, Data Step: 181200, Loss: 3.359375, Token per second per gpu: 22086.941328429228
Epoch: 9, Global Step: 90700, Data Step: 181400, Loss: 3.46875, Token per second per gpu: 21949.13735975165
Epoch: 9, Global Step: 90800, Data Step: 181600, Loss: 3.109375, Token per second per gpu: 22196.99468905338
Epoch: 9, Global Step: 90900, Data Step: 181800, Loss: 3.0625, Token per second per gpu: 22507.316133492448
Epoch: 9, Global Step: 91000, Data Step: 182000, Loss: 2.890625, Token per second per gpu: 22081.94862494438
Epoch: 9, Global Step: 91100, Data Step: 182200, Loss: 3.546875, Token per second per gpu: 22132.08934025466
Epoch: 9, Global Step: 91200, Data Step: 182400, Loss: 3.140625, Token per second per gpu: 22212.146265702428
Epoch: 9, Global Step: 91300, Data Step: 182600, Loss: 3.203125, Token per second per gpu: 22118.617784144346
Epoch: 9, Global Step: 91400, Data Step: 182800, Loss: 3.015625, Token per second per gpu: 22311.32378619153
Epoch: 9, Global Step: 91500, Data Step: 183000, Loss: 3.359375, Token per second per gpu: 22152.82947759473
Epoch: 9, Global Step: 91600, Data Step: 183200, Loss: 3.328125, Token per second per gpu: 22318.61734107768
Epoch: 9, Global Step: 91700, Data Step: 183400, Loss: 3.265625, Token per second per gpu: 21985.31829524094
Epoch: 9, Global Step: 91800, Data Step: 183600, Loss: 3.296875, Token per second per gpu: 22396.88921414579
Epoch: 9, Global Step: 91900, Data Step: 183800, Loss: 3.140625, Token per second per gpu: 22183.602297337322
Epoch: 9, Global Step: 92000, Data Step: 184000, Loss: 3.25, Token per second per gpu: 22027.93779096965
Epoch: 9, Global Step: 92100, Data Step: 184200, Loss: 3.4375, Token per second per gpu: 22629.338714730304
Epoch: 9, Global Step: 92200, Data Step: 184400, Loss: 3.265625, Token per second per gpu: 22090.086385023227
Epoch: 9, Global Step: 92300, Data Step: 184600, Loss: 3.3125, Token per second per gpu: 22216.4364532124
Epoch: 9, Global Step: 92400, Data Step: 184800, Loss: 2.953125, Token per second per gpu: 22390.548576632526
Epoch: 9, Global Step: 92500, Data Step: 185000, Loss: 3.328125, Token per second per gpu: 22439.2937197174
Epoch: 9, Global Step: 92600, Data Step: 185200, Loss: 3.140625, Token per second per gpu: 22165.899476995706
Epoch: 9, Global Step: 92700, Data Step: 185400, Loss: 3.359375, Token per second per gpu: 22381.68937445375
Epoch: 9, Global Step: 92800, Data Step: 185600, Loss: 3.28125, Token per second per gpu: 22422.390436123376
Epoch: 9, Global Step: 92900, Data Step: 185800, Loss: 3.25, Token per second per gpu: 22102.34407159713
Epoch: 9, Global Step: 93000, Data Step: 186000, Loss: 3.21875, Token per second per gpu: 22196.30110511146
Epoch: 9, Global Step: 93100, Data Step: 186200, Loss: 3.375, Token per second per gpu: 22267.34904858644
Epoch: 9, Global Step: 93200, Data Step: 186400, Loss: 3.265625, Token per second per gpu: 22282.250360182596
Epoch: 9, Global Step: 93300, Data Step: 186600, Loss: 3.25, Token per second per gpu: 22265.955169389686
Epoch: 9, Global Step: 93400, Data Step: 186800, Loss: 3.0625, Token per second per gpu: 22312.513529261396
Epoch: 9, Global Step: 93500, Data Step: 187000, Loss: 3.140625, Token per second per gpu: 22473.269132619265
Epoch: 9, Global Step: 93600, Data Step: 187200, Loss: 3.328125, Token per second per gpu: 22302.880582714024
Epoch: 9, Global Step: 93700, Data Step: 187400, Loss: 3.25, Token per second per gpu: 22213.370878276528
Epoch: 9, Global Step: 93800, Data Step: 187600, Loss: 3.09375, Token per second per gpu: 22310.92455483519
Epoch: 9, Global Step: 93900, Data Step: 187800, Loss: 3.296875, Token per second per gpu: 22190.39709823655
Epoch: 9, Global Step: 94000, Data Step: 188000, Loss: 3.625, Token per second per gpu: 22892.445793307797
Epoch: 9, Global Step: 94100, Data Step: 188200, Loss: 3.171875, Token per second per gpu: 25120.80834879923
Epoch: 9, Global Step: 94200, Data Step: 188400, Loss: 3.296875, Token per second per gpu: 25108.46924679266
Epoch: 9, Global Step: 94300, Data Step: 188600, Loss: 2.734375, Token per second per gpu: 25101.22124834419
Epoch: 9, Global Step: 94400, Data Step: 188800, Loss: 3.21875, Token per second per gpu: 25065.5110935263
Epoch: 9, Global Step: 94500, Data Step: 189000, Loss: 3.015625, Token per second per gpu: 25032.502300824064
Epoch: 9, Global Step: 94600, Data Step: 189200, Loss: 3.0625, Token per second per gpu: 25027.243615910797
Epoch: 9, Global Step: 94700, Data Step: 189400, Loss: 3.109375, Token per second per gpu: 25000.37185159082
Epoch: 9, Global Step: 94800, Data Step: 189600, Loss: 3.0, Token per second per gpu: 24992.588445933827
Epoch: 9, Global Step: 94900, Data Step: 189800, Loss: 2.796875, Token per second per gpu: 24999.67620403067
Epoch: 9, Global Step: 95000, Data Step: 190000, Loss: 3.046875, Token per second per gpu: 25000.18335806368
Epoch: 9, Global Step: 95100, Data Step: 190200, Loss: 3.375, Token per second per gpu: 25007.52138810899
Epoch: 9, Global Step: 95200, Data Step: 190400, Loss: 3.203125, Token per second per gpu: 25003.561512298824
Epoch: 9, Global Step: 95300, Data Step: 190600, Loss: 3.265625, Token per second per gpu: 25005.468773564706
Epoch: 9, Global Step: 95400, Data Step: 190800, Loss: 3.109375, Token per second per gpu: 25002.54037834875
Epoch: 9, Global Step: 95500, Data Step: 191000, Loss: 3.171875, Token per second per gpu: 25020.788290091088
Epoch: 9, Global Step: 95600, Data Step: 191200, Loss: 3.34375, Token per second per gpu: 25014.281904124473
Epoch: 9, Global Step: 95700, Data Step: 191400, Loss: 3.25, Token per second per gpu: 25006.791020217748
Epoch: 9, Global Step: 95800, Data Step: 191600, Loss: 2.96875, Token per second per gpu: 25010.754404769086
Epoch: 9, Global Step: 95900, Data Step: 191800, Loss: 3.109375, Token per second per gpu: 25000.53483895238
Epoch: 9, Global Step: 96000, Data Step: 192000, Loss: 3.0, Token per second per gpu: 25005.821334843182
Epoch: 9, Global Step: 96100, Data Step: 192200, Loss: 3.390625, Token per second per gpu: 25001.36931579415
Epoch: 9, Global Step: 96200, Data Step: 192400, Loss: 2.6875, Token per second per gpu: 24988.98497066197
Epoch: 9, Global Step: 96300, Data Step: 192600, Loss: 3.171875, Token per second per gpu: 25001.69443895306
Epoch: 9, Global Step: 96400, Data Step: 192800, Loss: 3.21875, Token per second per gpu: 24999.225153286134
Epoch: 9, Global Step: 96500, Data Step: 193000, Loss: 3.265625, Token per second per gpu: 24994.195007291204
Epoch: 9, Global Step: 96600, Data Step: 193200, Loss: 3.265625, Token per second per gpu: 24990.918910404635
Epoch: 9, Global Step: 96700, Data Step: 193400, Loss: 3.359375, Token per second per gpu: 24995.01044192213
Epoch: 9, Global Step: 96800, Data Step: 193600, Loss: 3.265625, Token per second per gpu: 24991.486410604233
Epoch: 9, Global Step: 96900, Data Step: 193800, Loss: 3.296875, Token per second per gpu: 24978.112754124468
Epoch: 9, Global Step: 97000, Data Step: 194000, Loss: 3.359375, Token per second per gpu: 24969.36137134555
Epoch: 9, Global Step: 97100, Data Step: 194200, Loss: 3.328125, Token per second per gpu: 24983.951813327792
Epoch: 9, Global Step: 97200, Data Step: 194400, Loss: 3.0, Token per second per gpu: 24989.83687435008
Epoch: 9, Global Step: 97300, Data Step: 194600, Loss: 2.734375, Token per second per gpu: 24983.79906669356
Epoch: 9, Global Step: 97400, Data Step: 194800, Loss: 3.28125, Token per second per gpu: 24985.40594525717
Epoch: 9, Global Step: 97500, Data Step: 195000, Loss: 3.0625, Token per second per gpu: 24980.302423693876
Epoch: 9, Global Step: 97600, Data Step: 195200, Loss: 3.140625, Token per second per gpu: 24992.71327321688
Epoch: 9, Global Step: 97700, Data Step: 195400, Loss: 3.1875, Token per second per gpu: 25066.802401583005
Epoch: 9, Global Step: 97800, Data Step: 195600, Loss: 3.296875, Token per second per gpu: 25075.212525458162
Epoch: 9, Global Step: 97900, Data Step: 195800, Loss: 3.453125, Token per second per gpu: 25051.186119021382
Epoch: 9, Global Step: 98000, Data Step: 196000, Loss: 3.109375, Token per second per gpu: 25038.015208379962
Epoch: 9, Global Step: 98100, Data Step: 196200, Loss: 3.25, Token per second per gpu: 25029.868066276562
Epoch: 9, Global Step: 98200, Data Step: 196400, Loss: 2.859375, Token per second per gpu: 25015.087512405327
Epoch: 9, Global Step: 98300, Data Step: 196600, Loss: 3.125, Token per second per gpu: 24996.92488211041
Epoch: 9, Global Step: 98400, Data Step: 196800, Loss: 3.046875, Token per second per gpu: 24382.078592348018
Epoch: 9, Global Step: 98500, Data Step: 197000, Loss: 2.90625, Token per second per gpu: 25002.652056769122
Epoch: 9, Global Step: 98600, Data Step: 197200, Loss: 3.0625, Token per second per gpu: 24982.188984816123
Epoch: 9, Global Step: 98700, Data Step: 197400, Loss: 2.984375, Token per second per gpu: 24986.528841709038
Epoch: 9, Global Step: 98800, Data Step: 197600, Loss: 3.25, Token per second per gpu: 24982.38955556876
Epoch: 9, Global Step: 98900, Data Step: 197800, Loss: 2.890625, Token per second per gpu: 24985.211320824215
Epoch: 9, Global Step: 99000, Data Step: 198000, Loss: 2.953125, Token per second per gpu: 24992.10683321501
Epoch: 9, Global Step: 99100, Data Step: 198200, Loss: 2.40625, Token per second per gpu: 24986.07785179156
Epoch: 9, Global Step: 99200, Data Step: 198400, Loss: 3.171875, Token per second per gpu: 24988.458782820442
Epoch: 9, Global Step: 99300, Data Step: 198600, Loss: 3.21875, Token per second per gpu: 24374.950054145782
Epoch: 9, Global Step: 99400, Data Step: 198800, Loss: 3.28125, Token per second per gpu: 24997.444494797634
Epoch: 9, Global Step: 99500, Data Step: 199000, Loss: 2.609375, Token per second per gpu: 24968.29121334386
Epoch: 9, Global Step: 99600, Data Step: 199200, Loss: 3.109375, Token per second per gpu: 24338.96579703283
Epoch: 9, Global Step: 99700, Data Step: 199400, Loss: 3.421875, Token per second per gpu: 24326.626882693996
Epoch: 9, Global Step: 99800, Data Step: 199600, Loss: 3.109375, Token per second per gpu: 24937.602491326787
Epoch: 9, Global Step: 99900, Data Step: 199800, Loss: 3.125, Token per second per gpu: 24917.297113613604
Epoch: 9, Global Step: 100000, Data Step: 200000, Loss: 3.484375, Token per second per gpu: 25040.305036782443
I0328 03:04:16.081873 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0328 03:04:16.082429 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-28 03:04:16,082] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-28 03:04:16,087] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-28 03:04:16,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-28 03:04:16,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-28 03:04:16,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-28 03:04:16,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-28 03:04:16,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-28 03:04:16,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-28 03:04:16,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-28 03:04:16,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-28 03:04:18,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-28 03:04:18,742] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-28 03:04:18,742] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 03:04:18,899] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-28 03:04:18,900] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-28 03:04:18,900] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 03:04:18,969] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-28 03:04:18,969] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-28 03:04:18,969] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 03:04:18,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-28 03:04:18,993] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-28 03:04:18,993] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 03:04:19,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-28 03:04:19,012] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-28 03:04:19,012] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 03:04:19,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-28 03:04:19,015] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-28 03:04:19,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0328 03:04:19.016373 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0328 03:04:19.017176 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0328 03:04:19.017452 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0328 03:04:19.018935 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 9, Global Step: 100100, Data Step: 200200, Loss: 3.4375, Token per second per gpu: 24533.08048574819
Epoch: 9, Global Step: 100200, Data Step: 200400, Loss: 3.15625, Token per second per gpu: 25056.10909250913
Epoch: 9, Global Step: 100300, Data Step: 200600, Loss: 3.578125, Token per second per gpu: 25020.93635849483
Epoch: 9, Global Step: 100400, Data Step: 200800, Loss: 3.21875, Token per second per gpu: 25016.92550052048
Epoch: 9, Global Step: 100500, Data Step: 201000, Loss: 3.171875, Token per second per gpu: 24979.75723027767
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0328 03:14:11.086808 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 10, Global Step: 100600, Data Step: 201200, Loss: 2.75, Token per second per gpu: 24344.874264993963
Epoch: 10, Global Step: 100700, Data Step: 201400, Loss: 3.09375, Token per second per gpu: 24948.10907664471
Epoch: 10, Global Step: 100800, Data Step: 201600, Loss: 2.890625, Token per second per gpu: 24941.98305642204
Epoch: 10, Global Step: 100900, Data Step: 201800, Loss: 3.421875, Token per second per gpu: 25021.70341923187
Epoch: 10, Global Step: 101000, Data Step: 202000, Loss: 3.328125, Token per second per gpu: 25059.766993869805
Epoch: 10, Global Step: 101100, Data Step: 202200, Loss: 3.171875, Token per second per gpu: 25046.630269556445
Epoch: 10, Global Step: 101200, Data Step: 202400, Loss: 3.140625, Token per second per gpu: 25042.48215828214
Epoch: 10, Global Step: 101300, Data Step: 202600, Loss: 3.09375, Token per second per gpu: 25025.43454465644
Epoch: 10, Global Step: 101400, Data Step: 202800, Loss: 3.015625, Token per second per gpu: 25031.8379583482
Epoch: 10, Global Step: 101500, Data Step: 203000, Loss: 3.234375, Token per second per gpu: 25021.19622096611
Epoch: 10, Global Step: 101600, Data Step: 203200, Loss: 3.40625, Token per second per gpu: 25036.458117262293
Epoch: 10, Global Step: 101700, Data Step: 203400, Loss: 2.96875, Token per second per gpu: 25043.211808903314
Epoch: 10, Global Step: 101800, Data Step: 203600, Loss: 3.03125, Token per second per gpu: 25042.785352181745
Epoch: 10, Global Step: 101900, Data Step: 203800, Loss: 2.9375, Token per second per gpu: 25021.07287113444
Epoch: 10, Global Step: 102000, Data Step: 204000, Loss: 3.171875, Token per second per gpu: 25031.723632958125
Epoch: 10, Global Step: 102100, Data Step: 204200, Loss: 3.109375, Token per second per gpu: 25028.333820272972
Epoch: 10, Global Step: 102200, Data Step: 204400, Loss: 3.234375, Token per second per gpu: 25030.5575113866
Epoch: 10, Global Step: 102300, Data Step: 204600, Loss: 3.046875, Token per second per gpu: 25025.658570961732
Epoch: 10, Global Step: 102400, Data Step: 204800, Loss: 3.34375, Token per second per gpu: 25015.653054309114
Epoch: 10, Global Step: 102500, Data Step: 205000, Loss: 3.28125, Token per second per gpu: 25020.95719290159
Epoch: 10, Global Step: 102600, Data Step: 205200, Loss: 3.015625, Token per second per gpu: 25018.862161212448
Epoch: 10, Global Step: 102700, Data Step: 205400, Loss: 3.109375, Token per second per gpu: 25025.746762005358
Epoch: 10, Global Step: 102800, Data Step: 205600, Loss: 3.21875, Token per second per gpu: 25028.197695104274
Epoch: 10, Global Step: 102900, Data Step: 205800, Loss: 3.1875, Token per second per gpu: 25030.25217582286
Epoch: 10, Global Step: 103000, Data Step: 206000, Loss: 3.078125, Token per second per gpu: 25035.32138921298
Epoch: 10, Global Step: 103100, Data Step: 206200, Loss: 3.375, Token per second per gpu: 25016.091435517283
Epoch: 10, Global Step: 103200, Data Step: 206400, Loss: 3.1875, Token per second per gpu: 25013.321841397796
Epoch: 10, Global Step: 103300, Data Step: 206600, Loss: 3.109375, Token per second per gpu: 25008.07110764733
Epoch: 10, Global Step: 103400, Data Step: 206800, Loss: 3.09375, Token per second per gpu: 24999.312795962687
Epoch: 10, Global Step: 103500, Data Step: 207000, Loss: 3.453125, Token per second per gpu: 24993.181309066007
Epoch: 10, Global Step: 103600, Data Step: 207200, Loss: 3.375, Token per second per gpu: 25012.890083232724
Epoch: 10, Global Step: 103700, Data Step: 207400, Loss: 3.21875, Token per second per gpu: 25016.885762141046
Epoch: 10, Global Step: 103800, Data Step: 207600, Loss: 3.328125, Token per second per gpu: 24994.51684083583
Epoch: 10, Global Step: 103900, Data Step: 207800, Loss: 3.234375, Token per second per gpu: 25005.52177876575
Epoch: 10, Global Step: 104000, Data Step: 208000, Loss: 3.078125, Token per second per gpu: 25007.523717816825
Epoch: 10, Global Step: 104100, Data Step: 208200, Loss: 3.0, Token per second per gpu: 25000.213988715193
Epoch: 10, Global Step: 104200, Data Step: 208400, Loss: 3.21875, Token per second per gpu: 24992.67986861049
Epoch: 10, Global Step: 104300, Data Step: 208600, Loss: 2.984375, Token per second per gpu: 25004.232998221647
Epoch: 10, Global Step: 104400, Data Step: 208800, Loss: 3.359375, Token per second per gpu: 24997.74499545771
Epoch: 10, Global Step: 104500, Data Step: 209000, Loss: 3.0625, Token per second per gpu: 25001.920938310792
Epoch: 10, Global Step: 104600, Data Step: 209200, Loss: 3.171875, Token per second per gpu: 25010.19695524984
Epoch: 10, Global Step: 104700, Data Step: 209400, Loss: 3.421875, Token per second per gpu: 24989.06654494322
Epoch: 10, Global Step: 104800, Data Step: 209600, Loss: 3.296875, Token per second per gpu: 25005.493205621642
Epoch: 10, Global Step: 104900, Data Step: 209800, Loss: 3.0625, Token per second per gpu: 25010.74518710169
Epoch: 10, Global Step: 105000, Data Step: 210000, Loss: 3.09375, Token per second per gpu: 24994.80930643529
Epoch: 10, Global Step: 105100, Data Step: 210200, Loss: 3.34375, Token per second per gpu: 24999.787287733834
Epoch: 10, Global Step: 105200, Data Step: 210400, Loss: 3.25, Token per second per gpu: 25012.345900646207
Epoch: 10, Global Step: 105300, Data Step: 210600, Loss: 3.203125, Token per second per gpu: 25000.42017841543
Epoch: 10, Global Step: 105400, Data Step: 210800, Loss: 3.09375, Token per second per gpu: 25002.59373339176
Epoch: 10, Global Step: 105500, Data Step: 211000, Loss: 3.1875, Token per second per gpu: 24993.13419959306
Epoch: 10, Global Step: 105600, Data Step: 211200, Loss: 3.0, Token per second per gpu: 25001.687660077347
Epoch: 10, Global Step: 105700, Data Step: 211400, Loss: 3.015625, Token per second per gpu: 24991.52436206543
Epoch: 10, Global Step: 105800, Data Step: 211600, Loss: 3.390625, Token per second per gpu: 24998.852497692496
Epoch: 10, Global Step: 105900, Data Step: 211800, Loss: 3.28125, Token per second per gpu: 25006.37460329826
Epoch: 10, Global Step: 106000, Data Step: 212000, Loss: 3.28125, Token per second per gpu: 24998.294440461978
Epoch: 10, Global Step: 106100, Data Step: 212200, Loss: 3.109375, Token per second per gpu: 24988.67242926403
Epoch: 10, Global Step: 106200, Data Step: 212400, Loss: 3.578125, Token per second per gpu: 24985.37483408473
Epoch: 10, Global Step: 106300, Data Step: 212600, Loss: 3.203125, Token per second per gpu: 25001.893977582626
Epoch: 10, Global Step: 106400, Data Step: 212800, Loss: 3.28125, Token per second per gpu: 24999.407423839995
Epoch: 10, Global Step: 106500, Data Step: 213000, Loss: 2.921875, Token per second per gpu: 24998.53013861315
Epoch: 10, Global Step: 106600, Data Step: 213200, Loss: 3.203125, Token per second per gpu: 24991.04718511766
Epoch: 10, Global Step: 106700, Data Step: 213400, Loss: 3.484375, Token per second per gpu: 24996.60055542485
Epoch: 10, Global Step: 106800, Data Step: 213600, Loss: 2.984375, Token per second per gpu: 24990.872378186097
Epoch: 10, Global Step: 106900, Data Step: 213800, Loss: 3.40625, Token per second per gpu: 24974.094391389957
Epoch: 10, Global Step: 107000, Data Step: 214000, Loss: 2.921875, Token per second per gpu: 25005.914873177604
Epoch: 10, Global Step: 107100, Data Step: 214200, Loss: 3.375, Token per second per gpu: 24978.110739790744
Epoch: 10, Global Step: 107200, Data Step: 214400, Loss: 2.921875, Token per second per gpu: 24987.97851812503
Epoch: 10, Global Step: 107300, Data Step: 214600, Loss: 3.125, Token per second per gpu: 24984.58038848497
Epoch: 10, Global Step: 107400, Data Step: 214800, Loss: 3.25, Token per second per gpu: 24983.49326899714
Epoch: 10, Global Step: 107500, Data Step: 215000, Loss: 3.21875, Token per second per gpu: 24984.432388330548
Epoch: 10, Global Step: 107600, Data Step: 215200, Loss: 3.046875, Token per second per gpu: 24982.701009787434
Epoch: 10, Global Step: 107700, Data Step: 215400, Loss: 3.03125, Token per second per gpu: 24999.31507240403
Epoch: 10, Global Step: 107800, Data Step: 215600, Loss: 3.046875, Token per second per gpu: 24994.128759119983
Epoch: 10, Global Step: 107900, Data Step: 215800, Loss: 3.234375, Token per second per gpu: 24989.40835627788
Epoch: 10, Global Step: 108000, Data Step: 216000, Loss: 3.3125, Token per second per gpu: 25019.78305717434
Epoch: 10, Global Step: 108100, Data Step: 216200, Loss: 2.984375, Token per second per gpu: 25007.8244090845
Epoch: 10, Global Step: 108200, Data Step: 216400, Loss: 3.359375, Token per second per gpu: 25011.562064521804
Epoch: 10, Global Step: 108300, Data Step: 216600, Loss: 3.1875, Token per second per gpu: 24403.560879969507
Epoch: 10, Global Step: 108400, Data Step: 216800, Loss: 3.109375, Token per second per gpu: 25019.520477543527
Epoch: 10, Global Step: 108500, Data Step: 217000, Loss: 3.3125, Token per second per gpu: 25002.754679663358
Epoch: 10, Global Step: 108600, Data Step: 217200, Loss: 2.96875, Token per second per gpu: 24993.351235599897
Epoch: 10, Global Step: 108700, Data Step: 217400, Loss: 3.4375, Token per second per gpu: 24992.47721935974
Epoch: 10, Global Step: 108800, Data Step: 217600, Loss: 3.203125, Token per second per gpu: 24981.768737015973
Epoch: 10, Global Step: 108900, Data Step: 217800, Loss: 3.109375, Token per second per gpu: 24993.36018188741
Epoch: 10, Global Step: 109000, Data Step: 218000, Loss: 3.09375, Token per second per gpu: 24982.08725399873
Epoch: 10, Global Step: 109100, Data Step: 218200, Loss: 3.109375, Token per second per gpu: 24974.00196871605
Epoch: 10, Global Step: 109200, Data Step: 218400, Loss: 3.59375, Token per second per gpu: 24369.29540427834
Epoch: 10, Global Step: 109300, Data Step: 218600, Loss: 3.09375, Token per second per gpu: 25000.820097820597
Epoch: 10, Global Step: 109400, Data Step: 218800, Loss: 3.3125, Token per second per gpu: 24991.839301594395
Epoch: 10, Global Step: 109500, Data Step: 219000, Loss: 3.09375, Token per second per gpu: 24986.86872293404
Epoch: 10, Global Step: 109600, Data Step: 219200, Loss: 3.171875, Token per second per gpu: 24374.74352693896
Epoch: 10, Global Step: 109700, Data Step: 219400, Loss: 3.375, Token per second per gpu: 25001.02376204388
Epoch: 10, Global Step: 109800, Data Step: 219600, Loss: 3.359375, Token per second per gpu: 24989.05191529592
Epoch: 10, Global Step: 109900, Data Step: 219800, Loss: 3.0625, Token per second per gpu: 24984.053094263898
Epoch: 10, Global Step: 110000, Data Step: 220000, Loss: 3.328125, Token per second per gpu: 24983.761913785827
I0328 06:16:28.016629 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0328 06:16:28.017096 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-28 06:16:28,017] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-28 06:16:28,021] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-28 06:16:28,021] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-28 06:16:28,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-28 06:16:28,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-28 06:16:28,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-28 06:16:28,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-28 06:16:28,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-28 06:16:28,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-28 06:16:28,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-28 06:16:30,842] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-28 06:16:30,842] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-28 06:16:30,842] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 06:16:30,864] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-28 06:16:30,864] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-28 06:16:30,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 06:16:30,892] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-28 06:16:30,906] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-28 06:16:30,906] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 06:16:30,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-28 06:16:30,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-28 06:16:30,914] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-28 06:16:30,914] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-28 06:16:30,914] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 06:16:30,914] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 06:16:30,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-28 06:16:30,925] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-28 06:16:30,925] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0328 06:16:30.925444 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0328 06:16:30.926052 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0328 06:16:30.926268 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0328 06:16:30.927500 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 10, Global Step: 110100, Data Step: 220200, Loss: 2.984375, Token per second per gpu: 24390.892863355086
Epoch: 10, Global Step: 110200, Data Step: 220400, Loss: 3.015625, Token per second per gpu: 24985.98446180464
Epoch: 10, Global Step: 110300, Data Step: 220600, Loss: 3.296875, Token per second per gpu: 24970.07215972352
Epoch: 10, Global Step: 110400, Data Step: 220800, Loss: 3.1875, Token per second per gpu: 24986.632055878366
Epoch: 10, Global Step: 110500, Data Step: 221000, Loss: 3.03125, Token per second per gpu: 24983.67598196223
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0328 06:27:23.731895 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 11, Global Step: 110600, Data Step: 221200, Loss: 3.09375, Token per second per gpu: 24358.594047225317
Epoch: 11, Global Step: 110700, Data Step: 221400, Loss: 3.203125, Token per second per gpu: 25002.6135022434
Epoch: 11, Global Step: 110800, Data Step: 221600, Loss: 3.03125, Token per second per gpu: 24374.860192985296
Epoch: 11, Global Step: 110900, Data Step: 221800, Loss: 2.96875, Token per second per gpu: 25010.12383877269
Epoch: 11, Global Step: 111000, Data Step: 222000, Loss: 3.046875, Token per second per gpu: 25005.906487310545
Epoch: 11, Global Step: 111100, Data Step: 222200, Loss: 3.25, Token per second per gpu: 24977.880281498143
Epoch: 11, Global Step: 111200, Data Step: 222400, Loss: 3.171875, Token per second per gpu: 24994.35300102958
Epoch: 11, Global Step: 111300, Data Step: 222600, Loss: 3.234375, Token per second per gpu: 24984.70699615387
Epoch: 11, Global Step: 111400, Data Step: 222800, Loss: 3.265625, Token per second per gpu: 24983.496162620555
Epoch: 11, Global Step: 111500, Data Step: 223000, Loss: 3.484375, Token per second per gpu: 24992.569313444725
Epoch: 11, Global Step: 111600, Data Step: 223200, Loss: 3.359375, Token per second per gpu: 24997.204782288463
Epoch: 11, Global Step: 111700, Data Step: 223400, Loss: 3.296875, Token per second per gpu: 24990.188064371767
Epoch: 11, Global Step: 111800, Data Step: 223600, Loss: 2.84375, Token per second per gpu: 24998.275092303687
Epoch: 11, Global Step: 111900, Data Step: 223800, Loss: 3.21875, Token per second per gpu: 25009.619336375854
Epoch: 11, Global Step: 112000, Data Step: 224000, Loss: 2.984375, Token per second per gpu: 24998.540537163364
Epoch: 11, Global Step: 112100, Data Step: 224200, Loss: 3.015625, Token per second per gpu: 25007.914493619883
Epoch: 11, Global Step: 112200, Data Step: 224400, Loss: 3.34375, Token per second per gpu: 25011.755028142954
Epoch: 11, Global Step: 112300, Data Step: 224600, Loss: 3.34375, Token per second per gpu: 24990.142568861742
Epoch: 11, Global Step: 112400, Data Step: 224800, Loss: 3.21875, Token per second per gpu: 25001.017397486674
Epoch: 11, Global Step: 112500, Data Step: 225000, Loss: 3.28125, Token per second per gpu: 24993.77538975684
Epoch: 11, Global Step: 112600, Data Step: 225200, Loss: 3.25, Token per second per gpu: 24996.871654713454
Epoch: 11, Global Step: 112700, Data Step: 225400, Loss: 2.984375, Token per second per gpu: 24995.13937928146
Epoch: 11, Global Step: 112800, Data Step: 225600, Loss: 3.015625, Token per second per gpu: 24993.736810887964
Epoch: 11, Global Step: 112900, Data Step: 225800, Loss: 2.96875, Token per second per gpu: 24997.972975214398
Epoch: 11, Global Step: 113000, Data Step: 226000, Loss: 2.875, Token per second per gpu: 25002.65604160608
Epoch: 11, Global Step: 113100, Data Step: 226200, Loss: 3.28125, Token per second per gpu: 24993.64269135905
Epoch: 11, Global Step: 113200, Data Step: 226400, Loss: 3.015625, Token per second per gpu: 25007.182549670353
Epoch: 11, Global Step: 113300, Data Step: 226600, Loss: 3.078125, Token per second per gpu: 24999.56894995265
Epoch: 11, Global Step: 113400, Data Step: 226800, Loss: 3.171875, Token per second per gpu: 25001.49350635763
Epoch: 11, Global Step: 113500, Data Step: 227000, Loss: 3.4375, Token per second per gpu: 25006.14398552404
Epoch: 11, Global Step: 113600, Data Step: 227200, Loss: 2.515625, Token per second per gpu: 24999.676669679906
Epoch: 11, Global Step: 113700, Data Step: 227400, Loss: 3.109375, Token per second per gpu: 24998.353623249735
Epoch: 11, Global Step: 113800, Data Step: 227600, Loss: 3.34375, Token per second per gpu: 25000.36093410064
Epoch: 11, Global Step: 113900, Data Step: 227800, Loss: 3.390625, Token per second per gpu: 24994.962911799128
Epoch: 11, Global Step: 114000, Data Step: 228000, Loss: 3.359375, Token per second per gpu: 24991.37307395662
Epoch: 11, Global Step: 114100, Data Step: 228200, Loss: 3.125, Token per second per gpu: 24994.969687028686
Epoch: 11, Global Step: 114200, Data Step: 228400, Loss: 3.0, Token per second per gpu: 24988.26612679613
Epoch: 11, Global Step: 114300, Data Step: 228600, Loss: 3.109375, Token per second per gpu: 24986.374202947016
Epoch: 11, Global Step: 114400, Data Step: 228800, Loss: 3.265625, Token per second per gpu: 24984.766166300324
Epoch: 11, Global Step: 114500, Data Step: 229000, Loss: 3.125, Token per second per gpu: 24968.054588427305
Epoch: 11, Global Step: 114600, Data Step: 229200, Loss: 2.859375, Token per second per gpu: 24987.613227439866
Epoch: 11, Global Step: 114700, Data Step: 229400, Loss: 3.3125, Token per second per gpu: 24980.34860660585
Epoch: 11, Global Step: 114800, Data Step: 229600, Loss: 3.359375, Token per second per gpu: 24988.57297181524
Epoch: 11, Global Step: 114900, Data Step: 229800, Loss: 3.0, Token per second per gpu: 24991.143870273892
Epoch: 11, Global Step: 115000, Data Step: 230000, Loss: 2.921875, Token per second per gpu: 24995.250991595636
Epoch: 11, Global Step: 115100, Data Step: 230200, Loss: 3.125, Token per second per gpu: 24991.08668634244
Epoch: 11, Global Step: 115200, Data Step: 230400, Loss: 3.25, Token per second per gpu: 24989.65650103602
Epoch: 11, Global Step: 115300, Data Step: 230600, Loss: 3.171875, Token per second per gpu: 24983.936724605548
Epoch: 11, Global Step: 115400, Data Step: 230800, Loss: 3.109375, Token per second per gpu: 24968.274130784193
Epoch: 11, Global Step: 115500, Data Step: 231000, Loss: 3.078125, Token per second per gpu: 24982.790086708737
Epoch: 11, Global Step: 115600, Data Step: 231200, Loss: 3.078125, Token per second per gpu: 24993.011488265158
Epoch: 11, Global Step: 115700, Data Step: 231400, Loss: 3.15625, Token per second per gpu: 24987.867694408502
Epoch: 11, Global Step: 115800, Data Step: 231600, Loss: 2.953125, Token per second per gpu: 24994.459072578178
Epoch: 11, Global Step: 115900, Data Step: 231800, Loss: 3.1875, Token per second per gpu: 24982.775722753875
Epoch: 11, Global Step: 116000, Data Step: 232000, Loss: 3.09375, Token per second per gpu: 24998.77546392912
Epoch: 11, Global Step: 116100, Data Step: 232200, Loss: 3.09375, Token per second per gpu: 24995.18753071829
Epoch: 11, Global Step: 116200, Data Step: 232400, Loss: 3.09375, Token per second per gpu: 24999.566311296625
Epoch: 11, Global Step: 116300, Data Step: 232600, Loss: 3.1875, Token per second per gpu: 24995.446910338884
Epoch: 11, Global Step: 116400, Data Step: 232800, Loss: 3.40625, Token per second per gpu: 24981.47719647392
Epoch: 11, Global Step: 116500, Data Step: 233000, Loss: 3.3125, Token per second per gpu: 24986.530909085133
Epoch: 11, Global Step: 116600, Data Step: 233200, Loss: 3.25, Token per second per gpu: 24977.708705943318
Epoch: 11, Global Step: 116700, Data Step: 233400, Loss: 3.171875, Token per second per gpu: 24986.73372034295
Epoch: 11, Global Step: 116800, Data Step: 233600, Loss: 3.21875, Token per second per gpu: 24987.773670766812
Epoch: 11, Global Step: 116900, Data Step: 233800, Loss: 2.96875, Token per second per gpu: 24993.374454590565
Epoch: 11, Global Step: 117000, Data Step: 234000, Loss: 3.1875, Token per second per gpu: 24991.460454766628
Epoch: 11, Global Step: 117100, Data Step: 234200, Loss: 2.703125, Token per second per gpu: 24983.779430928833
Epoch: 11, Global Step: 117200, Data Step: 234400, Loss: 3.25, Token per second per gpu: 24984.133964580546
Epoch: 11, Global Step: 117300, Data Step: 234600, Loss: 3.0625, Token per second per gpu: 24979.94464105884
Epoch: 11, Global Step: 117400, Data Step: 234800, Loss: 3.328125, Token per second per gpu: 24984.288162213357
Epoch: 11, Global Step: 117500, Data Step: 235000, Loss: 3.328125, Token per second per gpu: 24968.727110372496
Epoch: 11, Global Step: 117600, Data Step: 235200, Loss: 3.125, Token per second per gpu: 24967.057458499563
Epoch: 11, Global Step: 117700, Data Step: 235400, Loss: 3.3125, Token per second per gpu: 24984.042191021643
Epoch: 11, Global Step: 117800, Data Step: 235600, Loss: 3.140625, Token per second per gpu: 24982.099292188344
Epoch: 11, Global Step: 117900, Data Step: 235800, Loss: 2.71875, Token per second per gpu: 24991.45983430902
Epoch: 11, Global Step: 118000, Data Step: 236000, Loss: 3.078125, Token per second per gpu: 24990.821606563106
Epoch: 11, Global Step: 118100, Data Step: 236200, Loss: 3.0625, Token per second per gpu: 24390.61524632976
Epoch: 11, Global Step: 118200, Data Step: 236400, Loss: 3.34375, Token per second per gpu: 24992.928130056298
Epoch: 11, Global Step: 118300, Data Step: 236600, Loss: 3.03125, Token per second per gpu: 24976.577771091863
Epoch: 11, Global Step: 118400, Data Step: 236800, Loss: 2.921875, Token per second per gpu: 24962.37368579809
Epoch: 11, Global Step: 118500, Data Step: 237000, Loss: 3.28125, Token per second per gpu: 24971.795438523917
Epoch: 11, Global Step: 118600, Data Step: 237200, Loss: 3.234375, Token per second per gpu: 24976.901165241023
Epoch: 11, Global Step: 118700, Data Step: 237400, Loss: 3.15625, Token per second per gpu: 24985.921358063722
Epoch: 11, Global Step: 118800, Data Step: 237600, Loss: 3.125, Token per second per gpu: 24981.495847011935
Epoch: 11, Global Step: 118900, Data Step: 237800, Loss: 3.234375, Token per second per gpu: 24986.638051319514
Epoch: 11, Global Step: 119000, Data Step: 238000, Loss: 3.234375, Token per second per gpu: 24982.507254176522
Epoch: 11, Global Step: 119100, Data Step: 238200, Loss: 3.25, Token per second per gpu: 24991.940336249223
Epoch: 11, Global Step: 119200, Data Step: 238400, Loss: 3.28125, Token per second per gpu: 24393.94563919196
Epoch: 11, Global Step: 119300, Data Step: 238600, Loss: 2.921875, Token per second per gpu: 24997.899309415083
Epoch: 11, Global Step: 119400, Data Step: 238800, Loss: 3.125, Token per second per gpu: 24970.87270319788
Epoch: 11, Global Step: 119500, Data Step: 239000, Loss: 3.15625, Token per second per gpu: 24360.050668504722
Epoch: 11, Global Step: 119600, Data Step: 239200, Loss: 3.265625, Token per second per gpu: 25019.058916643487
Epoch: 11, Global Step: 119700, Data Step: 239400, Loss: 3.1875, Token per second per gpu: 25009.80388154056
Epoch: 11, Global Step: 119800, Data Step: 239600, Loss: 3.109375, Token per second per gpu: 25016.887886353703
Epoch: 11, Global Step: 119900, Data Step: 239800, Loss: 3.0625, Token per second per gpu: 25000.040657470516
Epoch: 11, Global Step: 120000, Data Step: 240000, Loss: 3.140625, Token per second per gpu: 24996.20015030793
I0328 09:28:49.683602 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0328 09:28:49.684011 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-28 09:28:49,684] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-28 09:28:49,688] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-28 09:28:49,688] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-28 09:28:50,227] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-28 09:28:50,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-28 09:28:50,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-28 09:28:50,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-28 09:28:50,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-28 09:28:50,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-28 09:28:50,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-28 09:28:52,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-28 09:28:52,375] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-28 09:28:52,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 09:28:52,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-28 09:28:52,482] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-28 09:28:52,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 09:28:52,514] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-28 09:28:52,514] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-28 09:28:52,514] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 09:28:52,514] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-28 09:28:52,528] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-28 09:28:52,528] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 09:28:52,533] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-28 09:28:52,534] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-28 09:28:52,534] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 09:28:52,545] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-28 09:28:52,545] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-28 09:28:52,545] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0328 09:28:52.546214 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0328 09:28:52.546838 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0328 09:28:52.547083 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0328 09:28:52.548618 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 11, Global Step: 120100, Data Step: 240200, Loss: 3.15625, Token per second per gpu: 24430.06841380322
Epoch: 11, Global Step: 120200, Data Step: 240400, Loss: 3.0625, Token per second per gpu: 25022.18150844399
Epoch: 11, Global Step: 120300, Data Step: 240600, Loss: 3.109375, Token per second per gpu: 25003.590960863607
Epoch: 11, Global Step: 120400, Data Step: 240800, Loss: 2.546875, Token per second per gpu: 24989.56370465897
Epoch: 11, Global Step: 120500, Data Step: 241000, Loss: 3.125, Token per second per gpu: 24994.898004251754
Epoch: 11, Global Step: 120600, Data Step: 241200, Loss: 2.890625, Token per second per gpu: 24999.860085088258
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0328 09:40:44.097116 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 12, Global Step: 120700, Data Step: 241400, Loss: 3.125, Token per second per gpu: 24387.2995485366
Epoch: 12, Global Step: 120800, Data Step: 241600, Loss: 3.15625, Token per second per gpu: 25007.875198044472
Epoch: 12, Global Step: 120900, Data Step: 241800, Loss: 3.25, Token per second per gpu: 25003.443615224573
Epoch: 12, Global Step: 121000, Data Step: 242000, Loss: 3.203125, Token per second per gpu: 24993.974698571572
Epoch: 12, Global Step: 121100, Data Step: 242200, Loss: 2.953125, Token per second per gpu: 24983.741141276398
Epoch: 12, Global Step: 121200, Data Step: 242400, Loss: 3.265625, Token per second per gpu: 24984.664259462577
Epoch: 12, Global Step: 121300, Data Step: 242600, Loss: 3.328125, Token per second per gpu: 24973.37924360453
Epoch: 12, Global Step: 121400, Data Step: 242800, Loss: 3.1875, Token per second per gpu: 24985.396694584844
Epoch: 12, Global Step: 121500, Data Step: 243000, Loss: 2.9375, Token per second per gpu: 24987.59870287183
Epoch: 12, Global Step: 121600, Data Step: 243200, Loss: 3.3125, Token per second per gpu: 24996.660764718818
Epoch: 12, Global Step: 121700, Data Step: 243400, Loss: 3.0625, Token per second per gpu: 24993.51159748219
Epoch: 12, Global Step: 121800, Data Step: 243600, Loss: 2.90625, Token per second per gpu: 22105.829077207218
Epoch: 12, Global Step: 121900, Data Step: 243800, Loss: 3.21875, Token per second per gpu: 24324.27987888946
Epoch: 12, Global Step: 122000, Data Step: 244000, Loss: 3.09375, Token per second per gpu: 25006.311914184887
Epoch: 12, Global Step: 122100, Data Step: 244200, Loss: 3.40625, Token per second per gpu: 24984.96321266024
Epoch: 12, Global Step: 122200, Data Step: 244400, Loss: 2.921875, Token per second per gpu: 24987.860095990065
Epoch: 12, Global Step: 122300, Data Step: 244600, Loss: 3.46875, Token per second per gpu: 24977.30079736603
Epoch: 12, Global Step: 122400, Data Step: 244800, Loss: 3.15625, Token per second per gpu: 24994.26591028975
Epoch: 12, Global Step: 122500, Data Step: 245000, Loss: 2.859375, Token per second per gpu: 25001.155193461764
Epoch: 12, Global Step: 122600, Data Step: 245200, Loss: 3.203125, Token per second per gpu: 24976.498860472147
Epoch: 12, Global Step: 122700, Data Step: 245400, Loss: 3.171875, Token per second per gpu: 24966.19988312947
Epoch: 12, Global Step: 122800, Data Step: 245600, Loss: 2.859375, Token per second per gpu: 24973.770192181208
Epoch: 12, Global Step: 122900, Data Step: 245800, Loss: 3.0, Token per second per gpu: 24979.796799133444
Epoch: 12, Global Step: 123000, Data Step: 246000, Loss: 2.75, Token per second per gpu: 24970.461869169045
Epoch: 12, Global Step: 123100, Data Step: 246200, Loss: 3.234375, Token per second per gpu: 24976.98297051772
Epoch: 12, Global Step: 123200, Data Step: 246400, Loss: 2.828125, Token per second per gpu: 24988.105883908836
Epoch: 12, Global Step: 123300, Data Step: 246600, Loss: 3.078125, Token per second per gpu: 24988.851340940146
Epoch: 12, Global Step: 123400, Data Step: 246800, Loss: 3.15625, Token per second per gpu: 24997.163606411636
Epoch: 12, Global Step: 123500, Data Step: 247000, Loss: 3.1875, Token per second per gpu: 24997.527003727417
Epoch: 12, Global Step: 123600, Data Step: 247200, Loss: 3.046875, Token per second per gpu: 24968.80241072599
Epoch: 12, Global Step: 123700, Data Step: 247400, Loss: 3.328125, Token per second per gpu: 24978.462478372876
Epoch: 12, Global Step: 123800, Data Step: 247600, Loss: 3.015625, Token per second per gpu: 24977.317892279494
Epoch: 12, Global Step: 123900, Data Step: 247800, Loss: 2.984375, Token per second per gpu: 24969.471566442964
Epoch: 12, Global Step: 124000, Data Step: 248000, Loss: 3.25, Token per second per gpu: 24981.64257243743
Epoch: 12, Global Step: 124100, Data Step: 248200, Loss: 2.859375, Token per second per gpu: 24956.48309265408
Epoch: 12, Global Step: 124200, Data Step: 248400, Loss: 3.296875, Token per second per gpu: 24969.007618107076
Epoch: 12, Global Step: 124300, Data Step: 248600, Loss: 3.4375, Token per second per gpu: 24963.560134492378
Epoch: 12, Global Step: 124400, Data Step: 248800, Loss: 2.984375, Token per second per gpu: 24975.05547284083
Epoch: 12, Global Step: 124500, Data Step: 249000, Loss: 3.484375, Token per second per gpu: 24979.081014810305
Epoch: 12, Global Step: 124600, Data Step: 249200, Loss: 3.453125, Token per second per gpu: 24972.183911135526
Epoch: 12, Global Step: 124700, Data Step: 249400, Loss: 3.109375, Token per second per gpu: 24965.56743388459
Epoch: 12, Global Step: 124800, Data Step: 249600, Loss: 3.15625, Token per second per gpu: 24973.923435797205
Epoch: 12, Global Step: 124900, Data Step: 249800, Loss: 3.3125, Token per second per gpu: 24967.75402706101
Epoch: 12, Global Step: 125000, Data Step: 250000, Loss: 3.140625, Token per second per gpu: 24960.635872827293
Epoch: 12, Global Step: 125100, Data Step: 250200, Loss: 3.234375, Token per second per gpu: 24960.138935099523
Epoch: 12, Global Step: 125200, Data Step: 250400, Loss: 3.171875, Token per second per gpu: 24966.663727129384
Epoch: 12, Global Step: 125300, Data Step: 250600, Loss: 3.46875, Token per second per gpu: 24968.848035013085
Epoch: 12, Global Step: 125400, Data Step: 250800, Loss: 3.421875, Token per second per gpu: 24941.985785937035
Epoch: 12, Global Step: 125500, Data Step: 251000, Loss: 3.25, Token per second per gpu: 24965.154710565344
Epoch: 12, Global Step: 125600, Data Step: 251200, Loss: 3.1875, Token per second per gpu: 24972.472033977756
Epoch: 12, Global Step: 125700, Data Step: 251400, Loss: 3.1875, Token per second per gpu: 24977.925784019968
Epoch: 12, Global Step: 125800, Data Step: 251600, Loss: 3.1875, Token per second per gpu: 24970.38965586486
Epoch: 12, Global Step: 125900, Data Step: 251800, Loss: 3.109375, Token per second per gpu: 24985.419071921275
Epoch: 12, Global Step: 126000, Data Step: 252000, Loss: 3.078125, Token per second per gpu: 24973.31238291701
Epoch: 12, Global Step: 126100, Data Step: 252200, Loss: 3.296875, Token per second per gpu: 24978.694910184553
Epoch: 12, Global Step: 126200, Data Step: 252400, Loss: 3.1875, Token per second per gpu: 24968.468955925586
Epoch: 12, Global Step: 126300, Data Step: 252600, Loss: 3.15625, Token per second per gpu: 24973.509558436737
Epoch: 12, Global Step: 126400, Data Step: 252800, Loss: 3.15625, Token per second per gpu: 24966.420270091934
Epoch: 12, Global Step: 126500, Data Step: 253000, Loss: 3.3125, Token per second per gpu: 24962.776670550174
Epoch: 12, Global Step: 126600, Data Step: 253200, Loss: 2.828125, Token per second per gpu: 24973.899684978853
Epoch: 12, Global Step: 126700, Data Step: 253400, Loss: 3.484375, Token per second per gpu: 24965.293246625057
Epoch: 12, Global Step: 126800, Data Step: 253600, Loss: 3.21875, Token per second per gpu: 24960.16291762832
Epoch: 12, Global Step: 126900, Data Step: 253800, Loss: 3.296875, Token per second per gpu: 24965.908602654978
Epoch: 12, Global Step: 127000, Data Step: 254000, Loss: 2.671875, Token per second per gpu: 24963.207990074545
Epoch: 12, Global Step: 127100, Data Step: 254200, Loss: 3.03125, Token per second per gpu: 24973.50202037848
Epoch: 12, Global Step: 127200, Data Step: 254400, Loss: 3.078125, Token per second per gpu: 24956.188017266468
Epoch: 12, Global Step: 127300, Data Step: 254600, Loss: 3.234375, Token per second per gpu: 24960.515852946166
Epoch: 12, Global Step: 127400, Data Step: 254800, Loss: 2.984375, Token per second per gpu: 24976.04260367438
Epoch: 12, Global Step: 127500, Data Step: 255000, Loss: 3.390625, Token per second per gpu: 24983.715718252166
Epoch: 12, Global Step: 127600, Data Step: 255200, Loss: 3.140625, Token per second per gpu: 24986.50527364574
Epoch: 12, Global Step: 127700, Data Step: 255400, Loss: 3.3125, Token per second per gpu: 24970.69657819697
Epoch: 12, Global Step: 127800, Data Step: 255600, Loss: 3.390625, Token per second per gpu: 24984.925436156882
Epoch: 12, Global Step: 127900, Data Step: 255800, Loss: 3.3125, Token per second per gpu: 24365.416759952055
Epoch: 12, Global Step: 128000, Data Step: 256000, Loss: 3.109375, Token per second per gpu: 25000.215230499944
Epoch: 12, Global Step: 128100, Data Step: 256200, Loss: 3.109375, Token per second per gpu: 24984.186982720817
Epoch: 12, Global Step: 128200, Data Step: 256400, Loss: 2.859375, Token per second per gpu: 24983.743983284356
Epoch: 12, Global Step: 128300, Data Step: 256600, Loss: 3.046875, Token per second per gpu: 24964.587734871344
Epoch: 12, Global Step: 128400, Data Step: 256800, Loss: 3.15625, Token per second per gpu: 24962.038339358434
Epoch: 12, Global Step: 128500, Data Step: 257000, Loss: 3.53125, Token per second per gpu: 24970.336283326626
Epoch: 12, Global Step: 128600, Data Step: 257200, Loss: 3.109375, Token per second per gpu: 24933.3883567791
Epoch: 12, Global Step: 128700, Data Step: 257400, Loss: 3.0625, Token per second per gpu: 24951.21604370479
Epoch: 12, Global Step: 128800, Data Step: 257600, Loss: 3.203125, Token per second per gpu: 24955.93424667109
Epoch: 12, Global Step: 128900, Data Step: 257800, Loss: 3.3125, Token per second per gpu: 24959.649340558757
Epoch: 12, Global Step: 129000, Data Step: 258000, Loss: 3.0625, Token per second per gpu: 24953.002338356393
Epoch: 12, Global Step: 129100, Data Step: 258200, Loss: 3.390625, Token per second per gpu: 24347.933640701754
Epoch: 12, Global Step: 129200, Data Step: 258400, Loss: 3.015625, Token per second per gpu: 24960.035784963413
Epoch: 12, Global Step: 129300, Data Step: 258600, Loss: 3.140625, Token per second per gpu: 24930.486286263702
Epoch: 12, Global Step: 129400, Data Step: 258800, Loss: 3.203125, Token per second per gpu: 24925.64675124512
Epoch: 12, Global Step: 129500, Data Step: 259000, Loss: 3.265625, Token per second per gpu: 24337.139433458116
Epoch: 12, Global Step: 129600, Data Step: 259200, Loss: 3.125, Token per second per gpu: 24956.098562800402
Epoch: 12, Global Step: 129700, Data Step: 259400, Loss: 2.71875, Token per second per gpu: 24951.210529095824
Epoch: 12, Global Step: 129800, Data Step: 259600, Loss: 3.203125, Token per second per gpu: 24943.38744505444
Epoch: 12, Global Step: 129900, Data Step: 259800, Loss: 3.234375, Token per second per gpu: 24946.21220365866
Epoch: 12, Global Step: 130000, Data Step: 260000, Loss: 3.03125, Token per second per gpu: 24972.35752745437
I0328 12:41:33.833199 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0328 12:41:33.833617 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-28 12:41:33,834] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-28 12:41:33,838] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-28 12:41:33,838] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-28 12:41:34,371] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-28 12:41:34,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-28 12:41:34,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-28 12:41:34,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-28 12:41:34,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-28 12:41:34,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-28 12:41:34,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-28 12:41:36,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-28 12:41:36,413] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-28 12:41:36,413] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 12:41:36,537] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-28 12:41:36,538] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-28 12:41:36,538] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-28 12:41:36,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 12:41:36,538] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-28 12:41:36,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 12:41:36,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-28 12:41:36,568] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-28 12:41:36,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 12:41:36,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-28 12:41:36,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-28 12:41:36,583] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-28 12:41:36,583] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-28 12:41:36,583] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 12:41:36,583] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0328 12:41:36.583466 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0328 12:41:36.584241 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0328 12:41:36.584574 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0328 12:41:36.586550 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 12, Global Step: 130100, Data Step: 260200, Loss: 3.21875, Token per second per gpu: 24479.581913325652
Epoch: 12, Global Step: 130200, Data Step: 260400, Loss: 2.8125, Token per second per gpu: 25051.843539793375
Epoch: 12, Global Step: 130300, Data Step: 260600, Loss: 3.3125, Token per second per gpu: 25043.065293889777
Epoch: 12, Global Step: 130400, Data Step: 260800, Loss: 2.921875, Token per second per gpu: 24996.08909848048
Epoch: 12, Global Step: 130500, Data Step: 261000, Loss: 3.0625, Token per second per gpu: 24984.468923121873
Epoch: 12, Global Step: 130600, Data Step: 261200, Loss: 3.234375, Token per second per gpu: 24972.344517762507
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0328 12:54:27.256086 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 13, Global Step: 130700, Data Step: 261400, Loss: 3.3125, Token per second per gpu: 24342.00715714037
Epoch: 13, Global Step: 130800, Data Step: 261600, Loss: 2.96875, Token per second per gpu: 24959.960434327855
Epoch: 13, Global Step: 130900, Data Step: 261800, Loss: 3.171875, Token per second per gpu: 24932.650733756567
Epoch: 13, Global Step: 131000, Data Step: 262000, Loss: 3.34375, Token per second per gpu: 25021.285883581862
Epoch: 13, Global Step: 131100, Data Step: 262200, Loss: 3.234375, Token per second per gpu: 25062.283665362378
Epoch: 13, Global Step: 131200, Data Step: 262400, Loss: 2.734375, Token per second per gpu: 25048.663780529896
Epoch: 13, Global Step: 131300, Data Step: 262600, Loss: 3.03125, Token per second per gpu: 25038.858469238552
Epoch: 13, Global Step: 131400, Data Step: 262800, Loss: 3.140625, Token per second per gpu: 24998.72921285805
Epoch: 13, Global Step: 131500, Data Step: 263000, Loss: 3.234375, Token per second per gpu: 24976.949453013163
Epoch: 13, Global Step: 131600, Data Step: 263200, Loss: 3.015625, Token per second per gpu: 24964.757995136904
Epoch: 13, Global Step: 131700, Data Step: 263400, Loss: 2.765625, Token per second per gpu: 24964.356752156895
Epoch: 13, Global Step: 131800, Data Step: 263600, Loss: 2.640625, Token per second per gpu: 24962.313280474897
Epoch: 13, Global Step: 131900, Data Step: 263800, Loss: 2.921875, Token per second per gpu: 24954.999433731857
Epoch: 13, Global Step: 132000, Data Step: 264000, Loss: 2.90625, Token per second per gpu: 24961.952298992506
Epoch: 13, Global Step: 132100, Data Step: 264200, Loss: 3.40625, Token per second per gpu: 24948.903420252915
Epoch: 13, Global Step: 132200, Data Step: 264400, Loss: 3.125, Token per second per gpu: 24957.75071235793
Epoch: 13, Global Step: 132300, Data Step: 264600, Loss: 3.0625, Token per second per gpu: 24963.037235077794
Epoch: 13, Global Step: 132400, Data Step: 264800, Loss: 3.421875, Token per second per gpu: 24975.185392067448
Epoch: 13, Global Step: 132500, Data Step: 265000, Loss: 3.1875, Token per second per gpu: 24996.41284291568
Epoch: 13, Global Step: 132600, Data Step: 265200, Loss: 2.90625, Token per second per gpu: 25011.78061184177
Epoch: 13, Global Step: 132700, Data Step: 265400, Loss: 3.140625, Token per second per gpu: 25029.317232478716
Epoch: 13, Global Step: 132800, Data Step: 265600, Loss: 3.359375, Token per second per gpu: 25028.556187162143
Epoch: 13, Global Step: 132900, Data Step: 265800, Loss: 3.046875, Token per second per gpu: 24387.034961852645
Epoch: 13, Global Step: 133000, Data Step: 266000, Loss: 3.140625, Token per second per gpu: 25055.276830120423
Epoch: 13, Global Step: 133100, Data Step: 266200, Loss: 3.171875, Token per second per gpu: 25045.78622549752
Epoch: 13, Global Step: 133200, Data Step: 266400, Loss: 3.0, Token per second per gpu: 25044.96217805217
Epoch: 13, Global Step: 133300, Data Step: 266600, Loss: 3.359375, Token per second per gpu: 25038.99413947901
Epoch: 13, Global Step: 133400, Data Step: 266800, Loss: 3.0625, Token per second per gpu: 25044.32116282454
Epoch: 13, Global Step: 133500, Data Step: 267000, Loss: 3.046875, Token per second per gpu: 25052.3049083074
Epoch: 13, Global Step: 133600, Data Step: 267200, Loss: 3.03125, Token per second per gpu: 25050.561876580185
Epoch: 13, Global Step: 133700, Data Step: 267400, Loss: 3.078125, Token per second per gpu: 25047.180982755526
Epoch: 13, Global Step: 133800, Data Step: 267600, Loss: 3.5, Token per second per gpu: 25042.01673878128
Epoch: 13, Global Step: 133900, Data Step: 267800, Loss: 3.15625, Token per second per gpu: 25047.28620476242
Epoch: 13, Global Step: 134000, Data Step: 268000, Loss: 3.265625, Token per second per gpu: 25026.925657528715
Epoch: 13, Global Step: 134100, Data Step: 268200, Loss: 3.171875, Token per second per gpu: 25022.93900418961
Epoch: 13, Global Step: 134200, Data Step: 268400, Loss: 2.96875, Token per second per gpu: 25034.47924858799
Epoch: 13, Global Step: 134300, Data Step: 268600, Loss: 3.28125, Token per second per gpu: 25021.880109007743
Epoch: 13, Global Step: 134400, Data Step: 268800, Loss: 3.203125, Token per second per gpu: 25031.938227347084
Epoch: 13, Global Step: 134500, Data Step: 269000, Loss: 3.015625, Token per second per gpu: 25029.62119679913
Epoch: 13, Global Step: 134600, Data Step: 269200, Loss: 2.921875, Token per second per gpu: 25027.141258563555
Epoch: 13, Global Step: 134700, Data Step: 269400, Loss: 3.109375, Token per second per gpu: 25036.67076904595
Epoch: 13, Global Step: 134800, Data Step: 269600, Loss: 3.46875, Token per second per gpu: 25028.28300003678
Epoch: 13, Global Step: 134900, Data Step: 269800, Loss: 3.0, Token per second per gpu: 25029.964844589493
Epoch: 13, Global Step: 135000, Data Step: 270000, Loss: 3.1875, Token per second per gpu: 25017.307658577643
Epoch: 13, Global Step: 135100, Data Step: 270200, Loss: 2.890625, Token per second per gpu: 25024.463516578395
Epoch: 13, Global Step: 135200, Data Step: 270400, Loss: 3.296875, Token per second per gpu: 25018.49119988288
Epoch: 13, Global Step: 135300, Data Step: 270600, Loss: 3.1875, Token per second per gpu: 25008.551005950994
Epoch: 13, Global Step: 135400, Data Step: 270800, Loss: 3.078125, Token per second per gpu: 25006.299024389635
Epoch: 13, Global Step: 135500, Data Step: 271000, Loss: 3.140625, Token per second per gpu: 25004.74069947387
Epoch: 13, Global Step: 135600, Data Step: 271200, Loss: 3.15625, Token per second per gpu: 25007.709578129186
Epoch: 13, Global Step: 135700, Data Step: 271400, Loss: 3.015625, Token per second per gpu: 25007.70818028366
Epoch: 13, Global Step: 135800, Data Step: 271600, Loss: 2.921875, Token per second per gpu: 24982.698788039394
Epoch: 13, Global Step: 135900, Data Step: 271800, Loss: 3.125, Token per second per gpu: 24980.48116387661
Epoch: 13, Global Step: 136000, Data Step: 272000, Loss: 3.109375, Token per second per gpu: 24978.967842304828
Epoch: 13, Global Step: 136100, Data Step: 272200, Loss: 3.203125, Token per second per gpu: 20845.409398791257
Epoch: 13, Global Step: 136200, Data Step: 272400, Loss: 3.28125, Token per second per gpu: 21562.61791670568
Epoch: 13, Global Step: 136300, Data Step: 272600, Loss: 3.375, Token per second per gpu: 21372.405625974527
Epoch: 13, Global Step: 136400, Data Step: 272800, Loss: 2.953125, Token per second per gpu: 21230.312310056048
Epoch: 13, Global Step: 136500, Data Step: 273000, Loss: 3.109375, Token per second per gpu: 21899.955337280222
Epoch: 13, Global Step: 136600, Data Step: 273200, Loss: 2.90625, Token per second per gpu: 21702.982631897656
Epoch: 13, Global Step: 136700, Data Step: 273400, Loss: 3.09375, Token per second per gpu: 21467.215942733816
Epoch: 13, Global Step: 136800, Data Step: 273600, Loss: 3.515625, Token per second per gpu: 21587.675933722152
Epoch: 13, Global Step: 136900, Data Step: 273800, Loss: 3.328125, Token per second per gpu: 21557.217088091595
Epoch: 13, Global Step: 137000, Data Step: 274000, Loss: 3.125, Token per second per gpu: 21498.80300160534
Epoch: 13, Global Step: 137100, Data Step: 274200, Loss: 3.3125, Token per second per gpu: 21668.990558357586
Epoch: 13, Global Step: 137200, Data Step: 274400, Loss: 3.015625, Token per second per gpu: 21940.534923720832
Epoch: 13, Global Step: 137300, Data Step: 274600, Loss: 2.890625, Token per second per gpu: 21299.865300113946
Epoch: 13, Global Step: 137400, Data Step: 274800, Loss: 3.265625, Token per second per gpu: 21650.05512955336
Epoch: 13, Global Step: 137500, Data Step: 275000, Loss: 3.125, Token per second per gpu: 21505.985979489877
Epoch: 13, Global Step: 137600, Data Step: 275200, Loss: 3.0625, Token per second per gpu: 21012.894242841277
Epoch: 13, Global Step: 137700, Data Step: 275400, Loss: 3.28125, Token per second per gpu: 21638.623006701037
Epoch: 13, Global Step: 137800, Data Step: 275600, Loss: 3.1875, Token per second per gpu: 20613.471902049747
Epoch: 13, Global Step: 137900, Data Step: 275800, Loss: 3.203125, Token per second per gpu: 21938.828784958536
Epoch: 13, Global Step: 138000, Data Step: 276000, Loss: 3.015625, Token per second per gpu: 21255.498553133053
Epoch: 13, Global Step: 138100, Data Step: 276200, Loss: 3.4375, Token per second per gpu: 21462.74988025943
Epoch: 13, Global Step: 138200, Data Step: 276400, Loss: 3.421875, Token per second per gpu: 21921.235730982175
Epoch: 13, Global Step: 138300, Data Step: 276600, Loss: 3.328125, Token per second per gpu: 21508.959976585502
Epoch: 13, Global Step: 138400, Data Step: 276800, Loss: 3.078125, Token per second per gpu: 20943.08049654967
Epoch: 13, Global Step: 138500, Data Step: 277000, Loss: 2.46875, Token per second per gpu: 20487.252289462656
Epoch: 13, Global Step: 138600, Data Step: 277200, Loss: 3.265625, Token per second per gpu: 20493.334023027237
Epoch: 13, Global Step: 138700, Data Step: 277400, Loss: 3.078125, Token per second per gpu: 20631.810188874544
Epoch: 13, Global Step: 138800, Data Step: 277600, Loss: 2.984375, Token per second per gpu: 20341.788379916634
Epoch: 13, Global Step: 138900, Data Step: 277800, Loss: 3.203125, Token per second per gpu: 20378.805519513695
Epoch: 13, Global Step: 139000, Data Step: 278000, Loss: 3.296875, Token per second per gpu: 18769.167199097767
Epoch: 13, Global Step: 139100, Data Step: 278200, Loss: 2.78125, Token per second per gpu: 20321.46129065357
Epoch: 13, Global Step: 139200, Data Step: 278400, Loss: 3.53125, Token per second per gpu: 20274.911016931444
Epoch: 13, Global Step: 139300, Data Step: 278600, Loss: 2.890625, Token per second per gpu: 20235.73958528016
Epoch: 13, Global Step: 139400, Data Step: 278800, Loss: 3.28125, Token per second per gpu: 18849.69592752696
Epoch: 13, Global Step: 139500, Data Step: 279000, Loss: 3.140625, Token per second per gpu: 20513.78069719634
Epoch: 13, Global Step: 139600, Data Step: 279200, Loss: 3.109375, Token per second per gpu: 20247.798814151643
Epoch: 13, Global Step: 139700, Data Step: 279400, Loss: 3.046875, Token per second per gpu: 20295.151689320744
Epoch: 13, Global Step: 139800, Data Step: 279600, Loss: 3.3125, Token per second per gpu: 20449.44700262106
Epoch: 13, Global Step: 139900, Data Step: 279800, Loss: 2.875, Token per second per gpu: 20315.594013634876
Epoch: 13, Global Step: 140000, Data Step: 280000, Loss: 3.453125, Token per second per gpu: 20242.08134682128
I0328 16:08:38.282745 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0328 16:08:38.286171 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-28 16:08:38,288] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-28 16:08:38,297] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-28 16:08:38,309] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-28 16:08:39,611] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-28 16:08:39,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-28 16:08:39,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-28 16:08:39,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-28 16:08:39,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-28 16:08:39,655] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-28 16:08:39,665] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-28 16:08:41,106] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-28 16:08:41,106] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-28 16:08:41,106] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 16:08:41,556] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-28 16:08:41,557] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-28 16:08:41,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 16:08:41,658] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-28 16:08:41,658] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-28 16:08:41,658] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 16:08:41,760] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-28 16:08:41,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-28 16:08:41,761] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-28 16:08:41,761] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 16:08:41,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-28 16:08:41,766] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-28 16:08:41,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 16:08:41,772] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-28 16:08:41,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0328 16:08:41.773090 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0328 16:08:41.773960 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0328 16:08:41.774258 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0328 16:08:41.776240 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 13, Global Step: 140100, Data Step: 280200, Loss: 3.296875, Token per second per gpu: 19871.66011311479
Epoch: 13, Global Step: 140200, Data Step: 280400, Loss: 3.0, Token per second per gpu: 20434.406382084504
Epoch: 13, Global Step: 140300, Data Step: 280600, Loss: 3.171875, Token per second per gpu: 20597.11511749744
Epoch: 13, Global Step: 140400, Data Step: 280800, Loss: 2.8125, Token per second per gpu: 20152.63839135026
Epoch: 13, Global Step: 140500, Data Step: 281000, Loss: 3.390625, Token per second per gpu: 20579.368345905652
Epoch: 13, Global Step: 140600, Data Step: 281200, Loss: 3.3125, Token per second per gpu: 20266.372741629297
Epoch: 13, Global Step: 140700, Data Step: 281400, Loss: 3.03125, Token per second per gpu: 20342.214420137978
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0328 16:25:40.020385 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 14, Global Step: 140800, Data Step: 281600, Loss: 2.921875, Token per second per gpu: 19226.52279398107
Epoch: 14, Global Step: 140900, Data Step: 281800, Loss: 3.015625, Token per second per gpu: 20458.678089621397
Epoch: 14, Global Step: 141000, Data Step: 282000, Loss: 2.921875, Token per second per gpu: 20702.88231048201
Epoch: 14, Global Step: 141100, Data Step: 282200, Loss: 2.953125, Token per second per gpu: 20730.817928172313
Epoch: 14, Global Step: 141200, Data Step: 282400, Loss: 3.34375, Token per second per gpu: 20274.689244023113
Epoch: 14, Global Step: 141300, Data Step: 282600, Loss: 3.125, Token per second per gpu: 20681.202662750173
Epoch: 14, Global Step: 141400, Data Step: 282800, Loss: 3.21875, Token per second per gpu: 20669.57950035258
Epoch: 14, Global Step: 141500, Data Step: 283000, Loss: 2.921875, Token per second per gpu: 20529.437279903956
Epoch: 14, Global Step: 141600, Data Step: 283200, Loss: 2.984375, Token per second per gpu: 21525.1975976398
Epoch: 14, Global Step: 141700, Data Step: 283400, Loss: 3.046875, Token per second per gpu: 20549.00624587205
Epoch: 14, Global Step: 141800, Data Step: 283600, Loss: 3.140625, Token per second per gpu: 20606.495149687
Epoch: 14, Global Step: 141900, Data Step: 283800, Loss: 2.953125, Token per second per gpu: 20363.886584229254
Epoch: 14, Global Step: 142000, Data Step: 284000, Loss: 3.28125, Token per second per gpu: 20579.41862204267
Epoch: 14, Global Step: 142100, Data Step: 284200, Loss: 3.4375, Token per second per gpu: 20476.546172842918
Epoch: 14, Global Step: 142200, Data Step: 284400, Loss: 3.421875, Token per second per gpu: 21378.106382467035
Epoch: 14, Global Step: 142300, Data Step: 284600, Loss: 3.4375, Token per second per gpu: 20241.223406565583
Epoch: 14, Global Step: 142400, Data Step: 284800, Loss: 3.390625, Token per second per gpu: 20707.652557531725
Epoch: 14, Global Step: 142500, Data Step: 285000, Loss: 3.421875, Token per second per gpu: 20649.015431720807
Epoch: 14, Global Step: 142600, Data Step: 285200, Loss: 3.171875, Token per second per gpu: 20681.409340260794
Epoch: 14, Global Step: 142700, Data Step: 285400, Loss: 2.96875, Token per second per gpu: 20965.492420928982
Epoch: 14, Global Step: 142800, Data Step: 285600, Loss: 3.25, Token per second per gpu: 20082.95171884276
Epoch: 14, Global Step: 142900, Data Step: 285800, Loss: 2.875, Token per second per gpu: 21102.60631295264
Epoch: 14, Global Step: 143000, Data Step: 286000, Loss: 3.109375, Token per second per gpu: 20931.886091467168
Epoch: 14, Global Step: 143100, Data Step: 286200, Loss: 2.9375, Token per second per gpu: 20995.051406705552
Epoch: 14, Global Step: 143200, Data Step: 286400, Loss: 2.859375, Token per second per gpu: 20911.507024434646
Epoch: 14, Global Step: 143300, Data Step: 286600, Loss: 3.359375, Token per second per gpu: 20555.948315510406
Epoch: 14, Global Step: 143400, Data Step: 286800, Loss: 2.875, Token per second per gpu: 20523.83239682029
Epoch: 14, Global Step: 143500, Data Step: 287000, Loss: 3.09375, Token per second per gpu: 20984.91249100105
Epoch: 14, Global Step: 143600, Data Step: 287200, Loss: 3.40625, Token per second per gpu: 20682.640603475284
Epoch: 14, Global Step: 143700, Data Step: 287400, Loss: 3.328125, Token per second per gpu: 20587.960329973812
Epoch: 14, Global Step: 143800, Data Step: 287600, Loss: 2.890625, Token per second per gpu: 20476.08533174922
Epoch: 14, Global Step: 143900, Data Step: 287800, Loss: 3.25, Token per second per gpu: 20628.565161922634
Epoch: 14, Global Step: 144000, Data Step: 288000, Loss: 2.984375, Token per second per gpu: 19033.97039948949
Epoch: 14, Global Step: 144100, Data Step: 288200, Loss: 3.234375, Token per second per gpu: 20438.194781141
Epoch: 14, Global Step: 144200, Data Step: 288400, Loss: 3.28125, Token per second per gpu: 20818.239674552315
Epoch: 14, Global Step: 144300, Data Step: 288600, Loss: 2.890625, Token per second per gpu: 25055.420369868927
Epoch: 14, Global Step: 144400, Data Step: 288800, Loss: 2.953125, Token per second per gpu: 25019.68469928792
Epoch: 14, Global Step: 144500, Data Step: 289000, Loss: 2.71875, Token per second per gpu: 24983.148054199297
Epoch: 14, Global Step: 144600, Data Step: 289200, Loss: 3.203125, Token per second per gpu: 24981.763673875666
Epoch: 14, Global Step: 144700, Data Step: 289400, Loss: 2.734375, Token per second per gpu: 24982.344605120077
Epoch: 14, Global Step: 144800, Data Step: 289600, Loss: 3.125, Token per second per gpu: 24995.391051461283
Epoch: 14, Global Step: 144900, Data Step: 289800, Loss: 3.125, Token per second per gpu: 24978.804257935255
Epoch: 14, Global Step: 145000, Data Step: 290000, Loss: 3.015625, Token per second per gpu: 24964.79942548617
Epoch: 14, Global Step: 145100, Data Step: 290200, Loss: 3.40625, Token per second per gpu: 24986.767832651658
Epoch: 14, Global Step: 145200, Data Step: 290400, Loss: 3.25, Token per second per gpu: 24998.83780484941
Epoch: 14, Global Step: 145300, Data Step: 290600, Loss: 3.140625, Token per second per gpu: 24987.35204623768
Epoch: 14, Global Step: 145400, Data Step: 290800, Loss: 3.265625, Token per second per gpu: 25007.34018994075
Epoch: 14, Global Step: 145500, Data Step: 291000, Loss: 3.125, Token per second per gpu: 24989.985041938064
Epoch: 14, Global Step: 145600, Data Step: 291200, Loss: 3.046875, Token per second per gpu: 24994.12069145099
Epoch: 14, Global Step: 145700, Data Step: 291400, Loss: 3.203125, Token per second per gpu: 24978.468314940976
Epoch: 14, Global Step: 145800, Data Step: 291600, Loss: 3.171875, Token per second per gpu: 24978.58628651283
Epoch: 14, Global Step: 145900, Data Step: 291800, Loss: 3.0625, Token per second per gpu: 24979.14883615964
Epoch: 14, Global Step: 146000, Data Step: 292000, Loss: 3.15625, Token per second per gpu: 24989.81469589162
Epoch: 14, Global Step: 146100, Data Step: 292200, Loss: 2.953125, Token per second per gpu: 24982.684372521024
Epoch: 14, Global Step: 146200, Data Step: 292400, Loss: 3.171875, Token per second per gpu: 24962.986163795435
Epoch: 14, Global Step: 146300, Data Step: 292600, Loss: 3.203125, Token per second per gpu: 24928.315629722492
Epoch: 14, Global Step: 146400, Data Step: 292800, Loss: 3.03125, Token per second per gpu: 24924.94707857283
Epoch: 14, Global Step: 146500, Data Step: 293000, Loss: 3.0625, Token per second per gpu: 21986.547677359253
Epoch: 14, Global Step: 146600, Data Step: 293200, Loss: 2.96875, Token per second per gpu: 22219.91833685847
Epoch: 14, Global Step: 146700, Data Step: 293400, Loss: 2.90625, Token per second per gpu: 22314.78584639164
Epoch: 14, Global Step: 146800, Data Step: 293600, Loss: 3.09375, Token per second per gpu: 22915.682179410804
Epoch: 14, Global Step: 146900, Data Step: 293800, Loss: 3.046875, Token per second per gpu: 22677.949176477236
Epoch: 14, Global Step: 147000, Data Step: 294000, Loss: 3.0625, Token per second per gpu: 22470.900344403377
Epoch: 14, Global Step: 147100, Data Step: 294200, Loss: 2.65625, Token per second per gpu: 22628.362876284038
Epoch: 14, Global Step: 147200, Data Step: 294400, Loss: 2.625, Token per second per gpu: 22027.398811216244
Epoch: 14, Global Step: 147300, Data Step: 294600, Loss: 2.96875, Token per second per gpu: 22054.68118039138
Epoch: 14, Global Step: 147400, Data Step: 294800, Loss: 2.734375, Token per second per gpu: 22318.821010189393
Epoch: 14, Global Step: 147500, Data Step: 295000, Loss: 3.0625, Token per second per gpu: 22276.403988235365
Epoch: 14, Global Step: 147600, Data Step: 295200, Loss: 3.296875, Token per second per gpu: 21893.36999073162
Epoch: 14, Global Step: 147700, Data Step: 295400, Loss: 3.5, Token per second per gpu: 22718.078585010506
Epoch: 14, Global Step: 147800, Data Step: 295600, Loss: 3.234375, Token per second per gpu: 22688.2531863643
Epoch: 14, Global Step: 147900, Data Step: 295800, Loss: 3.234375, Token per second per gpu: 22557.090471721305
Epoch: 14, Global Step: 148000, Data Step: 296000, Loss: 3.171875, Token per second per gpu: 22564.49192095305
Epoch: 14, Global Step: 148100, Data Step: 296200, Loss: 3.09375, Token per second per gpu: 22550.422477550215
Epoch: 14, Global Step: 148200, Data Step: 296400, Loss: 3.078125, Token per second per gpu: 22630.896415448915
Epoch: 14, Global Step: 148300, Data Step: 296600, Loss: 3.546875, Token per second per gpu: 22122.329535493816
Epoch: 14, Global Step: 148400, Data Step: 296800, Loss: 2.765625, Token per second per gpu: 22733.350778000364
Epoch: 14, Global Step: 148500, Data Step: 297000, Loss: 3.28125, Token per second per gpu: 24317.051208548997
Epoch: 14, Global Step: 148600, Data Step: 297200, Loss: 3.203125, Token per second per gpu: 24615.92264743201
Epoch: 14, Global Step: 148700, Data Step: 297400, Loss: 3.15625, Token per second per gpu: 22559.886654736645
Epoch: 14, Global Step: 148800, Data Step: 297600, Loss: 3.140625, Token per second per gpu: 22822.5416889693
Epoch: 14, Global Step: 148900, Data Step: 297800, Loss: 3.234375, Token per second per gpu: 22553.125465156285
Epoch: 14, Global Step: 149000, Data Step: 298000, Loss: 3.265625, Token per second per gpu: 21480.32857040888
Epoch: 14, Global Step: 149100, Data Step: 298200, Loss: 2.75, Token per second per gpu: 22515.781948106975
Epoch: 14, Global Step: 149200, Data Step: 298400, Loss: 2.734375, Token per second per gpu: 22207.706222154444
Epoch: 14, Global Step: 149300, Data Step: 298600, Loss: 3.03125, Token per second per gpu: 22457.78256109033
Epoch: 14, Global Step: 149400, Data Step: 298800, Loss: 3.078125, Token per second per gpu: 21251.487726523264
Epoch: 14, Global Step: 149500, Data Step: 299000, Loss: 3.078125, Token per second per gpu: 22344.291017469597
Epoch: 14, Global Step: 149600, Data Step: 299200, Loss: 3.453125, Token per second per gpu: 22283.184448207176
Epoch: 14, Global Step: 149700, Data Step: 299400, Loss: 3.09375, Token per second per gpu: 21948.37363619382
Epoch: 14, Global Step: 149800, Data Step: 299600, Loss: 3.140625, Token per second per gpu: 22690.36037618143
Epoch: 14, Global Step: 149900, Data Step: 299800, Loss: 3.265625, Token per second per gpu: 22337.825011350695
Epoch: 14, Global Step: 150000, Data Step: 300000, Loss: 2.859375, Token per second per gpu: 22276.07177361249
I0328 19:46:00.715960 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0328 19:46:00.716383 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-28 19:46:00,716] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-28 19:46:00,720] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-28 19:46:00,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-28 19:46:01,313] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-28 19:46:01,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-28 19:46:01,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-28 19:46:01,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-28 19:46:01,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-28 19:46:01,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-28 19:46:01,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-28 19:46:03,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-28 19:46:03,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-28 19:46:03,481] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-28 19:46:03,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 19:46:03,492] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-28 19:46:03,493] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 19:46:03,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-28 19:46:03,518] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-28 19:46:03,518] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 19:46:03,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-28 19:46:03,540] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-28 19:46:03,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 19:46:03,555] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-28 19:46:03,555] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-28 19:46:03,555] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-28 19:46:03,555] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-28 19:46:03,555] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 19:46:03,555] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0328 19:46:03.555844 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0328 19:46:03.556520 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0328 19:46:03.556764 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0328 19:46:03.558350 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 14, Global Step: 150100, Data Step: 300200, Loss: 2.9375, Token per second per gpu: 21900.694888623468
Epoch: 14, Global Step: 150200, Data Step: 300400, Loss: 3.15625, Token per second per gpu: 22215.63897995317
Epoch: 14, Global Step: 150300, Data Step: 300600, Loss: 3.09375, Token per second per gpu: 21968.732365716853
Epoch: 14, Global Step: 150400, Data Step: 300800, Loss: 2.984375, Token per second per gpu: 22475.119879010144
Epoch: 14, Global Step: 150500, Data Step: 301000, Loss: 3.125, Token per second per gpu: 22189.774525751716
Epoch: 14, Global Step: 150600, Data Step: 301200, Loss: 3.359375, Token per second per gpu: 22497.919678265996
Epoch: 14, Global Step: 150700, Data Step: 301400, Loss: 3.453125, Token per second per gpu: 22223.4791991103
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0328 20:02:40.490303 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 15, Global Step: 150800, Data Step: 301600, Loss: 3.15625, Token per second per gpu: 21833.028367105275
Epoch: 15, Global Step: 150900, Data Step: 301800, Loss: 3.171875, Token per second per gpu: 22369.799214662646
Epoch: 15, Global Step: 151000, Data Step: 302000, Loss: 3.171875, Token per second per gpu: 22628.823866799255
Epoch: 15, Global Step: 151100, Data Step: 302200, Loss: 3.296875, Token per second per gpu: 22373.26548252873
Epoch: 15, Global Step: 151200, Data Step: 302400, Loss: 3.109375, Token per second per gpu: 22716.198332862557
Epoch: 15, Global Step: 151300, Data Step: 302600, Loss: 3.015625, Token per second per gpu: 22220.751515675234
Epoch: 15, Global Step: 151400, Data Step: 302800, Loss: 2.984375, Token per second per gpu: 22078.72025171403
Epoch: 15, Global Step: 151500, Data Step: 303000, Loss: 2.671875, Token per second per gpu: 22441.31001506048
Epoch: 15, Global Step: 151600, Data Step: 303200, Loss: 3.5625, Token per second per gpu: 22010.462994768728
Epoch: 15, Global Step: 151700, Data Step: 303400, Loss: 2.984375, Token per second per gpu: 22413.819309658338
Epoch: 15, Global Step: 151800, Data Step: 303600, Loss: 3.3125, Token per second per gpu: 22021.491606295785
Epoch: 15, Global Step: 151900, Data Step: 303800, Loss: 3.0625, Token per second per gpu: 22304.871736368787
Epoch: 15, Global Step: 152000, Data Step: 304000, Loss: 2.84375, Token per second per gpu: 22468.995542448432
Epoch: 15, Global Step: 152100, Data Step: 304200, Loss: 2.921875, Token per second per gpu: 22198.583182014816
Epoch: 15, Global Step: 152200, Data Step: 304400, Loss: 3.265625, Token per second per gpu: 22297.859101080037
Epoch: 15, Global Step: 152300, Data Step: 304600, Loss: 2.84375, Token per second per gpu: 22284.164903552944
Epoch: 15, Global Step: 152400, Data Step: 304800, Loss: 2.875, Token per second per gpu: 22144.171238896062
Epoch: 15, Global Step: 152500, Data Step: 305000, Loss: 3.15625, Token per second per gpu: 22333.658848682633
Epoch: 15, Global Step: 152600, Data Step: 305200, Loss: 3.109375, Token per second per gpu: 22186.70416021961
Epoch: 15, Global Step: 152700, Data Step: 305400, Loss: 3.0625, Token per second per gpu: 22265.86651866497
Epoch: 15, Global Step: 152800, Data Step: 305600, Loss: 3.40625, Token per second per gpu: 21895.5566623977
Epoch: 15, Global Step: 152900, Data Step: 305800, Loss: 3.296875, Token per second per gpu: 22553.56735156956
Epoch: 15, Global Step: 153000, Data Step: 306000, Loss: 3.046875, Token per second per gpu: 22117.393877268634
Epoch: 15, Global Step: 153100, Data Step: 306200, Loss: 2.890625, Token per second per gpu: 22128.62588599651
Epoch: 15, Global Step: 153200, Data Step: 306400, Loss: 3.046875, Token per second per gpu: 22363.637031134516
Epoch: 15, Global Step: 153300, Data Step: 306600, Loss: 2.625, Token per second per gpu: 22174.898702203274
Epoch: 15, Global Step: 153400, Data Step: 306800, Loss: 3.328125, Token per second per gpu: 22274.743671396507
Epoch: 15, Global Step: 153500, Data Step: 307000, Loss: 3.046875, Token per second per gpu: 22253.137245213748
Epoch: 15, Global Step: 153600, Data Step: 307200, Loss: 2.578125, Token per second per gpu: 22091.872655343785
Epoch: 15, Global Step: 153700, Data Step: 307400, Loss: 3.078125, Token per second per gpu: 22031.01961635995
Epoch: 15, Global Step: 153800, Data Step: 307600, Loss: 3.203125, Token per second per gpu: 22260.528337645024
Epoch: 15, Global Step: 153900, Data Step: 307800, Loss: 2.765625, Token per second per gpu: 22437.255448248015
Epoch: 15, Global Step: 154000, Data Step: 308000, Loss: 2.96875, Token per second per gpu: 22376.56225287763
Epoch: 15, Global Step: 154100, Data Step: 308200, Loss: 3.15625, Token per second per gpu: 21946.634419246773
Epoch: 15, Global Step: 154200, Data Step: 308400, Loss: 2.859375, Token per second per gpu: 22151.590082733328
Epoch: 15, Global Step: 154300, Data Step: 308600, Loss: 2.71875, Token per second per gpu: 22340.84748019316
Epoch: 15, Global Step: 154400, Data Step: 308800, Loss: 3.375, Token per second per gpu: 22363.761323822153
Epoch: 15, Global Step: 154500, Data Step: 309000, Loss: 2.703125, Token per second per gpu: 22042.16239729564
Epoch: 15, Global Step: 154600, Data Step: 309200, Loss: 3.203125, Token per second per gpu: 22152.50252290242
Epoch: 15, Global Step: 154700, Data Step: 309400, Loss: 3.078125, Token per second per gpu: 22037.427639129204
Epoch: 15, Global Step: 154800, Data Step: 309600, Loss: 3.078125, Token per second per gpu: 22021.006694788848
Epoch: 15, Global Step: 154900, Data Step: 309800, Loss: 2.828125, Token per second per gpu: 22035.71894025125
Epoch: 15, Global Step: 155000, Data Step: 310000, Loss: 3.203125, Token per second per gpu: 22095.59022753877
Epoch: 15, Global Step: 155100, Data Step: 310200, Loss: 3.015625, Token per second per gpu: 21399.422935318173
Epoch: 15, Global Step: 155200, Data Step: 310400, Loss: 3.1875, Token per second per gpu: 22402.892325080524
Epoch: 15, Global Step: 155300, Data Step: 310600, Loss: 2.890625, Token per second per gpu: 22018.363441494595
Epoch: 15, Global Step: 155400, Data Step: 310800, Loss: 3.359375, Token per second per gpu: 22240.83053223585
Epoch: 15, Global Step: 155500, Data Step: 311000, Loss: 3.21875, Token per second per gpu: 22128.870734945784
Epoch: 15, Global Step: 155600, Data Step: 311200, Loss: 3.296875, Token per second per gpu: 22241.38189124928
Epoch: 15, Global Step: 155700, Data Step: 311400, Loss: 3.265625, Token per second per gpu: 22113.497117540024
Epoch: 15, Global Step: 155800, Data Step: 311600, Loss: 3.109375, Token per second per gpu: 22367.19966976168
Epoch: 15, Global Step: 155900, Data Step: 311800, Loss: 3.09375, Token per second per gpu: 22321.864242165433
Epoch: 15, Global Step: 156000, Data Step: 312000, Loss: 3.1875, Token per second per gpu: 22676.52717384041
Epoch: 15, Global Step: 156100, Data Step: 312200, Loss: 3.125, Token per second per gpu: 22227.451548407564
Epoch: 15, Global Step: 156200, Data Step: 312400, Loss: 3.21875, Token per second per gpu: 22281.143120309465
Epoch: 15, Global Step: 156300, Data Step: 312600, Loss: 2.953125, Token per second per gpu: 21990.547181491424
Epoch: 15, Global Step: 156400, Data Step: 312800, Loss: 3.28125, Token per second per gpu: 22380.108859568158
Epoch: 15, Global Step: 156500, Data Step: 313000, Loss: 3.171875, Token per second per gpu: 22340.354062896055
Epoch: 15, Global Step: 156600, Data Step: 313200, Loss: 3.15625, Token per second per gpu: 22026.163129577963
Epoch: 15, Global Step: 156700, Data Step: 313400, Loss: 3.046875, Token per second per gpu: 22174.955895922787
Epoch: 15, Global Step: 156800, Data Step: 313600, Loss: 3.171875, Token per second per gpu: 22428.18477106565
Epoch: 15, Global Step: 156900, Data Step: 313800, Loss: 3.359375, Token per second per gpu: 22041.527160537582
Epoch: 15, Global Step: 157000, Data Step: 314000, Loss: 3.125, Token per second per gpu: 22604.38803775476
Epoch: 15, Global Step: 157100, Data Step: 314200, Loss: 3.109375, Token per second per gpu: 22035.992328736935
Epoch: 15, Global Step: 157200, Data Step: 314400, Loss: 2.921875, Token per second per gpu: 22030.8110808353
Epoch: 15, Global Step: 157300, Data Step: 314600, Loss: 2.46875, Token per second per gpu: 21898.853766006087
Epoch: 15, Global Step: 157400, Data Step: 314800, Loss: 3.1875, Token per second per gpu: 20798.149084568086
Epoch: 15, Global Step: 157500, Data Step: 315000, Loss: 3.234375, Token per second per gpu: 22294.566468884663
Epoch: 15, Global Step: 157600, Data Step: 315200, Loss: 2.984375, Token per second per gpu: 22148.1606039856
Epoch: 15, Global Step: 157700, Data Step: 315400, Loss: 2.71875, Token per second per gpu: 22074.932471973276
Epoch: 15, Global Step: 157800, Data Step: 315600, Loss: 3.34375, Token per second per gpu: 22258.860337451715
Epoch: 15, Global Step: 157900, Data Step: 315800, Loss: 3.0625, Token per second per gpu: 22776.96461060903
Epoch: 15, Global Step: 158000, Data Step: 316000, Loss: 3.46875, Token per second per gpu: 22357.42721990381
Epoch: 15, Global Step: 158100, Data Step: 316200, Loss: 3.03125, Token per second per gpu: 21879.599898412947
Epoch: 15, Global Step: 158200, Data Step: 316400, Loss: 2.953125, Token per second per gpu: 22137.485406615677
Epoch: 15, Global Step: 158300, Data Step: 316600, Loss: 3.171875, Token per second per gpu: 22085.30033012011
Epoch: 15, Global Step: 158400, Data Step: 316800, Loss: 3.3125, Token per second per gpu: 22418.290237493715
Epoch: 15, Global Step: 158500, Data Step: 317000, Loss: 2.828125, Token per second per gpu: 22258.479305200857
Epoch: 15, Global Step: 158600, Data Step: 317200, Loss: 3.21875, Token per second per gpu: 22414.22327169822
Epoch: 15, Global Step: 158700, Data Step: 317400, Loss: 2.984375, Token per second per gpu: 22043.981233469094
Epoch: 15, Global Step: 158800, Data Step: 317600, Loss: 3.078125, Token per second per gpu: 22440.68187187837
Epoch: 15, Global Step: 158900, Data Step: 317800, Loss: 2.59375, Token per second per gpu: 20822.869080414726
Epoch: 15, Global Step: 159000, Data Step: 318000, Loss: 3.125, Token per second per gpu: 22300.685413286217
Epoch: 15, Global Step: 159100, Data Step: 318200, Loss: 3.28125, Token per second per gpu: 22009.022893765097
Epoch: 15, Global Step: 159200, Data Step: 318400, Loss: 2.921875, Token per second per gpu: 22101.939626329135
Epoch: 15, Global Step: 159300, Data Step: 318600, Loss: 3.234375, Token per second per gpu: 21037.734552884504
Epoch: 15, Global Step: 159400, Data Step: 318800, Loss: 3.03125, Token per second per gpu: 22229.088377968164
Epoch: 15, Global Step: 159500, Data Step: 319000, Loss: 3.234375, Token per second per gpu: 22205.911336161833
Epoch: 15, Global Step: 159600, Data Step: 319200, Loss: 3.25, Token per second per gpu: 22251.191263306235
Epoch: 15, Global Step: 159700, Data Step: 319400, Loss: 2.859375, Token per second per gpu: 22349.042996735807
Epoch: 15, Global Step: 159800, Data Step: 319600, Loss: 2.90625, Token per second per gpu: 22275.50973849389
Epoch: 15, Global Step: 159900, Data Step: 319800, Loss: 2.890625, Token per second per gpu: 22150.385513597026
Epoch: 15, Global Step: 160000, Data Step: 320000, Loss: 3.28125, Token per second per gpu: 21873.02238984504
I0328 23:22:30.460390 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0328 23:22:30.462216 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-28 23:22:30,471] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-28 23:22:30,487] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-28 23:22:30,493] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-28 23:22:31,545] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-28 23:22:31,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-28 23:22:31,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-28 23:22:31,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-28 23:22:31,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-28 23:22:31,608] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-28 23:22:31,609] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-28 23:22:32,287] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-28 23:22:32,288] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-28 23:22:32,288] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 23:22:33,424] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-28 23:22:33,425] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-28 23:22:33,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 23:22:33,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-28 23:22:33,781] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-28 23:22:33,781] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 23:22:33,794] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-28 23:22:33,794] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-28 23:22:33,794] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 23:22:33,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-28 23:22:33,808] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-28 23:22:33,808] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-28 23:22:33,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-28 23:22:33,829] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-28 23:22:33,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0328 23:22:33.830331 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0328 23:22:33.831157 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0328 23:22:33.831442 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0328 23:22:33.832810 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 15, Global Step: 160100, Data Step: 320200, Loss: 3.125, Token per second per gpu: 21764.421804531805
Epoch: 15, Global Step: 160200, Data Step: 320400, Loss: 2.96875, Token per second per gpu: 22285.67926840543
Epoch: 15, Global Step: 160300, Data Step: 320600, Loss: 3.15625, Token per second per gpu: 22330.65657068315
Epoch: 15, Global Step: 160400, Data Step: 320800, Loss: 3.375, Token per second per gpu: 22240.979303109158
Epoch: 15, Global Step: 160500, Data Step: 321000, Loss: 3.109375, Token per second per gpu: 22305.16185262471
Epoch: 15, Global Step: 160600, Data Step: 321200, Loss: 3.109375, Token per second per gpu: 22139.641583453573
Epoch: 15, Global Step: 160700, Data Step: 321400, Loss: 2.765625, Token per second per gpu: 22081.513118058985
Epoch: 15, Global Step: 160800, Data Step: 321600, Loss: 3.171875, Token per second per gpu: 22324.448803377425
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0328 23:40:20.744632 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 16, Global Step: 160900, Data Step: 321800, Loss: 3.25, Token per second per gpu: 21763.4111089164
Epoch: 16, Global Step: 161000, Data Step: 322000, Loss: 3.078125, Token per second per gpu: 22309.37799788842
Epoch: 16, Global Step: 161100, Data Step: 322200, Loss: 3.25, Token per second per gpu: 22668.20044696395
Epoch: 16, Global Step: 161200, Data Step: 322400, Loss: 2.75, Token per second per gpu: 22458.273705208816
Epoch: 16, Global Step: 161300, Data Step: 322600, Loss: 3.328125, Token per second per gpu: 21815.070682672005
Epoch: 16, Global Step: 161400, Data Step: 322800, Loss: 3.09375, Token per second per gpu: 22020.00012698382
Epoch: 16, Global Step: 161500, Data Step: 323000, Loss: 3.296875, Token per second per gpu: 22160.437229499923
Epoch: 16, Global Step: 161600, Data Step: 323200, Loss: 3.078125, Token per second per gpu: 22510.335231354187
Epoch: 16, Global Step: 161700, Data Step: 323400, Loss: 3.171875, Token per second per gpu: 22123.67129146305
Epoch: 16, Global Step: 161800, Data Step: 323600, Loss: 2.921875, Token per second per gpu: 21922.408503156534
Epoch: 16, Global Step: 161900, Data Step: 323800, Loss: 3.328125, Token per second per gpu: 22256.126955793166
Epoch: 16, Global Step: 162000, Data Step: 324000, Loss: 3.15625, Token per second per gpu: 21931.97148236232
Epoch: 16, Global Step: 162100, Data Step: 324200, Loss: 3.328125, Token per second per gpu: 22219.16454861857
Epoch: 16, Global Step: 162200, Data Step: 324400, Loss: 3.125, Token per second per gpu: 22220.411067272056
Epoch: 16, Global Step: 162300, Data Step: 324600, Loss: 3.046875, Token per second per gpu: 22206.259913092377
Epoch: 16, Global Step: 162400, Data Step: 324800, Loss: 2.796875, Token per second per gpu: 22189.471873492086
Epoch: 16, Global Step: 162500, Data Step: 325000, Loss: 3.21875, Token per second per gpu: 22239.33744644996
Epoch: 16, Global Step: 162600, Data Step: 325200, Loss: 3.140625, Token per second per gpu: 22321.75415036747
Epoch: 16, Global Step: 162700, Data Step: 325400, Loss: 2.734375, Token per second per gpu: 22197.146545058687
Epoch: 16, Global Step: 162800, Data Step: 325600, Loss: 3.078125, Token per second per gpu: 22197.647933441873
Epoch: 16, Global Step: 162900, Data Step: 325800, Loss: 3.03125, Token per second per gpu: 21993.43663287563
Epoch: 16, Global Step: 163000, Data Step: 326000, Loss: 3.078125, Token per second per gpu: 22368.00686030484
Epoch: 16, Global Step: 163100, Data Step: 326200, Loss: 3.53125, Token per second per gpu: 22085.706387563612
Epoch: 16, Global Step: 163200, Data Step: 326400, Loss: 3.140625, Token per second per gpu: 22364.890910406993
Epoch: 16, Global Step: 163300, Data Step: 326600, Loss: 2.78125, Token per second per gpu: 22023.573006000155
Epoch: 16, Global Step: 163400, Data Step: 326800, Loss: 3.390625, Token per second per gpu: 22353.759271337793
Epoch: 16, Global Step: 163500, Data Step: 327000, Loss: 2.921875, Token per second per gpu: 22220.123028847218
Epoch: 16, Global Step: 163600, Data Step: 327200, Loss: 3.546875, Token per second per gpu: 22453.284767185283
Epoch: 16, Global Step: 163700, Data Step: 327400, Loss: 3.40625, Token per second per gpu: 22438.071787720633
Epoch: 16, Global Step: 163800, Data Step: 327600, Loss: 3.171875, Token per second per gpu: 22311.35939133342
Epoch: 16, Global Step: 163900, Data Step: 327800, Loss: 3.15625, Token per second per gpu: 22405.999922270094
Epoch: 16, Global Step: 164000, Data Step: 328000, Loss: 3.125, Token per second per gpu: 22275.221625361726
Epoch: 16, Global Step: 164100, Data Step: 328200, Loss: 3.140625, Token per second per gpu: 22095.50329196525
Epoch: 16, Global Step: 164200, Data Step: 328400, Loss: 3.0, Token per second per gpu: 22004.14038522904
Epoch: 16, Global Step: 164300, Data Step: 328600, Loss: 3.34375, Token per second per gpu: 21997.169576034827
Epoch: 16, Global Step: 164400, Data Step: 328800, Loss: 2.921875, Token per second per gpu: 21740.17862598813
Epoch: 16, Global Step: 164500, Data Step: 329000, Loss: 3.140625, Token per second per gpu: 22212.82797359505
Epoch: 16, Global Step: 164600, Data Step: 329200, Loss: 3.25, Token per second per gpu: 22218.7205474261
Epoch: 16, Global Step: 164700, Data Step: 329400, Loss: 2.828125, Token per second per gpu: 22543.802600810322
Epoch: 16, Global Step: 164800, Data Step: 329600, Loss: 2.921875, Token per second per gpu: 22042.06176408571
Epoch: 16, Global Step: 164900, Data Step: 329800, Loss: 3.1875, Token per second per gpu: 22288.178430724856
Epoch: 16, Global Step: 165000, Data Step: 330000, Loss: 2.921875, Token per second per gpu: 22224.538762318407
Epoch: 16, Global Step: 165100, Data Step: 330200, Loss: 2.9375, Token per second per gpu: 22206.172226941355
Epoch: 16, Global Step: 165200, Data Step: 330400, Loss: 2.9375, Token per second per gpu: 22356.983923964846
Epoch: 16, Global Step: 165300, Data Step: 330600, Loss: 3.296875, Token per second per gpu: 22159.71763479866
Epoch: 16, Global Step: 165400, Data Step: 330800, Loss: 2.875, Token per second per gpu: 22065.304821569367
Epoch: 16, Global Step: 165500, Data Step: 331000, Loss: 3.09375, Token per second per gpu: 21882.176482409224
Epoch: 16, Global Step: 165600, Data Step: 331200, Loss: 3.03125, Token per second per gpu: 22256.28150825987
Epoch: 16, Global Step: 165700, Data Step: 331400, Loss: 2.75, Token per second per gpu: 22243.825825144395
Epoch: 16, Global Step: 165800, Data Step: 331600, Loss: 3.078125, Token per second per gpu: 22348.329169645927
Epoch: 16, Global Step: 165900, Data Step: 331800, Loss: 2.703125, Token per second per gpu: 22144.72341812978
Epoch: 16, Global Step: 166000, Data Step: 332000, Loss: 3.421875, Token per second per gpu: 22309.426369612866
Epoch: 16, Global Step: 166100, Data Step: 332200, Loss: 3.296875, Token per second per gpu: 22238.959006601777
Epoch: 16, Global Step: 166200, Data Step: 332400, Loss: 3.375, Token per second per gpu: 21350.39597225857
Epoch: 16, Global Step: 166300, Data Step: 332600, Loss: 3.1875, Token per second per gpu: 22216.569452709242
Epoch: 16, Global Step: 166400, Data Step: 332800, Loss: 2.921875, Token per second per gpu: 22342.36017843739
Epoch: 16, Global Step: 166500, Data Step: 333000, Loss: 3.109375, Token per second per gpu: 22227.226803294205
Epoch: 16, Global Step: 166600, Data Step: 333200, Loss: 3.0625, Token per second per gpu: 22242.345483784036
Epoch: 16, Global Step: 166700, Data Step: 333400, Loss: 2.921875, Token per second per gpu: 22141.88994507326
Epoch: 16, Global Step: 166800, Data Step: 333600, Loss: 2.671875, Token per second per gpu: 22238.656566964168
Epoch: 16, Global Step: 166900, Data Step: 333800, Loss: 2.703125, Token per second per gpu: 22254.494341991318
Epoch: 16, Global Step: 167000, Data Step: 334000, Loss: 3.203125, Token per second per gpu: 22170.595527770613
Epoch: 16, Global Step: 167100, Data Step: 334200, Loss: 3.15625, Token per second per gpu: 21887.668412811465
Epoch: 16, Global Step: 167200, Data Step: 334400, Loss: 3.296875, Token per second per gpu: 22536.475807829767
Epoch: 16, Global Step: 167300, Data Step: 334600, Loss: 3.265625, Token per second per gpu: 21517.017676403302
Epoch: 16, Global Step: 167400, Data Step: 334800, Loss: 2.921875, Token per second per gpu: 22365.854674896585
Epoch: 16, Global Step: 167500, Data Step: 335000, Loss: 3.046875, Token per second per gpu: 22185.541445717463
Epoch: 16, Global Step: 167600, Data Step: 335200, Loss: 2.8125, Token per second per gpu: 22033.898091144234
Epoch: 16, Global Step: 167700, Data Step: 335400, Loss: 3.40625, Token per second per gpu: 22217.316322472772
Epoch: 16, Global Step: 167800, Data Step: 335600, Loss: 3.0, Token per second per gpu: 22733.60927777766
Epoch: 16, Global Step: 167900, Data Step: 335800, Loss: 3.046875, Token per second per gpu: 22223.034126015213
Epoch: 16, Global Step: 168000, Data Step: 336000, Loss: 3.015625, Token per second per gpu: 22391.673524207275
Epoch: 16, Global Step: 168100, Data Step: 336200, Loss: 3.171875, Token per second per gpu: 22063.08637195219
Epoch: 16, Global Step: 168200, Data Step: 336400, Loss: 3.421875, Token per second per gpu: 22020.32679481576
Epoch: 16, Global Step: 168300, Data Step: 336600, Loss: 3.140625, Token per second per gpu: 21905.899961624582
Epoch: 16, Global Step: 168400, Data Step: 336800, Loss: 3.25, Token per second per gpu: 22378.120460602342
Epoch: 16, Global Step: 168500, Data Step: 337000, Loss: 3.0625, Token per second per gpu: 22099.46389954746
Epoch: 16, Global Step: 168600, Data Step: 337200, Loss: 3.234375, Token per second per gpu: 22187.155562952663
Epoch: 16, Global Step: 168700, Data Step: 337400, Loss: 3.015625, Token per second per gpu: 21967.44013752136
Epoch: 16, Global Step: 168800, Data Step: 337600, Loss: 3.125, Token per second per gpu: 20947.04937326419
Epoch: 16, Global Step: 168900, Data Step: 337800, Loss: 3.125, Token per second per gpu: 21837.915290178855
Epoch: 16, Global Step: 169000, Data Step: 338000, Loss: 3.078125, Token per second per gpu: 22422.767028318285
Epoch: 16, Global Step: 169100, Data Step: 338200, Loss: 3.265625, Token per second per gpu: 22219.54215159222
Epoch: 16, Global Step: 169200, Data Step: 338400, Loss: 3.0, Token per second per gpu: 22239.678147500035
Epoch: 16, Global Step: 169300, Data Step: 338600, Loss: 3.1875, Token per second per gpu: 20777.493274832963
Epoch: 16, Global Step: 169400, Data Step: 338800, Loss: 3.453125, Token per second per gpu: 22580.413465514055
Epoch: 16, Global Step: 169500, Data Step: 339000, Loss: 3.03125, Token per second per gpu: 21838.712881078256
Epoch: 16, Global Step: 169600, Data Step: 339200, Loss: 3.203125, Token per second per gpu: 22495.933542284223
Epoch: 16, Global Step: 169700, Data Step: 339400, Loss: 2.96875, Token per second per gpu: 22043.309527982103
Epoch: 16, Global Step: 169800, Data Step: 339600, Loss: 3.015625, Token per second per gpu: 22080.33512753823
Epoch: 16, Global Step: 169900, Data Step: 339800, Loss: 3.1875, Token per second per gpu: 22446.262809113297
Epoch: 16, Global Step: 170000, Data Step: 340000, Loss: 3.125, Token per second per gpu: 22174.342086350764
I0329 02:59:08.654240 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0329 02:59:08.654637 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-29 02:59:08,655] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-29 02:59:08,658] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-29 02:59:08,659] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-29 02:59:09,284] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-29 02:59:09,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-29 02:59:09,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-29 02:59:09,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-29 02:59:09,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-29 02:59:09,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-29 02:59:09,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-29 02:59:10,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-29 02:59:10,936] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-29 02:59:10,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 02:59:11,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-29 02:59:11,027] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-29 02:59:11,027] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 02:59:11,056] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-29 02:59:11,057] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-29 02:59:11,057] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 02:59:11,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-29 02:59:11,063] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-29 02:59:11,063] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 02:59:11,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-29 02:59:11,066] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-29 02:59:11,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 02:59:11,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-29 02:59:11,086] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-29 02:59:11,086] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0329 02:59:11.087251 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0329 02:59:11.088096 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0329 02:59:11.088450 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0329 02:59:11.090291 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 16, Global Step: 170100, Data Step: 340200, Loss: 3.578125, Token per second per gpu: 21879.590664578212
Epoch: 16, Global Step: 170200, Data Step: 340400, Loss: 3.21875, Token per second per gpu: 22102.373755494515
Epoch: 16, Global Step: 170300, Data Step: 340600, Loss: 3.4375, Token per second per gpu: 21881.257359290932
Epoch: 16, Global Step: 170400, Data Step: 340800, Loss: 3.046875, Token per second per gpu: 22638.173966534065
Epoch: 16, Global Step: 170500, Data Step: 341000, Loss: 3.015625, Token per second per gpu: 21828.383886942996
Epoch: 16, Global Step: 170600, Data Step: 341200, Loss: 2.71875, Token per second per gpu: 22112.639093363156
Epoch: 16, Global Step: 170700, Data Step: 341400, Loss: 3.15625, Token per second per gpu: 22114.551482244035
Epoch: 16, Global Step: 170800, Data Step: 341600, Loss: 3.046875, Token per second per gpu: 22286.903861101255
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0329 03:18:08.496856 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 17, Global Step: 170900, Data Step: 341800, Loss: 3.296875, Token per second per gpu: 21823.447844003487
Epoch: 17, Global Step: 171000, Data Step: 342000, Loss: 3.125, Token per second per gpu: 22298.444739019797
Epoch: 17, Global Step: 171100, Data Step: 342200, Loss: 2.96875, Token per second per gpu: 22236.439227794206
Epoch: 17, Global Step: 171200, Data Step: 342400, Loss: 3.140625, Token per second per gpu: 22361.08796947938
Epoch: 17, Global Step: 171300, Data Step: 342600, Loss: 3.265625, Token per second per gpu: 22068.624744896973
Epoch: 17, Global Step: 171400, Data Step: 342800, Loss: 3.375, Token per second per gpu: 22398.394227601093
Epoch: 17, Global Step: 171500, Data Step: 343000, Loss: 3.140625, Token per second per gpu: 21885.03351870336
Epoch: 17, Global Step: 171600, Data Step: 343200, Loss: 3.359375, Token per second per gpu: 22208.24487518949
Epoch: 17, Global Step: 171700, Data Step: 343400, Loss: 3.109375, Token per second per gpu: 22037.690777537147
Epoch: 17, Global Step: 171800, Data Step: 343600, Loss: 3.21875, Token per second per gpu: 22613.191960308264
Epoch: 17, Global Step: 171900, Data Step: 343800, Loss: 2.859375, Token per second per gpu: 22442.900937573362
Epoch: 17, Global Step: 172000, Data Step: 344000, Loss: 3.078125, Token per second per gpu: 22066.55126801362
Epoch: 17, Global Step: 172100, Data Step: 344200, Loss: 2.953125, Token per second per gpu: 22333.347758053013
Epoch: 17, Global Step: 172200, Data Step: 344400, Loss: 3.21875, Token per second per gpu: 22389.83599676301
Epoch: 17, Global Step: 172300, Data Step: 344600, Loss: 3.140625, Token per second per gpu: 22027.1793385696
Epoch: 17, Global Step: 172400, Data Step: 344800, Loss: 2.953125, Token per second per gpu: 22337.455728010267
Epoch: 17, Global Step: 172500, Data Step: 345000, Loss: 2.96875, Token per second per gpu: 22262.289804557127
Epoch: 17, Global Step: 172600, Data Step: 345200, Loss: 2.9375, Token per second per gpu: 22404.834970966767
Epoch: 17, Global Step: 172700, Data Step: 345400, Loss: 3.03125, Token per second per gpu: 22707.976618881374
Epoch: 17, Global Step: 172800, Data Step: 345600, Loss: 3.203125, Token per second per gpu: 22374.419859145015
Epoch: 17, Global Step: 172900, Data Step: 345800, Loss: 3.265625, Token per second per gpu: 22510.092438795913
Epoch: 17, Global Step: 173000, Data Step: 346000, Loss: 2.921875, Token per second per gpu: 22593.736640398023
Epoch: 17, Global Step: 173100, Data Step: 346200, Loss: 3.078125, Token per second per gpu: 22155.75901271982
Epoch: 17, Global Step: 173200, Data Step: 346400, Loss: 3.078125, Token per second per gpu: 22096.209142910455
Epoch: 17, Global Step: 173300, Data Step: 346600, Loss: 3.140625, Token per second per gpu: 22396.515276252198
Epoch: 17, Global Step: 173400, Data Step: 346800, Loss: 3.015625, Token per second per gpu: 22364.696875577964
Epoch: 17, Global Step: 173500, Data Step: 347000, Loss: 3.109375, Token per second per gpu: 22552.387173662773
Epoch: 17, Global Step: 173600, Data Step: 347200, Loss: 2.984375, Token per second per gpu: 22018.803805856023
Epoch: 17, Global Step: 173700, Data Step: 347400, Loss: 2.96875, Token per second per gpu: 22346.287877497838
Epoch: 17, Global Step: 173800, Data Step: 347600, Loss: 2.859375, Token per second per gpu: 22475.163243199855
Epoch: 17, Global Step: 173900, Data Step: 347800, Loss: 3.171875, Token per second per gpu: 22401.345579651585
Epoch: 17, Global Step: 174000, Data Step: 348000, Loss: 3.28125, Token per second per gpu: 22209.904766972377
Epoch: 17, Global Step: 174100, Data Step: 348200, Loss: 3.1875, Token per second per gpu: 22427.17365564583
Epoch: 17, Global Step: 174200, Data Step: 348400, Loss: 2.984375, Token per second per gpu: 22574.368852212334
Epoch: 17, Global Step: 174300, Data Step: 348600, Loss: 3.15625, Token per second per gpu: 22662.814317001674
Epoch: 17, Global Step: 174400, Data Step: 348800, Loss: 2.96875, Token per second per gpu: 22420.1586544752
Epoch: 17, Global Step: 174500, Data Step: 349000, Loss: 3.515625, Token per second per gpu: 21990.086289584982
Epoch: 17, Global Step: 174600, Data Step: 349200, Loss: 3.125, Token per second per gpu: 22065.032962344932
Epoch: 17, Global Step: 174700, Data Step: 349400, Loss: 3.421875, Token per second per gpu: 22037.95142946159
Epoch: 17, Global Step: 174800, Data Step: 349600, Loss: 3.0, Token per second per gpu: 22764.18150934404
Epoch: 17, Global Step: 174900, Data Step: 349800, Loss: 3.296875, Token per second per gpu: 21981.02334002647
Epoch: 17, Global Step: 175000, Data Step: 350000, Loss: 2.828125, Token per second per gpu: 22078.06551419619
Epoch: 17, Global Step: 175100, Data Step: 350200, Loss: 2.546875, Token per second per gpu: 22319.31945643291
Epoch: 17, Global Step: 175200, Data Step: 350400, Loss: 2.859375, Token per second per gpu: 22536.28769779277
Epoch: 17, Global Step: 175300, Data Step: 350600, Loss: 2.96875, Token per second per gpu: 22247.49601112798
Epoch: 17, Global Step: 175400, Data Step: 350800, Loss: 3.1875, Token per second per gpu: 22212.434138203316
Epoch: 17, Global Step: 175500, Data Step: 351000, Loss: 3.265625, Token per second per gpu: 22250.97562881212
Epoch: 17, Global Step: 175600, Data Step: 351200, Loss: 3.109375, Token per second per gpu: 22452.986611527223
Epoch: 17, Global Step: 175700, Data Step: 351400, Loss: 3.0625, Token per second per gpu: 22493.51536604154
Epoch: 17, Global Step: 175800, Data Step: 351600, Loss: 3.078125, Token per second per gpu: 22007.78357927196
Epoch: 17, Global Step: 175900, Data Step: 351800, Loss: 3.03125, Token per second per gpu: 22127.06392588916
Epoch: 17, Global Step: 176000, Data Step: 352000, Loss: 2.921875, Token per second per gpu: 22145.462988478357
Epoch: 17, Global Step: 176100, Data Step: 352200, Loss: 3.140625, Token per second per gpu: 22468.273238327907
Epoch: 17, Global Step: 176200, Data Step: 352400, Loss: 3.109375, Token per second per gpu: 22467.926751109244
Epoch: 17, Global Step: 176300, Data Step: 352600, Loss: 2.921875, Token per second per gpu: 22143.50088273861
Epoch: 17, Global Step: 176400, Data Step: 352800, Loss: 2.859375, Token per second per gpu: 22204.467015951075
Epoch: 17, Global Step: 176500, Data Step: 353000, Loss: 3.375, Token per second per gpu: 22499.63438732476
Epoch: 17, Global Step: 176600, Data Step: 353200, Loss: 3.453125, Token per second per gpu: 22145.20323804658
Epoch: 17, Global Step: 176700, Data Step: 353400, Loss: 2.921875, Token per second per gpu: 22096.994871424908
Epoch: 17, Global Step: 176800, Data Step: 353600, Loss: 3.046875, Token per second per gpu: 22078.770574287722
Epoch: 17, Global Step: 176900, Data Step: 353800, Loss: 3.140625, Token per second per gpu: 22726.59486096555
Epoch: 17, Global Step: 177000, Data Step: 354000, Loss: 3.3125, Token per second per gpu: 22512.774186046925
Epoch: 17, Global Step: 177100, Data Step: 354200, Loss: 3.265625, Token per second per gpu: 21571.183162942067
Epoch: 17, Global Step: 177200, Data Step: 354400, Loss: 3.03125, Token per second per gpu: 21159.008912677917
Epoch: 17, Global Step: 177300, Data Step: 354600, Loss: 2.8125, Token per second per gpu: 22338.193522032063
Epoch: 17, Global Step: 177400, Data Step: 354800, Loss: 3.046875, Token per second per gpu: 22488.09863163529
Epoch: 17, Global Step: 177500, Data Step: 355000, Loss: 3.28125, Token per second per gpu: 22521.45207979365
Epoch: 17, Global Step: 177600, Data Step: 355200, Loss: 3.171875, Token per second per gpu: 22176.397478619263
Epoch: 17, Global Step: 177700, Data Step: 355400, Loss: 3.0625, Token per second per gpu: 22135.332894678682
Epoch: 17, Global Step: 177800, Data Step: 355600, Loss: 3.21875, Token per second per gpu: 22249.911289322496
Epoch: 17, Global Step: 177900, Data Step: 355800, Loss: 3.03125, Token per second per gpu: 22437.878273176117
Epoch: 17, Global Step: 178000, Data Step: 356000, Loss: 3.171875, Token per second per gpu: 22397.457723327356
Epoch: 17, Global Step: 178100, Data Step: 356200, Loss: 3.265625, Token per second per gpu: 22506.016252417983
Epoch: 17, Global Step: 178200, Data Step: 356400, Loss: 3.234375, Token per second per gpu: 22228.588717924154
Epoch: 17, Global Step: 178300, Data Step: 356600, Loss: 3.375, Token per second per gpu: 22390.67777549872
Epoch: 17, Global Step: 178400, Data Step: 356800, Loss: 3.390625, Token per second per gpu: 22643.452122161405
Epoch: 17, Global Step: 178500, Data Step: 357000, Loss: 3.140625, Token per second per gpu: 22263.455735854095
Epoch: 17, Global Step: 178600, Data Step: 357200, Loss: 3.0, Token per second per gpu: 22244.629829359892
Epoch: 17, Global Step: 178700, Data Step: 357400, Loss: 2.96875, Token per second per gpu: 21084.667279634294
Epoch: 17, Global Step: 178800, Data Step: 357600, Loss: 3.0, Token per second per gpu: 22485.02819725326
Epoch: 17, Global Step: 178900, Data Step: 357800, Loss: 3.0, Token per second per gpu: 22309.30675916341
Epoch: 17, Global Step: 179000, Data Step: 358000, Loss: 3.046875, Token per second per gpu: 22137.732804770912
Epoch: 17, Global Step: 179100, Data Step: 358200, Loss: 3.078125, Token per second per gpu: 22356.811708254565
Epoch: 17, Global Step: 179200, Data Step: 358400, Loss: 2.765625, Token per second per gpu: 21255.44278749142
Epoch: 17, Global Step: 179300, Data Step: 358600, Loss: 3.1875, Token per second per gpu: 22142.007685883855
Epoch: 17, Global Step: 179400, Data Step: 358800, Loss: 3.1875, Token per second per gpu: 22090.89943246707
Epoch: 17, Global Step: 179500, Data Step: 359000, Loss: 2.921875, Token per second per gpu: 22238.332641823217
Epoch: 17, Global Step: 179600, Data Step: 359200, Loss: 2.84375, Token per second per gpu: 22171.288360708208
Epoch: 17, Global Step: 179700, Data Step: 359400, Loss: 2.75, Token per second per gpu: 22219.311803668883
Epoch: 17, Global Step: 179800, Data Step: 359600, Loss: 2.953125, Token per second per gpu: 22128.710812294794
Epoch: 17, Global Step: 179900, Data Step: 359800, Loss: 3.265625, Token per second per gpu: 22218.94168755452
Epoch: 17, Global Step: 180000, Data Step: 360000, Loss: 3.234375, Token per second per gpu: 22060.029772504517
I0329 06:35:00.354717 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0329 06:35:00.355121 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-29 06:35:00,355] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-29 06:35:00,359] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-29 06:35:00,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-29 06:35:00,907] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-29 06:35:00,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-29 06:35:00,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-29 06:35:00,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-29 06:35:00,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-29 06:35:00,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-29 06:35:00,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-29 06:35:02,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-29 06:35:02,682] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-29 06:35:02,682] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 06:35:02,756] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-29 06:35:02,758] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-29 06:35:02,758] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 06:35:02,821] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-29 06:35:02,821] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-29 06:35:02,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 06:35:02,822] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-29 06:35:02,822] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-29 06:35:02,822] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 06:35:02,853] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-29 06:35:02,853] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-29 06:35:02,853] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 06:35:02,889] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-29 06:35:02,895] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-29 06:35:02,896] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0329 06:35:02.896648 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0329 06:35:02.899708 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0329 06:35:02.900377 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0329 06:35:02.904086 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 17, Global Step: 180100, Data Step: 360200, Loss: 2.96875, Token per second per gpu: 21422.828213297777
Epoch: 17, Global Step: 180200, Data Step: 360400, Loss: 3.265625, Token per second per gpu: 22448.805629006463
Epoch: 17, Global Step: 180300, Data Step: 360600, Loss: 2.96875, Token per second per gpu: 22354.801755680557
Epoch: 17, Global Step: 180400, Data Step: 360800, Loss: 3.4375, Token per second per gpu: 22200.885843903583
Epoch: 17, Global Step: 180500, Data Step: 361000, Loss: 3.125, Token per second per gpu: 21882.183379691905
Epoch: 17, Global Step: 180600, Data Step: 361200, Loss: 3.21875, Token per second per gpu: 22562.03694409592
Epoch: 17, Global Step: 180700, Data Step: 361400, Loss: 3.3125, Token per second per gpu: 22141.819366125244
Epoch: 17, Global Step: 180800, Data Step: 361600, Loss: 3.3125, Token per second per gpu: 21892.826307399544
Epoch: 17, Global Step: 180900, Data Step: 361800, Loss: 3.109375, Token per second per gpu: 22209.05259749322
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0329 06:55:05.921201 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 18, Global Step: 181000, Data Step: 362000, Loss: 3.234375, Token per second per gpu: 21503.52229342488
Epoch: 18, Global Step: 181100, Data Step: 362200, Loss: 3.0625, Token per second per gpu: 22277.768854691614
Epoch: 18, Global Step: 181200, Data Step: 362400, Loss: 2.671875, Token per second per gpu: 22351.143973738304
Epoch: 18, Global Step: 181300, Data Step: 362600, Loss: 3.359375, Token per second per gpu: 21950.44630213262
Epoch: 18, Global Step: 181400, Data Step: 362800, Loss: 3.28125, Token per second per gpu: 21985.49739984031
Epoch: 18, Global Step: 181500, Data Step: 363000, Loss: 3.515625, Token per second per gpu: 22140.678314305904
Epoch: 18, Global Step: 181600, Data Step: 363200, Loss: 3.296875, Token per second per gpu: 22225.037218018515
Epoch: 18, Global Step: 181700, Data Step: 363400, Loss: 3.3125, Token per second per gpu: 21852.28875212898
Epoch: 18, Global Step: 181800, Data Step: 363600, Loss: 2.828125, Token per second per gpu: 21954.05936716737
Epoch: 18, Global Step: 181900, Data Step: 363800, Loss: 3.234375, Token per second per gpu: 22251.807203363755
Epoch: 18, Global Step: 182000, Data Step: 364000, Loss: 3.03125, Token per second per gpu: 21988.488950045714
Epoch: 18, Global Step: 182100, Data Step: 364200, Loss: 3.296875, Token per second per gpu: 22204.97812316384
Epoch: 18, Global Step: 182200, Data Step: 364400, Loss: 3.3125, Token per second per gpu: 22291.02297990782
Epoch: 18, Global Step: 182300, Data Step: 364600, Loss: 3.359375, Token per second per gpu: 22053.785195712946
Epoch: 18, Global Step: 182400, Data Step: 364800, Loss: 3.34375, Token per second per gpu: 22373.595008104523
Epoch: 18, Global Step: 182500, Data Step: 365000, Loss: 3.234375, Token per second per gpu: 21951.795372526165
Epoch: 18, Global Step: 182600, Data Step: 365200, Loss: 3.3125, Token per second per gpu: 22256.079922077675
Epoch: 18, Global Step: 182700, Data Step: 365400, Loss: 3.453125, Token per second per gpu: 22185.250113837745
Epoch: 18, Global Step: 182800, Data Step: 365600, Loss: 3.171875, Token per second per gpu: 22150.716020840642
Epoch: 18, Global Step: 182900, Data Step: 365800, Loss: 3.0625, Token per second per gpu: 22013.91286515516
Epoch: 18, Global Step: 183000, Data Step: 366000, Loss: 3.3125, Token per second per gpu: 21892.18088217638
Epoch: 18, Global Step: 183100, Data Step: 366200, Loss: 3.1875, Token per second per gpu: 22028.097666321904
Epoch: 18, Global Step: 183200, Data Step: 366400, Loss: 2.796875, Token per second per gpu: 22279.300882523723
Epoch: 18, Global Step: 183300, Data Step: 366600, Loss: 3.0625, Token per second per gpu: 22148.65157840344
Epoch: 18, Global Step: 183400, Data Step: 366800, Loss: 2.703125, Token per second per gpu: 21879.64063834337
Epoch: 18, Global Step: 183500, Data Step: 367000, Loss: 2.578125, Token per second per gpu: 22221.314429057602
Epoch: 18, Global Step: 183600, Data Step: 367200, Loss: 2.921875, Token per second per gpu: 22238.77443842623
Epoch: 18, Global Step: 183700, Data Step: 367400, Loss: 2.921875, Token per second per gpu: 22219.262841045882
Epoch: 18, Global Step: 183800, Data Step: 367600, Loss: 3.0625, Token per second per gpu: 22149.488681093855
Epoch: 18, Global Step: 183900, Data Step: 367800, Loss: 3.1875, Token per second per gpu: 22275.62943867036
Epoch: 18, Global Step: 184000, Data Step: 368000, Loss: 3.0625, Token per second per gpu: 22215.227109356376
Epoch: 18, Global Step: 184100, Data Step: 368200, Loss: 3.078125, Token per second per gpu: 22067.263695773265
Epoch: 18, Global Step: 184200, Data Step: 368400, Loss: 2.90625, Token per second per gpu: 22583.865853357533
Epoch: 18, Global Step: 184300, Data Step: 368600, Loss: 3.15625, Token per second per gpu: 22176.60193954096
Epoch: 18, Global Step: 184400, Data Step: 368800, Loss: 2.828125, Token per second per gpu: 22436.117748406257
Epoch: 18, Global Step: 184500, Data Step: 369000, Loss: 2.96875, Token per second per gpu: 22314.935360962852
Epoch: 18, Global Step: 184600, Data Step: 369200, Loss: 3.171875, Token per second per gpu: 22189.86041120316
Epoch: 18, Global Step: 184700, Data Step: 369400, Loss: 3.1875, Token per second per gpu: 22285.15325664384
Epoch: 18, Global Step: 184800, Data Step: 369600, Loss: 3.5, Token per second per gpu: 22200.325025846694
Epoch: 18, Global Step: 184900, Data Step: 369800, Loss: 3.359375, Token per second per gpu: 22203.88744671833
Epoch: 18, Global Step: 185000, Data Step: 370000, Loss: 3.328125, Token per second per gpu: 22115.856986910807
Epoch: 18, Global Step: 185100, Data Step: 370200, Loss: 3.125, Token per second per gpu: 22169.443330092854
Epoch: 18, Global Step: 185200, Data Step: 370400, Loss: 3.234375, Token per second per gpu: 22132.526155063304
Epoch: 18, Global Step: 185300, Data Step: 370600, Loss: 2.890625, Token per second per gpu: 22221.231611023093
Epoch: 18, Global Step: 185400, Data Step: 370800, Loss: 2.984375, Token per second per gpu: 22205.0833516392
Epoch: 18, Global Step: 185500, Data Step: 371000, Loss: 3.3125, Token per second per gpu: 22442.155293552125
Epoch: 18, Global Step: 185600, Data Step: 371200, Loss: 2.984375, Token per second per gpu: 22312.235668039604
Epoch: 18, Global Step: 185700, Data Step: 371400, Loss: 2.859375, Token per second per gpu: 22404.442816234834
Epoch: 18, Global Step: 185800, Data Step: 371600, Loss: 2.921875, Token per second per gpu: 22174.01356015336
Epoch: 18, Global Step: 185900, Data Step: 371800, Loss: 3.15625, Token per second per gpu: 21977.12617981298
Epoch: 18, Global Step: 186000, Data Step: 372000, Loss: 3.015625, Token per second per gpu: 22439.36629122579
Epoch: 18, Global Step: 186100, Data Step: 372200, Loss: 3.171875, Token per second per gpu: 22012.020930247818
Epoch: 18, Global Step: 186200, Data Step: 372400, Loss: 3.0, Token per second per gpu: 22208.91870800812
Epoch: 18, Global Step: 186300, Data Step: 372600, Loss: 3.140625, Token per second per gpu: 22278.674133338904
Epoch: 18, Global Step: 186400, Data Step: 372800, Loss: 2.921875, Token per second per gpu: 22131.35264969951
Epoch: 18, Global Step: 186500, Data Step: 373000, Loss: 3.375, Token per second per gpu: 22254.812751969694
Epoch: 18, Global Step: 186600, Data Step: 373200, Loss: 3.046875, Token per second per gpu: 22109.829081033688
Epoch: 18, Global Step: 186700, Data Step: 373400, Loss: 3.25, Token per second per gpu: 22277.03755272647
Epoch: 18, Global Step: 186800, Data Step: 373600, Loss: 2.875, Token per second per gpu: 22046.388212554957
Epoch: 18, Global Step: 186900, Data Step: 373800, Loss: 2.828125, Token per second per gpu: 22277.291448754233
Epoch: 18, Global Step: 187000, Data Step: 374000, Loss: 3.015625, Token per second per gpu: 21625.68603967889
Epoch: 18, Global Step: 187100, Data Step: 374200, Loss: 3.203125, Token per second per gpu: 22093.353602152216
Epoch: 18, Global Step: 187200, Data Step: 374400, Loss: 3.328125, Token per second per gpu: 21838.505285481224
Epoch: 18, Global Step: 187300, Data Step: 374600, Loss: 2.890625, Token per second per gpu: 21978.48768931494
Epoch: 18, Global Step: 187400, Data Step: 374800, Loss: 3.203125, Token per second per gpu: 22679.97874041285
Epoch: 18, Global Step: 187500, Data Step: 375000, Loss: 3.34375, Token per second per gpu: 22651.33718845999
Epoch: 18, Global Step: 187600, Data Step: 375200, Loss: 3.140625, Token per second per gpu: 22092.7234926368
Epoch: 18, Global Step: 187700, Data Step: 375400, Loss: 3.296875, Token per second per gpu: 22359.699299582844
Epoch: 18, Global Step: 187800, Data Step: 375600, Loss: 2.953125, Token per second per gpu: 22341.344679456895
Epoch: 18, Global Step: 187900, Data Step: 375800, Loss: 3.1875, Token per second per gpu: 22159.933048931645
Epoch: 18, Global Step: 188000, Data Step: 376000, Loss: 3.15625, Token per second per gpu: 22272.28739540769
Epoch: 18, Global Step: 188100, Data Step: 376200, Loss: 3.421875, Token per second per gpu: 22266.577878540775
Epoch: 18, Global Step: 188200, Data Step: 376400, Loss: 3.25, Token per second per gpu: 22283.33699252859
Epoch: 18, Global Step: 188300, Data Step: 376600, Loss: 3.15625, Token per second per gpu: 21557.344466185274
Epoch: 18, Global Step: 188400, Data Step: 376800, Loss: 3.390625, Token per second per gpu: 22240.11213574894
Epoch: 18, Global Step: 188500, Data Step: 377000, Loss: 3.03125, Token per second per gpu: 22459.09729559504
Epoch: 18, Global Step: 188600, Data Step: 377200, Loss: 3.28125, Token per second per gpu: 22274.232263652866
Epoch: 18, Global Step: 188700, Data Step: 377400, Loss: 2.96875, Token per second per gpu: 21512.434517131067
Epoch: 18, Global Step: 188800, Data Step: 377600, Loss: 2.84375, Token per second per gpu: 22032.013006822825
Epoch: 18, Global Step: 188900, Data Step: 377800, Loss: 3.03125, Token per second per gpu: 22215.481518643934
Epoch: 18, Global Step: 189000, Data Step: 378000, Loss: 3.15625, Token per second per gpu: 22609.473514417074
Epoch: 18, Global Step: 189100, Data Step: 378200, Loss: 2.953125, Token per second per gpu: 22658.771596098675
Epoch: 18, Global Step: 189200, Data Step: 378400, Loss: 2.9375, Token per second per gpu: 21496.775955782756
Epoch: 18, Global Step: 189300, Data Step: 378600, Loss: 3.25, Token per second per gpu: 22109.95926934578
Epoch: 18, Global Step: 189400, Data Step: 378800, Loss: 3.265625, Token per second per gpu: 21808.862727471664
Epoch: 18, Global Step: 189500, Data Step: 379000, Loss: 3.03125, Token per second per gpu: 21883.209022308616
Epoch: 18, Global Step: 189600, Data Step: 379200, Loss: 3.1875, Token per second per gpu: 22274.94165258911
Epoch: 18, Global Step: 189700, Data Step: 379400, Loss: 2.984375, Token per second per gpu: 22237.677123506393
Epoch: 18, Global Step: 189800, Data Step: 379600, Loss: 2.953125, Token per second per gpu: 22003.083015663356
Epoch: 18, Global Step: 189900, Data Step: 379800, Loss: 3.046875, Token per second per gpu: 22040.764353686816
Epoch: 18, Global Step: 190000, Data Step: 380000, Loss: 3.234375, Token per second per gpu: 22140.674824286554
I0329 10:11:41.418451 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0329 10:11:41.418838 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-29 10:11:41,419] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-29 10:11:41,423] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-29 10:11:41,423] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-29 10:11:41,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-29 10:11:41,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-29 10:11:41,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-29 10:11:41,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-29 10:11:41,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-29 10:11:41,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-29 10:11:41,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-29 10:11:43,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-29 10:11:43,993] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-29 10:11:43,993] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 10:11:44,202] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-29 10:11:44,202] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-29 10:11:44,202] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 10:11:44,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-29 10:11:44,223] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-29 10:11:44,223] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 10:11:44,227] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-29 10:11:44,228] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-29 10:11:44,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-29 10:11:44,228] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 10:11:44,228] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-29 10:11:44,228] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 10:11:44,234] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-29 10:11:44,235] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-29 10:11:44,235] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0329 10:11:44.235852 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0329 10:11:44.236520 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0329 10:11:44.236753 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0329 10:11:44.238066 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 18, Global Step: 190100, Data Step: 380200, Loss: 2.984375, Token per second per gpu: 21752.2528408358
Epoch: 18, Global Step: 190200, Data Step: 380400, Loss: 3.109375, Token per second per gpu: 22299.562385232788
Epoch: 18, Global Step: 190300, Data Step: 380600, Loss: 3.328125, Token per second per gpu: 21879.747085998282
Epoch: 18, Global Step: 190400, Data Step: 380800, Loss: 2.953125, Token per second per gpu: 22286.924667552314
Epoch: 18, Global Step: 190500, Data Step: 381000, Loss: 2.859375, Token per second per gpu: 22020.042113975065
Epoch: 18, Global Step: 190600, Data Step: 381200, Loss: 3.109375, Token per second per gpu: 22097.033514589893
Epoch: 18, Global Step: 190700, Data Step: 381400, Loss: 3.421875, Token per second per gpu: 22127.409302189295
Epoch: 18, Global Step: 190800, Data Step: 381600, Loss: 3.140625, Token per second per gpu: 22394.91684810602
Epoch: 18, Global Step: 190900, Data Step: 381800, Loss: 3.171875, Token per second per gpu: 22096.610629901566
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0329 10:32:55.710717 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 19, Global Step: 191000, Data Step: 382000, Loss: 3.34375, Token per second per gpu: 20923.0896947965
Epoch: 19, Global Step: 191100, Data Step: 382200, Loss: 3.203125, Token per second per gpu: 22212.427684680133
Epoch: 19, Global Step: 191200, Data Step: 382400, Loss: 2.75, Token per second per gpu: 22214.742983800767
Epoch: 19, Global Step: 191300, Data Step: 382600, Loss: 3.140625, Token per second per gpu: 22327.34937622875
Epoch: 19, Global Step: 191400, Data Step: 382800, Loss: 2.953125, Token per second per gpu: 21980.76739277177
Epoch: 19, Global Step: 191500, Data Step: 383000, Loss: 3.09375, Token per second per gpu: 22150.573572918292
Epoch: 19, Global Step: 191600, Data Step: 383200, Loss: 2.9375, Token per second per gpu: 21810.01409490468
Epoch: 19, Global Step: 191700, Data Step: 383400, Loss: 2.59375, Token per second per gpu: 22487.80989111669
Epoch: 19, Global Step: 191800, Data Step: 383600, Loss: 3.078125, Token per second per gpu: 22289.013734302498
Epoch: 19, Global Step: 191900, Data Step: 383800, Loss: 3.046875, Token per second per gpu: 22167.51687996404
Epoch: 19, Global Step: 192000, Data Step: 384000, Loss: 3.234375, Token per second per gpu: 21997.980645233558
Epoch: 19, Global Step: 192100, Data Step: 384200, Loss: 2.921875, Token per second per gpu: 22585.09248426417
Epoch: 19, Global Step: 192200, Data Step: 384400, Loss: 3.046875, Token per second per gpu: 22029.473533830722
Epoch: 19, Global Step: 192300, Data Step: 384600, Loss: 2.6875, Token per second per gpu: 22025.276769541633
Epoch: 19, Global Step: 192400, Data Step: 384800, Loss: 3.015625, Token per second per gpu: 22361.71501770416
Epoch: 19, Global Step: 192500, Data Step: 385000, Loss: 3.0625, Token per second per gpu: 22182.896962469502
Epoch: 19, Global Step: 192600, Data Step: 385200, Loss: 3.328125, Token per second per gpu: 22154.83382977107
Epoch: 19, Global Step: 192700, Data Step: 385400, Loss: 2.96875, Token per second per gpu: 22071.814232523793
Epoch: 19, Global Step: 192800, Data Step: 385600, Loss: 3.234375, Token per second per gpu: 22384.20215390303
Epoch: 19, Global Step: 192900, Data Step: 385800, Loss: 3.03125, Token per second per gpu: 22680.12748265636
Epoch: 19, Global Step: 193000, Data Step: 386000, Loss: 3.015625, Token per second per gpu: 22350.092856921478
Epoch: 19, Global Step: 193100, Data Step: 386200, Loss: 3.078125, Token per second per gpu: 22261.316237170806
Epoch: 19, Global Step: 193200, Data Step: 386400, Loss: 2.921875, Token per second per gpu: 22459.443761091126
Epoch: 19, Global Step: 193300, Data Step: 386600, Loss: 3.328125, Token per second per gpu: 22445.14788431613
Epoch: 19, Global Step: 193400, Data Step: 386800, Loss: 3.203125, Token per second per gpu: 22282.216409767196
Epoch: 19, Global Step: 193500, Data Step: 387000, Loss: 3.265625, Token per second per gpu: 22509.679518905978
Epoch: 19, Global Step: 193600, Data Step: 387200, Loss: 3.125, Token per second per gpu: 22298.014727615664
Epoch: 19, Global Step: 193700, Data Step: 387400, Loss: 3.0625, Token per second per gpu: 22282.40885162619
Epoch: 19, Global Step: 193800, Data Step: 387600, Loss: 2.765625, Token per second per gpu: 22405.517337134253
Epoch: 19, Global Step: 193900, Data Step: 387800, Loss: 3.0625, Token per second per gpu: 22304.54324359736
Epoch: 19, Global Step: 194000, Data Step: 388000, Loss: 2.859375, Token per second per gpu: 22398.61326877016
Epoch: 19, Global Step: 194100, Data Step: 388200, Loss: 3.296875, Token per second per gpu: 22333.862049139407
Epoch: 19, Global Step: 194200, Data Step: 388400, Loss: 2.953125, Token per second per gpu: 22525.04733010289
Epoch: 19, Global Step: 194300, Data Step: 388600, Loss: 2.546875, Token per second per gpu: 22306.624410473007
Epoch: 19, Global Step: 194400, Data Step: 388800, Loss: 3.3125, Token per second per gpu: 22489.165072364813
Epoch: 19, Global Step: 194500, Data Step: 389000, Loss: 3.15625, Token per second per gpu: 22238.39523986675
Epoch: 19, Global Step: 194600, Data Step: 389200, Loss: 2.9375, Token per second per gpu: 22628.428070859794
Epoch: 19, Global Step: 194700, Data Step: 389400, Loss: 3.03125, Token per second per gpu: 22408.85870545406
Epoch: 19, Global Step: 194800, Data Step: 389600, Loss: 3.25, Token per second per gpu: 22438.689406109865
Epoch: 19, Global Step: 194900, Data Step: 389800, Loss: 3.15625, Token per second per gpu: 22276.828359894993
Epoch: 19, Global Step: 195000, Data Step: 390000, Loss: 3.15625, Token per second per gpu: 22504.197685727187
Epoch: 19, Global Step: 195100, Data Step: 390200, Loss: 3.09375, Token per second per gpu: 22368.382166384246
Epoch: 19, Global Step: 195200, Data Step: 390400, Loss: 3.21875, Token per second per gpu: 22263.051978930354
Epoch: 19, Global Step: 195300, Data Step: 390600, Loss: 3.125, Token per second per gpu: 22333.969989272293
Epoch: 19, Global Step: 195400, Data Step: 390800, Loss: 2.796875, Token per second per gpu: 22532.81793780408
Epoch: 19, Global Step: 195500, Data Step: 391000, Loss: 3.078125, Token per second per gpu: 22446.75228157043
Epoch: 19, Global Step: 195600, Data Step: 391200, Loss: 3.015625, Token per second per gpu: 22299.690083371992
Epoch: 19, Global Step: 195700, Data Step: 391400, Loss: 3.0, Token per second per gpu: 22545.141143925648
Epoch: 19, Global Step: 195800, Data Step: 391600, Loss: 2.90625, Token per second per gpu: 22659.978237588643
Epoch: 19, Global Step: 195900, Data Step: 391800, Loss: 3.0, Token per second per gpu: 22771.85339964126
Epoch: 19, Global Step: 196000, Data Step: 392000, Loss: 2.84375, Token per second per gpu: 22331.003790879528
Epoch: 19, Global Step: 196100, Data Step: 392200, Loss: 3.21875, Token per second per gpu: 22515.165702033475
Epoch: 19, Global Step: 196200, Data Step: 392400, Loss: 3.3125, Token per second per gpu: 22479.67440927439
Epoch: 19, Global Step: 196300, Data Step: 392600, Loss: 3.09375, Token per second per gpu: 22547.8386146383
Epoch: 19, Global Step: 196400, Data Step: 392800, Loss: 3.140625, Token per second per gpu: 22615.441127077498
Epoch: 19, Global Step: 196500, Data Step: 393000, Loss: 3.0, Token per second per gpu: 22323.64293823655
Epoch: 19, Global Step: 196600, Data Step: 393200, Loss: 3.515625, Token per second per gpu: 22483.401417449477
Epoch: 19, Global Step: 196700, Data Step: 393400, Loss: 3.140625, Token per second per gpu: 22701.50907208049
Epoch: 19, Global Step: 196800, Data Step: 393600, Loss: 3.09375, Token per second per gpu: 21825.5347992929
Epoch: 19, Global Step: 196900, Data Step: 393800, Loss: 3.25, Token per second per gpu: 22593.715257141903
Epoch: 19, Global Step: 197000, Data Step: 394000, Loss: 3.0, Token per second per gpu: 22480.550653025755
Epoch: 19, Global Step: 197100, Data Step: 394200, Loss: 3.125, Token per second per gpu: 22691.20747074264
Epoch: 19, Global Step: 197200, Data Step: 394400, Loss: 3.140625, Token per second per gpu: 22526.737442637626
Epoch: 19, Global Step: 197300, Data Step: 394600, Loss: 3.1875, Token per second per gpu: 22504.60310927897
Epoch: 19, Global Step: 197400, Data Step: 394800, Loss: 3.015625, Token per second per gpu: 22365.72717030689
Epoch: 19, Global Step: 197500, Data Step: 395000, Loss: 2.984375, Token per second per gpu: 22408.297265974317
Epoch: 19, Global Step: 197600, Data Step: 395200, Loss: 3.296875, Token per second per gpu: 22595.100772941056
Epoch: 19, Global Step: 197700, Data Step: 395400, Loss: 2.875, Token per second per gpu: 22501.732337476635
Epoch: 19, Global Step: 197800, Data Step: 395600, Loss: 2.84375, Token per second per gpu: 22626.02133449768
Epoch: 19, Global Step: 197900, Data Step: 395800, Loss: 3.09375, Token per second per gpu: 22379.345697801054
Epoch: 19, Global Step: 198000, Data Step: 396000, Loss: 3.28125, Token per second per gpu: 22625.72505772352
Epoch: 19, Global Step: 198100, Data Step: 396200, Loss: 3.125, Token per second per gpu: 22340.212140251984
Epoch: 19, Global Step: 198200, Data Step: 396400, Loss: 2.984375, Token per second per gpu: 22641.437195654944
Epoch: 19, Global Step: 198300, Data Step: 396600, Loss: 3.078125, Token per second per gpu: 22444.126439831027
Epoch: 19, Global Step: 198400, Data Step: 396800, Loss: 2.921875, Token per second per gpu: 22414.56506831098
Epoch: 19, Global Step: 198500, Data Step: 397000, Loss: 2.890625, Token per second per gpu: 22435.357470833333
Epoch: 19, Global Step: 198600, Data Step: 397200, Loss: 2.90625, Token per second per gpu: 21389.288414413335
Epoch: 19, Global Step: 198700, Data Step: 397400, Loss: 3.109375, Token per second per gpu: 22495.375313676668
Epoch: 19, Global Step: 198800, Data Step: 397600, Loss: 3.125, Token per second per gpu: 22454.570675042927
Epoch: 19, Global Step: 198900, Data Step: 397800, Loss: 3.25, Token per second per gpu: 22529.77045162698
Epoch: 19, Global Step: 199000, Data Step: 398000, Loss: 3.0625, Token per second per gpu: 22305.395631521103
Epoch: 19, Global Step: 199100, Data Step: 398200, Loss: 3.171875, Token per second per gpu: 21968.517616382145
Epoch: 19, Global Step: 199200, Data Step: 398400, Loss: 3.078125, Token per second per gpu: 22333.76331827885
Epoch: 19, Global Step: 199300, Data Step: 398600, Loss: 3.296875, Token per second per gpu: 22599.67979596745
Epoch: 19, Global Step: 199400, Data Step: 398800, Loss: 2.953125, Token per second per gpu: 21247.436218816838
Epoch: 19, Global Step: 199500, Data Step: 399000, Loss: 3.25, Token per second per gpu: 22420.696760067796
Epoch: 19, Global Step: 199600, Data Step: 399200, Loss: 2.859375, Token per second per gpu: 22332.29331861226
Epoch: 19, Global Step: 199700, Data Step: 399400, Loss: 3.046875, Token per second per gpu: 22231.1142867988
Epoch: 19, Global Step: 199800, Data Step: 399600, Loss: 3.140625, Token per second per gpu: 22293.295036008716
Epoch: 19, Global Step: 199900, Data Step: 399800, Loss: 3.234375, Token per second per gpu: 22420.649902116107
Epoch: 19, Global Step: 200000, Data Step: 400000, Loss: 3.0625, Token per second per gpu: 22372.402761591482
I0329 13:46:45.971821 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0329 13:46:45.973439 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-29 13:46:45,983] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-29 13:46:45,999] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-29 13:46:46,005] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-29 13:46:47,085] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-29 13:46:47,128] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-29 13:46:47,128] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-29 13:46:47,128] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-29 13:46:47,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-29 13:46:47,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-29 13:46:47,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-29 13:46:49,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-29 13:46:49,639] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-29 13:46:49,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 13:46:49,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-29 13:46:49,776] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-29 13:46:49,776] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 13:46:49,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-29 13:46:49,875] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-29 13:46:49,875] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 13:46:49,901] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-29 13:46:49,902] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-29 13:46:49,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 13:46:49,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-29 13:46:49,907] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-29 13:46:49,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 13:46:49,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-29 13:46:49,909] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-29 13:46:49,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0329 13:46:49.909835 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0329 13:46:49.910604 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0329 13:46:49.910884 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0329 13:46:49.912640 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 19, Global Step: 200100, Data Step: 400200, Loss: 3.203125, Token per second per gpu: 21898.635696143992
Epoch: 19, Global Step: 200200, Data Step: 400400, Loss: 2.5625, Token per second per gpu: 22379.000248009495
Epoch: 19, Global Step: 200300, Data Step: 400600, Loss: 3.21875, Token per second per gpu: 22274.056432875954
Epoch: 19, Global Step: 200400, Data Step: 400800, Loss: 2.859375, Token per second per gpu: 22561.431226485118
Epoch: 19, Global Step: 200500, Data Step: 401000, Loss: 3.125, Token per second per gpu: 22379.51983640053
Epoch: 19, Global Step: 200600, Data Step: 401200, Loss: 3.125, Token per second per gpu: 22680.195232776132
Epoch: 19, Global Step: 200700, Data Step: 401400, Loss: 3.203125, Token per second per gpu: 22475.802518515862
Epoch: 19, Global Step: 200800, Data Step: 401600, Loss: 3.09375, Token per second per gpu: 22559.85914198132
Epoch: 19, Global Step: 200900, Data Step: 401800, Loss: 2.78125, Token per second per gpu: 22335.419147354874
Epoch: 19, Global Step: 201000, Data Step: 402000, Loss: 3.28125, Token per second per gpu: 22598.954899908862
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0329 14:08:50.256487 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 20, Global Step: 201100, Data Step: 402200, Loss: 3.328125, Token per second per gpu: 21717.73133232281
Epoch: 20, Global Step: 201200, Data Step: 402400, Loss: 3.375, Token per second per gpu: 22494.88711153218
Epoch: 20, Global Step: 201300, Data Step: 402600, Loss: 2.953125, Token per second per gpu: 22238.031283643224
Epoch: 20, Global Step: 201400, Data Step: 402800, Loss: 3.140625, Token per second per gpu: 22453.91461782969
Epoch: 20, Global Step: 201500, Data Step: 403000, Loss: 3.328125, Token per second per gpu: 22196.941990651332
Epoch: 20, Global Step: 201600, Data Step: 403200, Loss: 2.828125, Token per second per gpu: 22108.74307806617
Epoch: 20, Global Step: 201700, Data Step: 403400, Loss: 3.265625, Token per second per gpu: 22242.10897017097
Epoch: 20, Global Step: 201800, Data Step: 403600, Loss: 3.046875, Token per second per gpu: 22403.351321230326
Epoch: 20, Global Step: 201900, Data Step: 403800, Loss: 3.40625, Token per second per gpu: 22172.00927684437
Epoch: 20, Global Step: 202000, Data Step: 404000, Loss: 2.8125, Token per second per gpu: 22504.940414358705
Epoch: 20, Global Step: 202100, Data Step: 404200, Loss: 2.890625, Token per second per gpu: 22278.062950220516
Epoch: 20, Global Step: 202200, Data Step: 404400, Loss: 2.9375, Token per second per gpu: 22624.175229894696
Epoch: 20, Global Step: 202300, Data Step: 404600, Loss: 2.84375, Token per second per gpu: 22258.317790838108
Epoch: 20, Global Step: 202400, Data Step: 404800, Loss: 2.90625, Token per second per gpu: 22325.393901148716
Epoch: 20, Global Step: 202500, Data Step: 405000, Loss: 2.8125, Token per second per gpu: 22088.894109860787
Epoch: 20, Global Step: 202600, Data Step: 405200, Loss: 2.921875, Token per second per gpu: 22491.739776283597
Epoch: 20, Global Step: 202700, Data Step: 405400, Loss: 2.5625, Token per second per gpu: 22275.052474303386
Epoch: 20, Global Step: 202800, Data Step: 405600, Loss: 2.796875, Token per second per gpu: 22102.610906006368
Epoch: 20, Global Step: 202900, Data Step: 405800, Loss: 2.90625, Token per second per gpu: 22200.545677544105
Epoch: 20, Global Step: 203000, Data Step: 406000, Loss: 3.390625, Token per second per gpu: 22804.850813089815
Epoch: 20, Global Step: 203100, Data Step: 406200, Loss: 2.828125, Token per second per gpu: 22131.560132935614
Epoch: 20, Global Step: 203200, Data Step: 406400, Loss: 3.0625, Token per second per gpu: 22303.37403446421
Epoch: 20, Global Step: 203300, Data Step: 406600, Loss: 3.34375, Token per second per gpu: 22309.555128419568
Epoch: 20, Global Step: 203400, Data Step: 406800, Loss: 3.328125, Token per second per gpu: 22569.138114757618
Epoch: 20, Global Step: 203500, Data Step: 407000, Loss: 3.078125, Token per second per gpu: 22405.948554141632
Epoch: 20, Global Step: 203600, Data Step: 407200, Loss: 2.96875, Token per second per gpu: 22503.756935205955
Epoch: 20, Global Step: 203700, Data Step: 407400, Loss: 3.046875, Token per second per gpu: 22547.754186590846
Epoch: 20, Global Step: 203800, Data Step: 407600, Loss: 2.859375, Token per second per gpu: 22390.501637098903
Epoch: 20, Global Step: 203900, Data Step: 407800, Loss: 3.296875, Token per second per gpu: 22418.363130875045
Epoch: 20, Global Step: 204000, Data Step: 408000, Loss: 3.171875, Token per second per gpu: 22253.2911822235
Epoch: 20, Global Step: 204100, Data Step: 408200, Loss: 3.1875, Token per second per gpu: 22496.937376145255
Epoch: 20, Global Step: 204200, Data Step: 408400, Loss: 3.03125, Token per second per gpu: 22224.880318599764
Epoch: 20, Global Step: 204300, Data Step: 408600, Loss: 2.921875, Token per second per gpu: 22240.559858707915
Epoch: 20, Global Step: 204400, Data Step: 408800, Loss: 3.359375, Token per second per gpu: 22309.470168041757
Epoch: 20, Global Step: 204500, Data Step: 409000, Loss: 3.359375, Token per second per gpu: 22503.851556971975
Epoch: 20, Global Step: 204600, Data Step: 409200, Loss: 2.90625, Token per second per gpu: 22591.770353039334
Epoch: 20, Global Step: 204700, Data Step: 409400, Loss: 3.0625, Token per second per gpu: 22377.901944423393
Epoch: 20, Global Step: 204800, Data Step: 409600, Loss: 2.765625, Token per second per gpu: 22085.938698223435
Epoch: 20, Global Step: 204900, Data Step: 409800, Loss: 3.203125, Token per second per gpu: 22304.649829710022
Epoch: 20, Global Step: 205000, Data Step: 410000, Loss: 3.375, Token per second per gpu: 22410.703556058426
Epoch: 20, Global Step: 205100, Data Step: 410200, Loss: 3.28125, Token per second per gpu: 22359.754470568903
Epoch: 20, Global Step: 205200, Data Step: 410400, Loss: 3.078125, Token per second per gpu: 22239.859700670517
Epoch: 20, Global Step: 205300, Data Step: 410600, Loss: 3.171875, Token per second per gpu: 22142.766679451302
Epoch: 20, Global Step: 205400, Data Step: 410800, Loss: 3.546875, Token per second per gpu: 22307.963856298382
Epoch: 20, Global Step: 205500, Data Step: 411000, Loss: 3.15625, Token per second per gpu: 22226.078158847202
Epoch: 20, Global Step: 205600, Data Step: 411200, Loss: 3.28125, Token per second per gpu: 22168.054886961498
Epoch: 20, Global Step: 205700, Data Step: 411400, Loss: 3.484375, Token per second per gpu: 22105.24558444509
Epoch: 20, Global Step: 205800, Data Step: 411600, Loss: 3.265625, Token per second per gpu: 22500.638635329018
Epoch: 20, Global Step: 205900, Data Step: 411800, Loss: 3.34375, Token per second per gpu: 22384.650221679945
Epoch: 20, Global Step: 206000, Data Step: 412000, Loss: 3.515625, Token per second per gpu: 22383.857176336373
Epoch: 20, Global Step: 206100, Data Step: 412200, Loss: 3.421875, Token per second per gpu: 22547.45174913346
Epoch: 20, Global Step: 206200, Data Step: 412400, Loss: 3.21875, Token per second per gpu: 22413.905025085405
Epoch: 20, Global Step: 206300, Data Step: 412600, Loss: 3.125, Token per second per gpu: 22153.166558161116
Epoch: 20, Global Step: 206400, Data Step: 412800, Loss: 3.0, Token per second per gpu: 22117.983235852993
Epoch: 20, Global Step: 206500, Data Step: 413000, Loss: 3.234375, Token per second per gpu: 22057.132061515953
Epoch: 20, Global Step: 206600, Data Step: 413200, Loss: 3.078125, Token per second per gpu: 21053.14916325718
Epoch: 20, Global Step: 206700, Data Step: 413400, Loss: 3.125, Token per second per gpu: 22494.348287310215
Epoch: 20, Global Step: 206800, Data Step: 413600, Loss: 2.921875, Token per second per gpu: 22500.567091951743
Epoch: 20, Global Step: 206900, Data Step: 413800, Loss: 2.875, Token per second per gpu: 22470.098082974357
Epoch: 20, Global Step: 207000, Data Step: 414000, Loss: 2.859375, Token per second per gpu: 22555.80201913977
Epoch: 20, Global Step: 207100, Data Step: 414200, Loss: 3.203125, Token per second per gpu: 22612.64220475043
Epoch: 20, Global Step: 207200, Data Step: 414400, Loss: 3.078125, Token per second per gpu: 22342.749088372828
Epoch: 20, Global Step: 207300, Data Step: 414600, Loss: 3.015625, Token per second per gpu: 22100.38862840507
Epoch: 20, Global Step: 207400, Data Step: 414800, Loss: 3.0, Token per second per gpu: 22387.568493449948
Epoch: 20, Global Step: 207500, Data Step: 415000, Loss: 2.828125, Token per second per gpu: 22340.839257770225
Epoch: 20, Global Step: 207600, Data Step: 415200, Loss: 2.953125, Token per second per gpu: 22350.72205944258
Epoch: 20, Global Step: 207700, Data Step: 415400, Loss: 3.15625, Token per second per gpu: 22112.0286073625
Epoch: 20, Global Step: 207800, Data Step: 415600, Loss: 3.21875, Token per second per gpu: 22137.34158717299
Epoch: 20, Global Step: 207900, Data Step: 415800, Loss: 3.15625, Token per second per gpu: 22602.856695575043
Epoch: 20, Global Step: 208000, Data Step: 416000, Loss: 3.234375, Token per second per gpu: 21995.245233509908
Epoch: 20, Global Step: 208100, Data Step: 416200, Loss: 2.65625, Token per second per gpu: 22215.714402014397
Epoch: 20, Global Step: 208200, Data Step: 416400, Loss: 3.234375, Token per second per gpu: 22201.223164415933
Epoch: 20, Global Step: 208300, Data Step: 416600, Loss: 3.171875, Token per second per gpu: 22633.352449038655
Epoch: 20, Global Step: 208400, Data Step: 416800, Loss: 3.109375, Token per second per gpu: 22689.17197596512
Epoch: 20, Global Step: 208500, Data Step: 417000, Loss: 3.03125, Token per second per gpu: 21433.9988250333
Epoch: 20, Global Step: 208600, Data Step: 417200, Loss: 3.25, Token per second per gpu: 22550.49913747613
Epoch: 20, Global Step: 208700, Data Step: 417400, Loss: 2.90625, Token per second per gpu: 22205.811039147415
Epoch: 20, Global Step: 208800, Data Step: 417600, Loss: 2.984375, Token per second per gpu: 22224.240312958103
Epoch: 20, Global Step: 208900, Data Step: 417800, Loss: 2.609375, Token per second per gpu: 22442.248939350862
Epoch: 20, Global Step: 209000, Data Step: 418000, Loss: 3.234375, Token per second per gpu: 22298.05296572955
Epoch: 20, Global Step: 209100, Data Step: 418200, Loss: 3.0625, Token per second per gpu: 21504.9707360066
Epoch: 20, Global Step: 209200, Data Step: 418400, Loss: 3.203125, Token per second per gpu: 22564.666255160297
Epoch: 20, Global Step: 209300, Data Step: 418600, Loss: 3.25, Token per second per gpu: 22405.070552980516
Epoch: 20, Global Step: 209400, Data Step: 418800, Loss: 3.3125, Token per second per gpu: 22835.187202091372
Epoch: 20, Global Step: 209500, Data Step: 419000, Loss: 3.046875, Token per second per gpu: 22162.758170698224
Epoch: 20, Global Step: 209600, Data Step: 419200, Loss: 3.1875, Token per second per gpu: 22109.030218902248
Epoch: 20, Global Step: 209700, Data Step: 419400, Loss: 3.171875, Token per second per gpu: 22443.624988945263
Epoch: 20, Global Step: 209800, Data Step: 419600, Loss: 3.265625, Token per second per gpu: 22118.97387288903
Epoch: 20, Global Step: 209900, Data Step: 419800, Loss: 3.234375, Token per second per gpu: 22054.733769206796
Epoch: 20, Global Step: 210000, Data Step: 420000, Loss: 3.0, Token per second per gpu: 22470.264650022356
I0329 17:21:55.466430 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0329 17:21:55.467828 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-29 17:21:55,468] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-29 17:21:55,495] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-29 17:21:55,499] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-29 17:21:56,775] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-29 17:21:56,842] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-29 17:21:56,842] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-29 17:21:56,842] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-29 17:21:56,842] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-29 17:21:56,851] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-29 17:21:56,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-29 17:21:57,814] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-29 17:21:57,815] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-29 17:21:57,815] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 17:21:58,576] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-29 17:21:58,576] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-29 17:21:58,576] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 17:21:59,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-29 17:21:59,006] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-29 17:21:59,006] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 17:21:59,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-29 17:21:59,063] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-29 17:21:59,063] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 17:21:59,102] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-29 17:21:59,102] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-29 17:21:59,102] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 17:21:59,115] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-29 17:21:59,115] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-29 17:21:59,116] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0329 17:21:59.116555 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0329 17:21:59.117320 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0329 17:21:59.117585 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0329 17:21:59.119050 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 20, Global Step: 210100, Data Step: 420200, Loss: 3.125, Token per second per gpu: 21725.469725464158
Epoch: 20, Global Step: 210200, Data Step: 420400, Loss: 3.40625, Token per second per gpu: 22440.446374149666
Epoch: 20, Global Step: 210300, Data Step: 420600, Loss: 3.109375, Token per second per gpu: 22247.92555136612
Epoch: 20, Global Step: 210400, Data Step: 420800, Loss: 2.828125, Token per second per gpu: 22392.691403651945
Epoch: 20, Global Step: 210500, Data Step: 421000, Loss: 2.6875, Token per second per gpu: 20756.15507188391
Epoch: 20, Global Step: 210600, Data Step: 421200, Loss: 3.140625, Token per second per gpu: 22201.835933124574
Epoch: 20, Global Step: 210700, Data Step: 421400, Loss: 3.328125, Token per second per gpu: 22120.875731579756
Epoch: 20, Global Step: 210800, Data Step: 421600, Loss: 3.296875, Token per second per gpu: 22379.408719076524
Epoch: 20, Global Step: 210900, Data Step: 421800, Loss: 3.09375, Token per second per gpu: 21790.884220475375
Epoch: 20, Global Step: 211000, Data Step: 422000, Loss: 3.171875, Token per second per gpu: 22077.6416201064
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0329 17:45:27.919232 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 21, Global Step: 211100, Data Step: 422200, Loss: 3.03125, Token per second per gpu: 21129.07518512969
Epoch: 21, Global Step: 211200, Data Step: 422400, Loss: 3.28125, Token per second per gpu: 22067.04366988744
Epoch: 21, Global Step: 211300, Data Step: 422600, Loss: 2.984375, Token per second per gpu: 22428.558143131413
Epoch: 21, Global Step: 211400, Data Step: 422800, Loss: 3.421875, Token per second per gpu: 22100.918933921505
Epoch: 21, Global Step: 211500, Data Step: 423000, Loss: 2.953125, Token per second per gpu: 22259.069274990172
Epoch: 21, Global Step: 211600, Data Step: 423200, Loss: 3.09375, Token per second per gpu: 22554.755738837586
Epoch: 21, Global Step: 211700, Data Step: 423400, Loss: 3.375, Token per second per gpu: 22456.085872107196
Epoch: 21, Global Step: 211800, Data Step: 423600, Loss: 3.109375, Token per second per gpu: 22297.433022299756
Epoch: 21, Global Step: 211900, Data Step: 423800, Loss: 3.03125, Token per second per gpu: 22301.7320114248
Epoch: 21, Global Step: 212000, Data Step: 424000, Loss: 3.046875, Token per second per gpu: 22060.218677632685
Epoch: 21, Global Step: 212100, Data Step: 424200, Loss: 2.90625, Token per second per gpu: 22820.070034230743
Epoch: 21, Global Step: 212200, Data Step: 424400, Loss: 3.296875, Token per second per gpu: 22545.643312153625
Epoch: 21, Global Step: 212300, Data Step: 424600, Loss: 3.25, Token per second per gpu: 22464.611528744124
Epoch: 21, Global Step: 212400, Data Step: 424800, Loss: 2.984375, Token per second per gpu: 22335.922176201264
Epoch: 21, Global Step: 212500, Data Step: 425000, Loss: 2.796875, Token per second per gpu: 23102.026549512215
Epoch: 21, Global Step: 212600, Data Step: 425200, Loss: 3.0625, Token per second per gpu: 22340.728193675608
Epoch: 21, Global Step: 212700, Data Step: 425400, Loss: 3.0625, Token per second per gpu: 22339.209312939383
Epoch: 21, Global Step: 212800, Data Step: 425600, Loss: 3.28125, Token per second per gpu: 22481.611608091407
Epoch: 21, Global Step: 212900, Data Step: 425800, Loss: 3.0, Token per second per gpu: 22430.355924996053
Epoch: 21, Global Step: 213000, Data Step: 426000, Loss: 3.28125, Token per second per gpu: 22654.722202076722
Epoch: 21, Global Step: 213100, Data Step: 426200, Loss: 2.796875, Token per second per gpu: 22305.24084924413
Epoch: 21, Global Step: 213200, Data Step: 426400, Loss: 2.984375, Token per second per gpu: 22485.756015845705
Epoch: 21, Global Step: 213300, Data Step: 426600, Loss: 2.828125, Token per second per gpu: 22536.188766796015
Epoch: 21, Global Step: 213400, Data Step: 426800, Loss: 3.203125, Token per second per gpu: 22851.761638391326
Epoch: 21, Global Step: 213500, Data Step: 427000, Loss: 3.421875, Token per second per gpu: 22366.19835235105
Epoch: 21, Global Step: 213600, Data Step: 427200, Loss: 3.0625, Token per second per gpu: 22647.680622767875
Epoch: 21, Global Step: 213700, Data Step: 427400, Loss: 3.15625, Token per second per gpu: 22370.29409921513
Epoch: 21, Global Step: 213800, Data Step: 427600, Loss: 3.046875, Token per second per gpu: 22384.418636178034
Epoch: 21, Global Step: 213900, Data Step: 427800, Loss: 3.109375, Token per second per gpu: 22444.866042593894
Epoch: 21, Global Step: 214000, Data Step: 428000, Loss: 3.0, Token per second per gpu: 22698.147084453532
Epoch: 21, Global Step: 214100, Data Step: 428200, Loss: 2.984375, Token per second per gpu: 22520.673831876364
Epoch: 21, Global Step: 214200, Data Step: 428400, Loss: 3.1875, Token per second per gpu: 22783.52670964737
Epoch: 21, Global Step: 214300, Data Step: 428600, Loss: 3.046875, Token per second per gpu: 22397.17504316769
Epoch: 21, Global Step: 214400, Data Step: 428800, Loss: 3.25, Token per second per gpu: 22236.55154968256
Epoch: 21, Global Step: 214500, Data Step: 429000, Loss: 2.875, Token per second per gpu: 22685.969873292775
Epoch: 21, Global Step: 214600, Data Step: 429200, Loss: 2.953125, Token per second per gpu: 22530.427209053338
Epoch: 21, Global Step: 214700, Data Step: 429400, Loss: 2.796875, Token per second per gpu: 21957.79727467236
Epoch: 21, Global Step: 214800, Data Step: 429600, Loss: 2.6875, Token per second per gpu: 22313.040586142124
Epoch: 21, Global Step: 214900, Data Step: 429800, Loss: 3.1875, Token per second per gpu: 22503.66181115773
Epoch: 21, Global Step: 215000, Data Step: 430000, Loss: 2.9375, Token per second per gpu: 22666.80233677488
Epoch: 21, Global Step: 215100, Data Step: 430200, Loss: 3.46875, Token per second per gpu: 22568.70320784129
Epoch: 21, Global Step: 215200, Data Step: 430400, Loss: 3.046875, Token per second per gpu: 22465.068712373573
Epoch: 21, Global Step: 215300, Data Step: 430600, Loss: 3.109375, Token per second per gpu: 22402.6756099807
Epoch: 21, Global Step: 215400, Data Step: 430800, Loss: 2.859375, Token per second per gpu: 22170.32932801605
Epoch: 21, Global Step: 215500, Data Step: 431000, Loss: 3.0, Token per second per gpu: 22567.886906589025
Epoch: 21, Global Step: 215600, Data Step: 431200, Loss: 3.21875, Token per second per gpu: 22625.394589652093
Epoch: 21, Global Step: 215700, Data Step: 431400, Loss: 2.9375, Token per second per gpu: 22255.748846430528
Epoch: 21, Global Step: 215800, Data Step: 431600, Loss: 3.15625, Token per second per gpu: 22272.01004238734
Epoch: 21, Global Step: 215900, Data Step: 431800, Loss: 2.703125, Token per second per gpu: 22670.834478807155
Epoch: 21, Global Step: 216000, Data Step: 432000, Loss: 3.03125, Token per second per gpu: 22500.76076725395
Epoch: 21, Global Step: 216100, Data Step: 432200, Loss: 3.0, Token per second per gpu: 22281.06922603467
Epoch: 21, Global Step: 216200, Data Step: 432400, Loss: 3.578125, Token per second per gpu: 22276.988417529283
Epoch: 21, Global Step: 216300, Data Step: 432600, Loss: 3.15625, Token per second per gpu: 22149.115607815325
Epoch: 21, Global Step: 216400, Data Step: 432800, Loss: 3.421875, Token per second per gpu: 21992.971415437
Epoch: 21, Global Step: 216500, Data Step: 433000, Loss: 3.15625, Token per second per gpu: 21860.294169821947
Epoch: 21, Global Step: 216600, Data Step: 433200, Loss: 3.03125, Token per second per gpu: 22440.69921444501
Epoch: 21, Global Step: 216700, Data Step: 433400, Loss: 3.21875, Token per second per gpu: 22267.83198059559
Epoch: 21, Global Step: 216800, Data Step: 433600, Loss: 2.96875, Token per second per gpu: 22418.397289409204
Epoch: 21, Global Step: 216900, Data Step: 433800, Loss: 3.21875, Token per second per gpu: 22413.693836098497
Epoch: 21, Global Step: 217000, Data Step: 434000, Loss: 3.34375, Token per second per gpu: 22553.185595051196
Epoch: 21, Global Step: 217100, Data Step: 434200, Loss: 2.84375, Token per second per gpu: 22582.443930545778
Epoch: 21, Global Step: 217200, Data Step: 434400, Loss: 3.078125, Token per second per gpu: 22165.425674124315
Epoch: 21, Global Step: 217300, Data Step: 434600, Loss: 3.15625, Token per second per gpu: 22218.979695818824
Epoch: 21, Global Step: 217400, Data Step: 434800, Loss: 3.28125, Token per second per gpu: 22256.474240530562
Epoch: 21, Global Step: 217500, Data Step: 435000, Loss: 2.921875, Token per second per gpu: 22452.110929401737
Epoch: 21, Global Step: 217600, Data Step: 435200, Loss: 3.078125, Token per second per gpu: 22328.47157283643
Epoch: 21, Global Step: 217700, Data Step: 435400, Loss: 2.96875, Token per second per gpu: 22398.2888617692
Epoch: 21, Global Step: 217800, Data Step: 435600, Loss: 3.234375, Token per second per gpu: 22428.010665500016
Epoch: 21, Global Step: 217900, Data Step: 435800, Loss: 2.6875, Token per second per gpu: 22120.016041064504
Epoch: 21, Global Step: 218000, Data Step: 436000, Loss: 3.03125, Token per second per gpu: 22438.97376041022
Epoch: 21, Global Step: 218100, Data Step: 436200, Loss: 3.359375, Token per second per gpu: 22694.29132851964
Epoch: 21, Global Step: 218200, Data Step: 436400, Loss: 3.171875, Token per second per gpu: 22382.727622391616
Epoch: 21, Global Step: 218300, Data Step: 436600, Loss: 3.28125, Token per second per gpu: 22637.76715251759
Epoch: 21, Global Step: 218400, Data Step: 436800, Loss: 2.953125, Token per second per gpu: 21224.178740416315
Epoch: 21, Global Step: 218500, Data Step: 437000, Loss: 3.1875, Token per second per gpu: 22576.44641660045
Epoch: 21, Global Step: 218600, Data Step: 437200, Loss: 3.140625, Token per second per gpu: 22309.71347288743
Epoch: 21, Global Step: 218700, Data Step: 437400, Loss: 3.21875, Token per second per gpu: 22311.560702635554
Epoch: 21, Global Step: 218800, Data Step: 437600, Loss: 3.0, Token per second per gpu: 22667.02738220369
Epoch: 21, Global Step: 218900, Data Step: 437800, Loss: 2.953125, Token per second per gpu: 22430.26137882089
Epoch: 21, Global Step: 219000, Data Step: 438000, Loss: 3.03125, Token per second per gpu: 22360.45917659398
Epoch: 21, Global Step: 219100, Data Step: 438200, Loss: 3.3125, Token per second per gpu: 20730.157872148146
Epoch: 21, Global Step: 219200, Data Step: 438400, Loss: 3.0, Token per second per gpu: 22405.725005781816
Epoch: 21, Global Step: 219300, Data Step: 438600, Loss: 2.78125, Token per second per gpu: 22215.138984678026
Epoch: 21, Global Step: 219400, Data Step: 438800, Loss: 3.453125, Token per second per gpu: 22505.18930125827
Epoch: 21, Global Step: 219500, Data Step: 439000, Loss: 2.90625, Token per second per gpu: 22343.310223602522
Epoch: 21, Global Step: 219600, Data Step: 439200, Loss: 3.078125, Token per second per gpu: 22550.943573247245
Epoch: 21, Global Step: 219700, Data Step: 439400, Loss: 2.734375, Token per second per gpu: 22734.55399429122
Epoch: 21, Global Step: 219800, Data Step: 439600, Loss: 3.28125, Token per second per gpu: 22720.996366807958
Epoch: 21, Global Step: 219900, Data Step: 439800, Loss: 3.171875, Token per second per gpu: 22564.411498827318
Epoch: 21, Global Step: 220000, Data Step: 440000, Loss: 2.8125, Token per second per gpu: 22547.214804775962
I0329 20:56:48.065246 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0329 20:56:48.066934 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-29 20:56:48,073] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-29 20:56:48,085] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-29 20:56:48,091] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-29 20:56:49,050] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-29 20:56:49,123] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-29 20:56:49,123] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-29 20:56:49,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-29 20:56:49,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-29 20:56:49,134] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-29 20:56:49,134] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-29 20:56:51,245] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-29 20:56:51,245] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-29 20:56:51,245] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 20:56:51,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-29 20:56:51,535] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-29 20:56:51,535] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 20:56:51,572] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-29 20:56:51,572] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-29 20:56:51,572] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 20:56:51,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-29 20:56:51,579] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-29 20:56:51,579] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 20:56:51,591] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-29 20:56:51,591] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-29 20:56:51,591] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-29 20:56:51,614] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-29 20:56:51,614] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-29 20:56:51,615] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0329 20:56:51.615576 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0329 20:56:51.616194 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0329 20:56:51.616415 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0329 20:56:51.617679 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 21, Global Step: 220100, Data Step: 440200, Loss: 2.953125, Token per second per gpu: 21851.695995856662
Epoch: 21, Global Step: 220200, Data Step: 440400, Loss: 3.15625, Token per second per gpu: 22173.41779289863
Epoch: 21, Global Step: 220300, Data Step: 440600, Loss: 3.296875, Token per second per gpu: 22294.943799772
Epoch: 21, Global Step: 220400, Data Step: 440800, Loss: 2.875, Token per second per gpu: 22303.115384453973
Epoch: 21, Global Step: 220500, Data Step: 441000, Loss: 3.015625, Token per second per gpu: 22219.980913022693
Epoch: 21, Global Step: 220600, Data Step: 441200, Loss: 3.25, Token per second per gpu: 22648.81346664794
Epoch: 21, Global Step: 220700, Data Step: 441400, Loss: 3.09375, Token per second per gpu: 22432.452014306004
Epoch: 21, Global Step: 220800, Data Step: 441600, Loss: 2.9375, Token per second per gpu: 22287.91733666042
Epoch: 21, Global Step: 220900, Data Step: 441800, Loss: 3.0, Token per second per gpu: 22333.88492542321
Epoch: 21, Global Step: 221000, Data Step: 442000, Loss: 2.9375, Token per second per gpu: 22515.127554975435
Epoch: 21, Global Step: 221100, Data Step: 442200, Loss: 3.375, Token per second per gpu: 21935.552257259707
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0329 21:21:12.707715 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 22, Global Step: 221200, Data Step: 442400, Loss: 3.171875, Token per second per gpu: 21899.78441297151
Epoch: 22, Global Step: 221300, Data Step: 442600, Loss: 3.03125, Token per second per gpu: 22248.57978903938
Epoch: 22, Global Step: 221400, Data Step: 442800, Loss: 2.796875, Token per second per gpu: 22262.97053177315
Epoch: 22, Global Step: 221500, Data Step: 443000, Loss: 3.265625, Token per second per gpu: 22072.107513108196
Epoch: 22, Global Step: 221600, Data Step: 443200, Loss: 3.203125, Token per second per gpu: 22118.138062602033
Epoch: 22, Global Step: 221700, Data Step: 443400, Loss: 3.09375, Token per second per gpu: 22257.25914642436
Epoch: 22, Global Step: 221800, Data Step: 443600, Loss: 3.03125, Token per second per gpu: 22212.341175155456
Epoch: 22, Global Step: 221900, Data Step: 443800, Loss: 3.1875, Token per second per gpu: 22032.67019980024
Epoch: 22, Global Step: 222000, Data Step: 444000, Loss: 2.859375, Token per second per gpu: 22383.052657131124
Epoch: 22, Global Step: 222100, Data Step: 444200, Loss: 3.25, Token per second per gpu: 22203.79455519545
Epoch: 22, Global Step: 222200, Data Step: 444400, Loss: 2.984375, Token per second per gpu: 22513.12327467265
Epoch: 22, Global Step: 222300, Data Step: 444600, Loss: 3.046875, Token per second per gpu: 22254.608199367405
Epoch: 22, Global Step: 222400, Data Step: 444800, Loss: 2.859375, Token per second per gpu: 22224.94226845744
Epoch: 22, Global Step: 222500, Data Step: 445000, Loss: 2.9375, Token per second per gpu: 22420.780156054225
Epoch: 22, Global Step: 222600, Data Step: 445200, Loss: 3.109375, Token per second per gpu: 22128.95931160394
Epoch: 22, Global Step: 222700, Data Step: 445400, Loss: 3.40625, Token per second per gpu: 22221.38686468567
Epoch: 22, Global Step: 222800, Data Step: 445600, Loss: 2.984375, Token per second per gpu: 22497.73057718363
Epoch: 22, Global Step: 222900, Data Step: 445800, Loss: 3.109375, Token per second per gpu: 22147.986311272067
Epoch: 22, Global Step: 223000, Data Step: 446000, Loss: 2.78125, Token per second per gpu: 22415.510657322437
Epoch: 22, Global Step: 223100, Data Step: 446200, Loss: 2.953125, Token per second per gpu: 22421.892203444524
Epoch: 22, Global Step: 223200, Data Step: 446400, Loss: 3.28125, Token per second per gpu: 22591.954531376097
Epoch: 22, Global Step: 223300, Data Step: 446600, Loss: 2.890625, Token per second per gpu: 22435.983190732873
Epoch: 22, Global Step: 223400, Data Step: 446800, Loss: 3.375, Token per second per gpu: 22484.596986232715
Epoch: 22, Global Step: 223500, Data Step: 447000, Loss: 3.09375, Token per second per gpu: 22412.676915787706
Epoch: 22, Global Step: 223600, Data Step: 447200, Loss: 3.03125, Token per second per gpu: 22290.186908755946
Epoch: 22, Global Step: 223700, Data Step: 447400, Loss: 2.984375, Token per second per gpu: 22199.50137441752
Epoch: 22, Global Step: 223800, Data Step: 447600, Loss: 2.765625, Token per second per gpu: 22204.806404515886
Epoch: 22, Global Step: 223900, Data Step: 447800, Loss: 3.21875, Token per second per gpu: 22604.85401637978
Epoch: 22, Global Step: 224000, Data Step: 448000, Loss: 2.921875, Token per second per gpu: 22336.622613464144
Epoch: 22, Global Step: 224100, Data Step: 448200, Loss: 2.90625, Token per second per gpu: 22297.619800193308
Epoch: 22, Global Step: 224200, Data Step: 448400, Loss: 3.09375, Token per second per gpu: 22140.94278792632
Epoch: 22, Global Step: 224300, Data Step: 448600, Loss: 2.609375, Token per second per gpu: 22174.48207215975
Epoch: 22, Global Step: 224400, Data Step: 448800, Loss: 3.296875, Token per second per gpu: 22066.16175600796
Epoch: 22, Global Step: 224500, Data Step: 449000, Loss: 3.265625, Token per second per gpu: 22365.365452791728
Epoch: 22, Global Step: 224600, Data Step: 449200, Loss: 3.203125, Token per second per gpu: 22432.13774881247
Epoch: 22, Global Step: 224700, Data Step: 449400, Loss: 3.234375, Token per second per gpu: 22283.622890300227
Epoch: 22, Global Step: 224800, Data Step: 449600, Loss: 2.890625, Token per second per gpu: 22618.541820738017
Epoch: 22, Global Step: 224900, Data Step: 449800, Loss: 3.15625, Token per second per gpu: 22226.62326552358
Epoch: 22, Global Step: 225000, Data Step: 450000, Loss: 3.09375, Token per second per gpu: 22176.239270306214
Epoch: 22, Global Step: 225100, Data Step: 450200, Loss: 2.9375, Token per second per gpu: 22276.974038541
Epoch: 22, Global Step: 225200, Data Step: 450400, Loss: 3.015625, Token per second per gpu: 22113.941174031468
Epoch: 22, Global Step: 225300, Data Step: 450600, Loss: 2.5625, Token per second per gpu: 22111.05202938617
Epoch: 22, Global Step: 225400, Data Step: 450800, Loss: 3.328125, Token per second per gpu: 22274.96991243226
Epoch: 22, Global Step: 225500, Data Step: 451000, Loss: 2.96875, Token per second per gpu: 22004.079299440684
Epoch: 22, Global Step: 225600, Data Step: 451200, Loss: 3.015625, Token per second per gpu: 22215.553916249435
Epoch: 22, Global Step: 225700, Data Step: 451400, Loss: 3.1875, Token per second per gpu: 22140.18075416255
Epoch: 22, Global Step: 225800, Data Step: 451600, Loss: 3.390625, Token per second per gpu: 22035.397242123698
Epoch: 22, Global Step: 225900, Data Step: 451800, Loss: 3.109375, Token per second per gpu: 22209.401477221025
Epoch: 22, Global Step: 226000, Data Step: 452000, Loss: 3.203125, Token per second per gpu: 22038.227768244426
Epoch: 22, Global Step: 226100, Data Step: 452200, Loss: 2.984375, Token per second per gpu: 22013.816501515295
Epoch: 22, Global Step: 226200, Data Step: 452400, Loss: 2.65625, Token per second per gpu: 22093.56873819221
Epoch: 22, Global Step: 226300, Data Step: 452600, Loss: 3.171875, Token per second per gpu: 21069.085852201453
Epoch: 22, Global Step: 226400, Data Step: 452800, Loss: 3.03125, Token per second per gpu: 22090.017186243098
Epoch: 22, Global Step: 226500, Data Step: 453000, Loss: 2.921875, Token per second per gpu: 22612.150507941784
Epoch: 22, Global Step: 226600, Data Step: 453200, Loss: 3.296875, Token per second per gpu: 22477.9053121144
Epoch: 22, Global Step: 226700, Data Step: 453400, Loss: 3.28125, Token per second per gpu: 22446.902693841163
Epoch: 22, Global Step: 226800, Data Step: 453600, Loss: 3.046875, Token per second per gpu: 22018.13218902873
Epoch: 22, Global Step: 226900, Data Step: 453800, Loss: 3.109375, Token per second per gpu: 22575.010366257382
Epoch: 22, Global Step: 227000, Data Step: 454000, Loss: 3.1875, Token per second per gpu: 22445.236758928462
Epoch: 22, Global Step: 227100, Data Step: 454200, Loss: 3.0, Token per second per gpu: 22552.598079081614
Epoch: 22, Global Step: 227200, Data Step: 454400, Loss: 3.0, Token per second per gpu: 22222.38903471757
Epoch: 22, Global Step: 227300, Data Step: 454600, Loss: 3.015625, Token per second per gpu: 22307.07576288377
Epoch: 22, Global Step: 227400, Data Step: 454800, Loss: 3.21875, Token per second per gpu: 22177.68395152806
Epoch: 22, Global Step: 227500, Data Step: 455000, Loss: 3.046875, Token per second per gpu: 22554.52083081411
Epoch: 22, Global Step: 227600, Data Step: 455200, Loss: 3.234375, Token per second per gpu: 22202.225312025093
Epoch: 22, Global Step: 227700, Data Step: 455400, Loss: 3.03125, Token per second per gpu: 22125.362412933326
Epoch: 22, Global Step: 227800, Data Step: 455600, Loss: 3.125, Token per second per gpu: 22069.521089040612
Epoch: 22, Global Step: 227900, Data Step: 455800, Loss: 3.125, Token per second per gpu: 22446.089840866538
Epoch: 22, Global Step: 228000, Data Step: 456000, Loss: 3.265625, Token per second per gpu: 22297.0668429897
Epoch: 22, Global Step: 228100, Data Step: 456200, Loss: 3.421875, Token per second per gpu: 22595.73070124461
Epoch: 22, Global Step: 228200, Data Step: 456400, Loss: 3.296875, Token per second per gpu: 22512.02204594139
Epoch: 22, Global Step: 228300, Data Step: 456600, Loss: 2.984375, Token per second per gpu: 22691.978367268155
Epoch: 22, Global Step: 228400, Data Step: 456800, Loss: 3.203125, Token per second per gpu: 21202.201051724063
Epoch: 22, Global Step: 228500, Data Step: 457000, Loss: 3.015625, Token per second per gpu: 22699.078448191536
Epoch: 22, Global Step: 228600, Data Step: 457200, Loss: 3.140625, Token per second per gpu: 22123.878590176686
Epoch: 22, Global Step: 228700, Data Step: 457400, Loss: 3.078125, Token per second per gpu: 22787.78331836678
Epoch: 22, Global Step: 228800, Data Step: 457600, Loss: 2.75, Token per second per gpu: 22234.951315459348
Epoch: 22, Global Step: 228900, Data Step: 457800, Loss: 2.8125, Token per second per gpu: 22430.54018820225
Epoch: 22, Global Step: 229000, Data Step: 458000, Loss: 3.0, Token per second per gpu: 21603.256375021454
Epoch: 22, Global Step: 229100, Data Step: 458200, Loss: 3.25, Token per second per gpu: 22137.74323147908
Epoch: 22, Global Step: 229200, Data Step: 458400, Loss: 3.4375, Token per second per gpu: 22441.440217320374
Epoch: 22, Global Step: 229300, Data Step: 458600, Loss: 3.234375, Token per second per gpu: 22606.914941611936
Epoch: 22, Global Step: 229400, Data Step: 458800, Loss: 3.234375, Token per second per gpu: 22451.53171383092
Epoch: 22, Global Step: 229500, Data Step: 459000, Loss: 3.09375, Token per second per gpu: 22263.548101411903
Epoch: 22, Global Step: 229600, Data Step: 459200, Loss: 2.734375, Token per second per gpu: 22357.35894299085
Epoch: 22, Global Step: 229700, Data Step: 459400, Loss: 3.171875, Token per second per gpu: 22169.65885197893
Epoch: 22, Global Step: 229800, Data Step: 459600, Loss: 3.328125, Token per second per gpu: 22527.337013609114
Epoch: 22, Global Step: 229900, Data Step: 459800, Loss: 3.1875, Token per second per gpu: 22506.76572678073
Epoch: 22, Global Step: 230000, Data Step: 460000, Loss: 2.890625, Token per second per gpu: 22393.488147122847
I0330 00:32:20.732060 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0330 00:32:20.732485 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-30 00:32:20,732] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-30 00:32:20,736] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-30 00:32:20,737] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-30 00:32:21,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-30 00:32:21,348] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-30 00:32:21,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-30 00:32:21,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-30 00:32:21,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-30 00:32:21,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-30 00:32:21,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-30 00:32:23,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-30 00:32:23,338] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-30 00:32:23,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 00:32:23,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-30 00:32:23,438] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-30 00:32:23,439] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 00:32:23,488] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-30 00:32:23,488] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-30 00:32:23,488] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 00:32:23,532] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-30 00:32:23,532] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-30 00:32:23,532] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 00:32:23,544] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-30 00:32:23,549] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-30 00:32:23,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 00:32:23,568] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-30 00:32:23,569] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-30 00:32:23,569] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0330 00:32:23.569870 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0330 00:32:23.586123 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0330 00:32:23.586441 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0330 00:32:23.588347 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 22, Global Step: 230100, Data Step: 460200, Loss: 3.234375, Token per second per gpu: 21858.05358524199
Epoch: 22, Global Step: 230200, Data Step: 460400, Loss: 3.109375, Token per second per gpu: 22243.673288821345
Epoch: 22, Global Step: 230300, Data Step: 460600, Loss: 3.046875, Token per second per gpu: 22238.411820833328
Epoch: 22, Global Step: 230400, Data Step: 460800, Loss: 3.203125, Token per second per gpu: 22272.509890579313
Epoch: 22, Global Step: 230500, Data Step: 461000, Loss: 2.75, Token per second per gpu: 22569.167252517782
Epoch: 22, Global Step: 230600, Data Step: 461200, Loss: 2.734375, Token per second per gpu: 22842.61563416128
Epoch: 22, Global Step: 230700, Data Step: 461400, Loss: 3.140625, Token per second per gpu: 25321.082910229128
Epoch: 22, Global Step: 230800, Data Step: 461600, Loss: 3.125, Token per second per gpu: 25266.28992753402
Epoch: 22, Global Step: 230900, Data Step: 461800, Loss: 3.03125, Token per second per gpu: 25185.64998625887
Epoch: 22, Global Step: 231000, Data Step: 462000, Loss: 2.96875, Token per second per gpu: 25178.967998313135
Epoch: 22, Global Step: 231100, Data Step: 462200, Loss: 3.421875, Token per second per gpu: 25189.043732807473
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0330 00:56:21.637901 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 23, Global Step: 231200, Data Step: 462400, Loss: 3.125, Token per second per gpu: 24565.680534964726
Epoch: 23, Global Step: 231300, Data Step: 462600, Loss: 3.4375, Token per second per gpu: 25212.022089075654
Epoch: 23, Global Step: 231400, Data Step: 462800, Loss: 2.671875, Token per second per gpu: 25205.20771467352
Epoch: 23, Global Step: 231500, Data Step: 463000, Loss: 3.234375, Token per second per gpu: 25204.86817866372
Epoch: 23, Global Step: 231600, Data Step: 463200, Loss: 2.984375, Token per second per gpu: 25202.03612315021
Epoch: 23, Global Step: 231700, Data Step: 463400, Loss: 3.09375, Token per second per gpu: 25201.871023683645
Epoch: 23, Global Step: 231800, Data Step: 463600, Loss: 3.046875, Token per second per gpu: 25265.066127187572
Epoch: 23, Global Step: 231900, Data Step: 463800, Loss: 3.125, Token per second per gpu: 25269.81169682271
Epoch: 23, Global Step: 232000, Data Step: 464000, Loss: 3.015625, Token per second per gpu: 25248.371499186407
Epoch: 23, Global Step: 232100, Data Step: 464200, Loss: 3.046875, Token per second per gpu: 25210.491897021286
Epoch: 23, Global Step: 232200, Data Step: 464400, Loss: 2.875, Token per second per gpu: 25183.627828022476
Epoch: 23, Global Step: 232300, Data Step: 464600, Loss: 2.796875, Token per second per gpu: 25194.572161587595
Epoch: 23, Global Step: 232400, Data Step: 464800, Loss: 2.921875, Token per second per gpu: 25200.921060251017
Epoch: 23, Global Step: 232500, Data Step: 465000, Loss: 3.59375, Token per second per gpu: 25273.36358367524
Epoch: 23, Global Step: 232600, Data Step: 465200, Loss: 3.125, Token per second per gpu: 24634.090083252104
Epoch: 23, Global Step: 232700, Data Step: 465400, Loss: 3.171875, Token per second per gpu: 25217.720124149575
Epoch: 23, Global Step: 232800, Data Step: 465600, Loss: 2.90625, Token per second per gpu: 25183.374713560792
Epoch: 23, Global Step: 232900, Data Step: 465800, Loss: 2.921875, Token per second per gpu: 25262.584963448542
Epoch: 23, Global Step: 233000, Data Step: 466000, Loss: 3.375, Token per second per gpu: 25250.46376496068
Epoch: 23, Global Step: 233100, Data Step: 466200, Loss: 3.296875, Token per second per gpu: 25212.548998853694
Epoch: 23, Global Step: 233200, Data Step: 466400, Loss: 3.4375, Token per second per gpu: 25191.155018410336
Epoch: 23, Global Step: 233300, Data Step: 466600, Loss: 2.765625, Token per second per gpu: 25178.95534977648
Epoch: 23, Global Step: 233400, Data Step: 466800, Loss: 2.75, Token per second per gpu: 25254.621617226945
Epoch: 23, Global Step: 233500, Data Step: 467000, Loss: 3.078125, Token per second per gpu: 25262.952843214367
Epoch: 23, Global Step: 233600, Data Step: 467200, Loss: 3.125, Token per second per gpu: 25225.128645311586
Epoch: 23, Global Step: 233700, Data Step: 467400, Loss: 3.109375, Token per second per gpu: 25194.53380112973
Epoch: 23, Global Step: 233800, Data Step: 467600, Loss: 3.03125, Token per second per gpu: 25212.0940754504
Epoch: 23, Global Step: 233900, Data Step: 467800, Loss: 3.109375, Token per second per gpu: 25262.73849634082
Epoch: 23, Global Step: 234000, Data Step: 468000, Loss: 2.828125, Token per second per gpu: 25259.532867512433
Epoch: 23, Global Step: 234100, Data Step: 468200, Loss: 3.0, Token per second per gpu: 25218.046528966526
Epoch: 23, Global Step: 234200, Data Step: 468400, Loss: 3.171875, Token per second per gpu: 25198.706789939002
Epoch: 23, Global Step: 234300, Data Step: 468600, Loss: 2.734375, Token per second per gpu: 25186.19538189348
Epoch: 23, Global Step: 234400, Data Step: 468800, Loss: 2.96875, Token per second per gpu: 25186.78691027295
Epoch: 23, Global Step: 234500, Data Step: 469000, Loss: 3.21875, Token per second per gpu: 25190.969100544306
Epoch: 23, Global Step: 234600, Data Step: 469200, Loss: 2.84375, Token per second per gpu: 25188.232291053864
Epoch: 23, Global Step: 234700, Data Step: 469400, Loss: 3.09375, Token per second per gpu: 25216.322891163923
Epoch: 23, Global Step: 234800, Data Step: 469600, Loss: 2.96875, Token per second per gpu: 25282.864595583836
Epoch: 23, Global Step: 234900, Data Step: 469800, Loss: 3.078125, Token per second per gpu: 25268.648764844453
Epoch: 23, Global Step: 235000, Data Step: 470000, Loss: 3.046875, Token per second per gpu: 25235.84753597322
Epoch: 23, Global Step: 235100, Data Step: 470200, Loss: 3.203125, Token per second per gpu: 25218.207260120722
Epoch: 23, Global Step: 235200, Data Step: 470400, Loss: 3.0625, Token per second per gpu: 25209.930664442243
Epoch: 23, Global Step: 235300, Data Step: 470600, Loss: 3.265625, Token per second per gpu: 25222.925761248796
Epoch: 23, Global Step: 235400, Data Step: 470800, Loss: 3.125, Token per second per gpu: 25226.9248207059
Epoch: 23, Global Step: 235500, Data Step: 471000, Loss: 2.765625, Token per second per gpu: 25214.212753283657
Epoch: 23, Global Step: 235600, Data Step: 471200, Loss: 3.03125, Token per second per gpu: 25226.203232765165
Epoch: 23, Global Step: 235700, Data Step: 471400, Loss: 3.1875, Token per second per gpu: 25227.249777927504
Epoch: 23, Global Step: 235800, Data Step: 471600, Loss: 3.125, Token per second per gpu: 25253.91090392845
Epoch: 23, Global Step: 235900, Data Step: 471800, Loss: 2.984375, Token per second per gpu: 25245.82787073594
Epoch: 23, Global Step: 236000, Data Step: 472000, Loss: 3.109375, Token per second per gpu: 25234.702277583554
Epoch: 23, Global Step: 236100, Data Step: 472200, Loss: 3.171875, Token per second per gpu: 24594.1458382041
Epoch: 23, Global Step: 236200, Data Step: 472400, Loss: 3.34375, Token per second per gpu: 25231.15460810753
Epoch: 23, Global Step: 236300, Data Step: 472600, Loss: 3.15625, Token per second per gpu: 25199.687655267244
Epoch: 23, Global Step: 236400, Data Step: 472800, Loss: 3.109375, Token per second per gpu: 25184.5984360512
Epoch: 23, Global Step: 236500, Data Step: 473000, Loss: 2.796875, Token per second per gpu: 25231.48504998865
Epoch: 23, Global Step: 236600, Data Step: 473200, Loss: 3.390625, Token per second per gpu: 25268.283097718784
Epoch: 23, Global Step: 236700, Data Step: 473400, Loss: 3.0625, Token per second per gpu: 25241.091251372174
Epoch: 23, Global Step: 236800, Data Step: 473600, Loss: 2.9375, Token per second per gpu: 25178.624183069067
Epoch: 23, Global Step: 236900, Data Step: 473800, Loss: 3.09375, Token per second per gpu: 25175.204228647253
Epoch: 23, Global Step: 237000, Data Step: 474000, Loss: 3.15625, Token per second per gpu: 25162.084966594975
Epoch: 23, Global Step: 237100, Data Step: 474200, Loss: 3.109375, Token per second per gpu: 25231.70671967998
Epoch: 23, Global Step: 237200, Data Step: 474400, Loss: 3.09375, Token per second per gpu: 25259.722439845435
Epoch: 23, Global Step: 237300, Data Step: 474600, Loss: 3.265625, Token per second per gpu: 25227.515102645393
Epoch: 23, Global Step: 237400, Data Step: 474800, Loss: 2.953125, Token per second per gpu: 25193.72463288391
Epoch: 23, Global Step: 237500, Data Step: 475000, Loss: 2.8125, Token per second per gpu: 25189.07419768856
Epoch: 23, Global Step: 237600, Data Step: 475200, Loss: 3.3125, Token per second per gpu: 25194.54294457073
Epoch: 23, Global Step: 237700, Data Step: 475400, Loss: 2.703125, Token per second per gpu: 25262.60873820651
Epoch: 23, Global Step: 237800, Data Step: 475600, Loss: 3.25, Token per second per gpu: 25267.18024041682
Epoch: 23, Global Step: 237900, Data Step: 475800, Loss: 2.75, Token per second per gpu: 25224.07632421266
Epoch: 23, Global Step: 238000, Data Step: 476000, Loss: 3.09375, Token per second per gpu: 25174.635856810095
Epoch: 23, Global Step: 238100, Data Step: 476200, Loss: 3.15625, Token per second per gpu: 25200.524492212666
Epoch: 23, Global Step: 238200, Data Step: 476400, Loss: 3.1875, Token per second per gpu: 25260.913129022556
Epoch: 23, Global Step: 238300, Data Step: 476600, Loss: 2.890625, Token per second per gpu: 24619.25093294545
Epoch: 23, Global Step: 238400, Data Step: 476800, Loss: 3.078125, Token per second per gpu: 25219.883398019316
Epoch: 23, Global Step: 238500, Data Step: 477000, Loss: 3.078125, Token per second per gpu: 25188.909268005867
Epoch: 23, Global Step: 238600, Data Step: 477200, Loss: 2.96875, Token per second per gpu: 25250.666080212854
Epoch: 23, Global Step: 238700, Data Step: 477400, Loss: 3.234375, Token per second per gpu: 25253.267700903303
Epoch: 23, Global Step: 238800, Data Step: 477600, Loss: 3.203125, Token per second per gpu: 25204.85723961353
Epoch: 23, Global Step: 238900, Data Step: 477800, Loss: 2.875, Token per second per gpu: 25193.03089880628
Epoch: 23, Global Step: 239000, Data Step: 478000, Loss: 3.015625, Token per second per gpu: 24542.790536555225
Epoch: 23, Global Step: 239100, Data Step: 478200, Loss: 3.140625, Token per second per gpu: 25180.599503652502
Epoch: 23, Global Step: 239200, Data Step: 478400, Loss: 3.203125, Token per second per gpu: 25247.778130011404
Epoch: 23, Global Step: 239300, Data Step: 478600, Loss: 3.140625, Token per second per gpu: 25240.230622360305
Epoch: 23, Global Step: 239400, Data Step: 478800, Loss: 3.203125, Token per second per gpu: 25205.60674428804
Epoch: 23, Global Step: 239500, Data Step: 479000, Loss: 3.03125, Token per second per gpu: 25192.62948261595
Epoch: 23, Global Step: 239600, Data Step: 479200, Loss: 2.953125, Token per second per gpu: 25227.085032842337
Epoch: 23, Global Step: 239700, Data Step: 479400, Loss: 3.296875, Token per second per gpu: 25232.097734272043
Epoch: 23, Global Step: 239800, Data Step: 479600, Loss: 2.90625, Token per second per gpu: 25207.27599998786
Epoch: 23, Global Step: 239900, Data Step: 479800, Loss: 2.75, Token per second per gpu: 25223.541874793085
Epoch: 23, Global Step: 240000, Data Step: 480000, Loss: 3.109375, Token per second per gpu: 25243.74344215384
I0330 03:44:22.860333 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0330 03:44:22.860724 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-30 03:44:22,861] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-30 03:44:22,865] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-30 03:44:22,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-30 03:44:24,492] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-30 03:44:24,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-30 03:44:24,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-30 03:44:24,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-30 03:44:24,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-30 03:44:24,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-30 03:44:24,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-30 03:44:26,466] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-30 03:44:26,466] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-30 03:44:26,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 03:44:26,604] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-30 03:44:26,605] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-30 03:44:26,605] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 03:44:26,653] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-30 03:44:26,653] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-30 03:44:26,653] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 03:44:26,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-30 03:44:26,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-30 03:44:26,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-30 03:44:26,666] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-30 03:44:26,666] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-30 03:44:26,666] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 03:44:26,666] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 03:44:26,667] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-30 03:44:26,667] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0330 03:44:26.668303 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0330 03:44:26.669130 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0330 03:44:26.669456 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0330 03:44:26.671378 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 23, Global Step: 240100, Data Step: 480200, Loss: 3.328125, Token per second per gpu: 24411.511252036355
Epoch: 23, Global Step: 240200, Data Step: 480400, Loss: 3.171875, Token per second per gpu: 25199.187304004045
Epoch: 23, Global Step: 240300, Data Step: 480600, Loss: 2.984375, Token per second per gpu: 25192.730308332993
Epoch: 23, Global Step: 240400, Data Step: 480800, Loss: 2.8125, Token per second per gpu: 25239.858815871656
Epoch: 23, Global Step: 240500, Data Step: 481000, Loss: 3.03125, Token per second per gpu: 25272.0971700153
Epoch: 23, Global Step: 240600, Data Step: 481200, Loss: 3.078125, Token per second per gpu: 25253.07373803892
Epoch: 23, Global Step: 240700, Data Step: 481400, Loss: 2.921875, Token per second per gpu: 25212.159431857188
Epoch: 23, Global Step: 240800, Data Step: 481600, Loss: 2.984375, Token per second per gpu: 25220.085065560397
Epoch: 23, Global Step: 240900, Data Step: 481800, Loss: 3.078125, Token per second per gpu: 25204.82515879994
Epoch: 23, Global Step: 241000, Data Step: 482000, Loss: 2.875, Token per second per gpu: 25258.40066915926
Epoch: 23, Global Step: 241100, Data Step: 482200, Loss: 3.171875, Token per second per gpu: 25263.974963800614
Epoch: 23, Global Step: 241200, Data Step: 482400, Loss: 3.171875, Token per second per gpu: 25249.07546202234
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0330 04:07:57.266836 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 24, Global Step: 241300, Data Step: 482600, Loss: 3.28125, Token per second per gpu: 24591.240595466945
Epoch: 24, Global Step: 241400, Data Step: 482800, Loss: 2.8125, Token per second per gpu: 25197.23308596095
Epoch: 24, Global Step: 241500, Data Step: 483000, Loss: 3.078125, Token per second per gpu: 25209.672759435034
Epoch: 24, Global Step: 241600, Data Step: 483200, Loss: 3.09375, Token per second per gpu: 25271.91814486361
Epoch: 24, Global Step: 241700, Data Step: 483400, Loss: 3.328125, Token per second per gpu: 25265.290184021174
Epoch: 24, Global Step: 241800, Data Step: 483600, Loss: 3.125, Token per second per gpu: 25193.25504592538
Epoch: 24, Global Step: 241900, Data Step: 483800, Loss: 3.421875, Token per second per gpu: 25161.33830999333
Epoch: 24, Global Step: 242000, Data Step: 484000, Loss: 3.03125, Token per second per gpu: 25182.38934295297
Epoch: 24, Global Step: 242100, Data Step: 484200, Loss: 2.765625, Token per second per gpu: 25227.10737103287
Epoch: 24, Global Step: 242200, Data Step: 484400, Loss: 2.953125, Token per second per gpu: 25274.111192677017
Epoch: 24, Global Step: 242300, Data Step: 484600, Loss: 3.1875, Token per second per gpu: 25255.026278640664
Epoch: 24, Global Step: 242400, Data Step: 484800, Loss: 3.09375, Token per second per gpu: 25213.102927755255
Epoch: 24, Global Step: 242500, Data Step: 485000, Loss: 2.84375, Token per second per gpu: 25185.751176211877
Epoch: 24, Global Step: 242600, Data Step: 485200, Loss: 2.8125, Token per second per gpu: 25229.755095610493
Epoch: 24, Global Step: 242700, Data Step: 485400, Loss: 2.765625, Token per second per gpu: 25262.197969514218
Epoch: 24, Global Step: 242800, Data Step: 485600, Loss: 3.0625, Token per second per gpu: 25218.670828037
Epoch: 24, Global Step: 242900, Data Step: 485800, Loss: 3.140625, Token per second per gpu: 25219.449902705826
Epoch: 24, Global Step: 243000, Data Step: 486000, Loss: 2.953125, Token per second per gpu: 25205.904433763135
Epoch: 24, Global Step: 243100, Data Step: 486200, Loss: 2.78125, Token per second per gpu: 25208.72746853325
Epoch: 24, Global Step: 243200, Data Step: 486400, Loss: 2.9375, Token per second per gpu: 25267.532292281583
Epoch: 24, Global Step: 243300, Data Step: 486600, Loss: 3.078125, Token per second per gpu: 25269.537710982313
Epoch: 24, Global Step: 243400, Data Step: 486800, Loss: 3.125, Token per second per gpu: 25231.31424154965
Epoch: 24, Global Step: 243500, Data Step: 487000, Loss: 3.296875, Token per second per gpu: 25199.366772112997
Epoch: 24, Global Step: 243600, Data Step: 487200, Loss: 3.125, Token per second per gpu: 25196.87200563321
Epoch: 24, Global Step: 243700, Data Step: 487400, Loss: 3.046875, Token per second per gpu: 24581.233014038335
Epoch: 24, Global Step: 243800, Data Step: 487600, Loss: 3.15625, Token per second per gpu: 25198.93576908506
Epoch: 24, Global Step: 243900, Data Step: 487800, Loss: 2.734375, Token per second per gpu: 25229.86327997267
Epoch: 24, Global Step: 244000, Data Step: 488000, Loss: 2.953125, Token per second per gpu: 25267.13336080808
Epoch: 24, Global Step: 244100, Data Step: 488200, Loss: 3.34375, Token per second per gpu: 25254.65778489708
Epoch: 24, Global Step: 244200, Data Step: 488400, Loss: 3.3125, Token per second per gpu: 25225.497647447282
Epoch: 24, Global Step: 244300, Data Step: 488600, Loss: 3.390625, Token per second per gpu: 25184.474204969527
Epoch: 24, Global Step: 244400, Data Step: 488800, Loss: 3.125, Token per second per gpu: 25199.33197171165
Epoch: 24, Global Step: 244500, Data Step: 489000, Loss: 2.8125, Token per second per gpu: 25227.57210920076
Epoch: 24, Global Step: 244600, Data Step: 489200, Loss: 3.5, Token per second per gpu: 25259.63032075257
Epoch: 24, Global Step: 244700, Data Step: 489400, Loss: 3.3125, Token per second per gpu: 25261.098971389652
Epoch: 24, Global Step: 244800, Data Step: 489600, Loss: 3.25, Token per second per gpu: 25257.46703575864
Epoch: 24, Global Step: 244900, Data Step: 489800, Loss: 3.453125, Token per second per gpu: 25251.14192706462
Epoch: 24, Global Step: 245000, Data Step: 490000, Loss: 3.28125, Token per second per gpu: 25225.08229034619
Epoch: 24, Global Step: 245100, Data Step: 490200, Loss: 3.265625, Token per second per gpu: 25219.836483134713
Epoch: 24, Global Step: 245200, Data Step: 490400, Loss: 3.09375, Token per second per gpu: 25223.221753228132
Epoch: 24, Global Step: 245300, Data Step: 490600, Loss: 3.265625, Token per second per gpu: 25242.78931721734
Epoch: 24, Global Step: 245400, Data Step: 490800, Loss: 3.1875, Token per second per gpu: 25227.39392505642
Epoch: 24, Global Step: 245500, Data Step: 491000, Loss: 3.015625, Token per second per gpu: 25222.817636373762
Epoch: 24, Global Step: 245600, Data Step: 491200, Loss: 2.90625, Token per second per gpu: 25204.60890469214
Epoch: 24, Global Step: 245700, Data Step: 491400, Loss: 3.296875, Token per second per gpu: 25184.46932186244
Epoch: 24, Global Step: 245800, Data Step: 491600, Loss: 2.9375, Token per second per gpu: 25181.630089513354
Epoch: 24, Global Step: 245900, Data Step: 491800, Loss: 2.875, Token per second per gpu: 25258.956720306054
Epoch: 24, Global Step: 246000, Data Step: 492000, Loss: 2.9375, Token per second per gpu: 24639.74169636562
Epoch: 24, Global Step: 246100, Data Step: 492200, Loss: 3.125, Token per second per gpu: 25237.71367999604
Epoch: 24, Global Step: 246200, Data Step: 492400, Loss: 3.359375, Token per second per gpu: 25243.18784960217
Epoch: 24, Global Step: 246300, Data Step: 492600, Loss: 2.84375, Token per second per gpu: 25271.471121780112
Epoch: 24, Global Step: 246400, Data Step: 492800, Loss: 3.1875, Token per second per gpu: 25266.880309058095
Epoch: 24, Global Step: 246500, Data Step: 493000, Loss: 3.453125, Token per second per gpu: 25220.74664198681
Epoch: 24, Global Step: 246600, Data Step: 493200, Loss: 3.515625, Token per second per gpu: 25220.741639487867
Epoch: 24, Global Step: 246700, Data Step: 493400, Loss: 3.140625, Token per second per gpu: 25219.98791721698
Epoch: 24, Global Step: 246800, Data Step: 493600, Loss: 2.6875, Token per second per gpu: 25214.759597219054
Epoch: 24, Global Step: 246900, Data Step: 493800, Loss: 3.0625, Token per second per gpu: 25232.44364392533
Epoch: 24, Global Step: 247000, Data Step: 494000, Loss: 3.09375, Token per second per gpu: 25239.880332884717
Epoch: 24, Global Step: 247100, Data Step: 494200, Loss: 3.296875, Token per second per gpu: 25229.447884481837
Epoch: 24, Global Step: 247200, Data Step: 494400, Loss: 2.859375, Token per second per gpu: 25219.1068764688
Epoch: 24, Global Step: 247300, Data Step: 494600, Loss: 3.265625, Token per second per gpu: 25214.015705806105
Epoch: 24, Global Step: 247400, Data Step: 494800, Loss: 3.0625, Token per second per gpu: 25202.51918296188
Epoch: 24, Global Step: 247500, Data Step: 495000, Loss: 3.015625, Token per second per gpu: 25273.713428300554
Epoch: 24, Global Step: 247600, Data Step: 495200, Loss: 3.34375, Token per second per gpu: 25265.481111058038
Epoch: 24, Global Step: 247700, Data Step: 495400, Loss: 2.984375, Token per second per gpu: 25216.718324711637
Epoch: 24, Global Step: 247800, Data Step: 495600, Loss: 3.03125, Token per second per gpu: 25201.449924679982
Epoch: 24, Global Step: 247900, Data Step: 495800, Loss: 3.171875, Token per second per gpu: 25271.20080076908
Epoch: 24, Global Step: 248000, Data Step: 496000, Loss: 3.28125, Token per second per gpu: 25268.646756236365
Epoch: 24, Global Step: 248100, Data Step: 496200, Loss: 3.140625, Token per second per gpu: 25238.195999255968
Epoch: 24, Global Step: 248200, Data Step: 496400, Loss: 3.25, Token per second per gpu: 24599.11572178055
Epoch: 24, Global Step: 248300, Data Step: 496600, Loss: 3.234375, Token per second per gpu: 25191.337419164458
Epoch: 24, Global Step: 248400, Data Step: 496800, Loss: 2.953125, Token per second per gpu: 25206.84004497869
Epoch: 24, Global Step: 248500, Data Step: 497000, Loss: 3.28125, Token per second per gpu: 25264.892802929233
Epoch: 24, Global Step: 248600, Data Step: 497200, Loss: 2.953125, Token per second per gpu: 25215.92057447164
Epoch: 24, Global Step: 248700, Data Step: 497400, Loss: 3.0625, Token per second per gpu: 25184.9191532334
Epoch: 24, Global Step: 248800, Data Step: 497600, Loss: 3.03125, Token per second per gpu: 25179.49909087083
Epoch: 24, Global Step: 248900, Data Step: 497800, Loss: 3.109375, Token per second per gpu: 24612.408407881052
Epoch: 24, Global Step: 249000, Data Step: 498000, Loss: 3.234375, Token per second per gpu: 25239.866673796194
Epoch: 24, Global Step: 249100, Data Step: 498200, Loss: 2.96875, Token per second per gpu: 25184.303244842395
Epoch: 24, Global Step: 249200, Data Step: 498400, Loss: 3.25, Token per second per gpu: 25173.100232178153
Epoch: 24, Global Step: 249300, Data Step: 498600, Loss: 3.234375, Token per second per gpu: 25181.819439245686
Epoch: 24, Global Step: 249400, Data Step: 498800, Loss: 2.90625, Token per second per gpu: 25215.762399044674
Epoch: 24, Global Step: 249500, Data Step: 499000, Loss: 3.34375, Token per second per gpu: 25279.62989181691
Epoch: 24, Global Step: 249600, Data Step: 499200, Loss: 3.046875, Token per second per gpu: 25208.626567336232
Epoch: 24, Global Step: 249700, Data Step: 499400, Loss: 2.828125, Token per second per gpu: 25166.599970165746
Epoch: 24, Global Step: 249800, Data Step: 499600, Loss: 2.875, Token per second per gpu: 25192.975992302538
Epoch: 24, Global Step: 249900, Data Step: 499800, Loss: 3.09375, Token per second per gpu: 25283.711887722093
Epoch: 24, Global Step: 250000, Data Step: 500000, Loss: 3.28125, Token per second per gpu: 25229.054422768124
I0330 06:54:57.701921 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0330 06:54:57.702275 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-30 06:54:57,702] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-30 06:54:57,706] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-30 06:54:57,706] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-30 06:54:58,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-30 06:54:58,193] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-30 06:54:58,193] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-30 06:54:58,194] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-30 06:54:58,194] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-30 06:54:58,194] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-30 06:54:58,194] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-30 06:55:00,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-30 06:55:00,335] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-30 06:55:00,335] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 06:55:00,419] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-30 06:55:00,420] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-30 06:55:00,420] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 06:55:00,448] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-30 06:55:00,448] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-30 06:55:00,448] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-30 06:55:00,449] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 06:55:00,459] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-30 06:55:00,459] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 06:55:00,470] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-30 06:55:00,470] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-30 06:55:00,470] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 06:55:00,480] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-30 06:55:00,480] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-30 06:55:00,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0330 06:55:00.480571 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0330 06:55:00.481116 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0330 06:55:00.481317 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0330 06:55:00.482482 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 24, Global Step: 250100, Data Step: 500200, Loss: 2.953125, Token per second per gpu: 24562.334752656134
Epoch: 24, Global Step: 250200, Data Step: 500400, Loss: 2.890625, Token per second per gpu: 25188.356664259958
Epoch: 24, Global Step: 250300, Data Step: 500600, Loss: 3.125, Token per second per gpu: 25257.30036426554
Epoch: 24, Global Step: 250400, Data Step: 500800, Loss: 3.125, Token per second per gpu: 25212.600043915223
Epoch: 24, Global Step: 250500, Data Step: 501000, Loss: 3.15625, Token per second per gpu: 25177.455827233083
Epoch: 24, Global Step: 250600, Data Step: 501200, Loss: 3.171875, Token per second per gpu: 25170.114455759936
Epoch: 24, Global Step: 250700, Data Step: 501400, Loss: 3.171875, Token per second per gpu: 25161.495017202073
Epoch: 24, Global Step: 250800, Data Step: 501600, Loss: 3.328125, Token per second per gpu: 25170.06903701838
Epoch: 24, Global Step: 250900, Data Step: 501800, Loss: 3.171875, Token per second per gpu: 25173.359330448577
Epoch: 24, Global Step: 251000, Data Step: 502000, Loss: 3.0625, Token per second per gpu: 25173.848164102306
Epoch: 24, Global Step: 251100, Data Step: 502200, Loss: 3.25, Token per second per gpu: 25165.436085288344
Epoch: 24, Global Step: 251200, Data Step: 502400, Loss: 2.953125, Token per second per gpu: 25162.437764937782
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0330 07:19:32.776843 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 25, Global Step: 251300, Data Step: 502600, Loss: 2.84375, Token per second per gpu: 24559.40596208285
Epoch: 25, Global Step: 251400, Data Step: 502800, Loss: 2.90625, Token per second per gpu: 25195.595062635188
Epoch: 25, Global Step: 251500, Data Step: 503000, Loss: 2.90625, Token per second per gpu: 25204.913249754325
Epoch: 25, Global Step: 251600, Data Step: 503200, Loss: 3.328125, Token per second per gpu: 25209.631722405215
Epoch: 25, Global Step: 251700, Data Step: 503400, Loss: 3.078125, Token per second per gpu: 25193.891990193526
Epoch: 25, Global Step: 251800, Data Step: 503600, Loss: 3.375, Token per second per gpu: 25203.961163801996
Epoch: 25, Global Step: 251900, Data Step: 503800, Loss: 3.1875, Token per second per gpu: 25209.408914709857
Epoch: 25, Global Step: 252000, Data Step: 504000, Loss: 3.15625, Token per second per gpu: 25226.925136808077
Epoch: 25, Global Step: 252100, Data Step: 504200, Loss: 2.90625, Token per second per gpu: 25235.662855764567
Epoch: 25, Global Step: 252200, Data Step: 504400, Loss: 3.015625, Token per second per gpu: 25213.73898140193
Epoch: 25, Global Step: 252300, Data Step: 504600, Loss: 3.140625, Token per second per gpu: 25219.781196457712
Epoch: 25, Global Step: 252400, Data Step: 504800, Loss: 2.921875, Token per second per gpu: 25221.78035881418
Epoch: 25, Global Step: 252500, Data Step: 505000, Loss: 2.859375, Token per second per gpu: 25217.46916670784
Epoch: 25, Global Step: 252600, Data Step: 505200, Loss: 2.78125, Token per second per gpu: 25196.18498296806
Epoch: 25, Global Step: 252700, Data Step: 505400, Loss: 2.953125, Token per second per gpu: 25206.56379364944
Epoch: 25, Global Step: 252800, Data Step: 505600, Loss: 2.96875, Token per second per gpu: 25293.48978067651
Epoch: 25, Global Step: 252900, Data Step: 505800, Loss: 3.125, Token per second per gpu: 25258.71281111825
Epoch: 25, Global Step: 253000, Data Step: 506000, Loss: 3.234375, Token per second per gpu: 25202.744024779066
Epoch: 25, Global Step: 253100, Data Step: 506200, Loss: 2.9375, Token per second per gpu: 25179.59897173497
Epoch: 25, Global Step: 253200, Data Step: 506400, Loss: 2.96875, Token per second per gpu: 25258.199867191786
Epoch: 25, Global Step: 253300, Data Step: 506600, Loss: 3.3125, Token per second per gpu: 25236.172617665805
Epoch: 25, Global Step: 253400, Data Step: 506800, Loss: 3.28125, Token per second per gpu: 25203.422675976544
Epoch: 25, Global Step: 253500, Data Step: 507000, Loss: 2.75, Token per second per gpu: 25170.688602645005
Epoch: 25, Global Step: 253600, Data Step: 507200, Loss: 3.0625, Token per second per gpu: 25227.361049250434
Epoch: 25, Global Step: 253700, Data Step: 507400, Loss: 2.9375, Token per second per gpu: 25266.675937002547
Epoch: 25, Global Step: 253800, Data Step: 507600, Loss: 2.9375, Token per second per gpu: 25196.59886338717
Epoch: 25, Global Step: 253900, Data Step: 507800, Loss: 3.109375, Token per second per gpu: 25174.752750536994
Epoch: 25, Global Step: 254000, Data Step: 508000, Loss: 3.390625, Token per second per gpu: 25164.518695444836
Epoch: 25, Global Step: 254100, Data Step: 508200, Loss: 2.921875, Token per second per gpu: 25233.41638448179
Epoch: 25, Global Step: 254200, Data Step: 508400, Loss: 3.0, Token per second per gpu: 25240.742520251006
Epoch: 25, Global Step: 254300, Data Step: 508600, Loss: 2.75, Token per second per gpu: 25192.46718590567
Epoch: 25, Global Step: 254400, Data Step: 508800, Loss: 3.328125, Token per second per gpu: 25158.22888793766
Epoch: 25, Global Step: 254500, Data Step: 509000, Loss: 2.765625, Token per second per gpu: 25202.61567094909
Epoch: 25, Global Step: 254600, Data Step: 509200, Loss: 2.984375, Token per second per gpu: 25245.43484765621
Epoch: 25, Global Step: 254700, Data Step: 509400, Loss: 3.421875, Token per second per gpu: 25206.75578031757
Epoch: 25, Global Step: 254800, Data Step: 509600, Loss: 3.21875, Token per second per gpu: 24553.907677516814
Epoch: 25, Global Step: 254900, Data Step: 509800, Loss: 2.734375, Token per second per gpu: 25150.272276741278
Epoch: 25, Global Step: 255000, Data Step: 510000, Loss: 3.0, Token per second per gpu: 25222.49863916096
Epoch: 25, Global Step: 255100, Data Step: 510200, Loss: 3.109375, Token per second per gpu: 25267.127547114495
Epoch: 25, Global Step: 255200, Data Step: 510400, Loss: 2.84375, Token per second per gpu: 25206.305852330213
Epoch: 25, Global Step: 255300, Data Step: 510600, Loss: 2.90625, Token per second per gpu: 25181.626572370325
Epoch: 25, Global Step: 255400, Data Step: 510800, Loss: 3.203125, Token per second per gpu: 25173.384668751605
Epoch: 25, Global Step: 255500, Data Step: 511000, Loss: 2.875, Token per second per gpu: 25175.70457240513
Epoch: 25, Global Step: 255600, Data Step: 511200, Loss: 3.125, Token per second per gpu: 25160.638025576958
Epoch: 25, Global Step: 255700, Data Step: 511400, Loss: 3.328125, Token per second per gpu: 25163.0959010194
Epoch: 25, Global Step: 255800, Data Step: 511600, Loss: 3.1875, Token per second per gpu: 24550.520693230334
Epoch: 25, Global Step: 255900, Data Step: 511800, Loss: 2.9375, Token per second per gpu: 25266.545979869137
Epoch: 25, Global Step: 256000, Data Step: 512000, Loss: 3.03125, Token per second per gpu: 25240.263689935895
Epoch: 25, Global Step: 256100, Data Step: 512200, Loss: 3.359375, Token per second per gpu: 25195.631271648737
Epoch: 25, Global Step: 256200, Data Step: 512400, Loss: 3.0, Token per second per gpu: 25188.85033505702
Epoch: 25, Global Step: 256300, Data Step: 512600, Loss: 3.046875, Token per second per gpu: 25191.082626216506
Epoch: 25, Global Step: 256400, Data Step: 512800, Loss: 2.78125, Token per second per gpu: 25186.520340955252
Epoch: 25, Global Step: 256500, Data Step: 513000, Loss: 3.234375, Token per second per gpu: 25203.051795351417
Epoch: 25, Global Step: 256600, Data Step: 513200, Loss: 3.125, Token per second per gpu: 25206.65258054506
Epoch: 25, Global Step: 256700, Data Step: 513400, Loss: 3.046875, Token per second per gpu: 25210.625013880133
Epoch: 25, Global Step: 256800, Data Step: 513600, Loss: 2.828125, Token per second per gpu: 25232.58621648766
Epoch: 25, Global Step: 256900, Data Step: 513800, Loss: 3.078125, Token per second per gpu: 25248.2738689367
Epoch: 25, Global Step: 257000, Data Step: 514000, Loss: 3.015625, Token per second per gpu: 25235.00840418679
Epoch: 25, Global Step: 257100, Data Step: 514200, Loss: 2.875, Token per second per gpu: 25196.824650742117
Epoch: 25, Global Step: 257200, Data Step: 514400, Loss: 3.03125, Token per second per gpu: 25180.164785808312
Epoch: 25, Global Step: 257300, Data Step: 514600, Loss: 2.8125, Token per second per gpu: 25173.718268267945
Epoch: 25, Global Step: 257400, Data Step: 514800, Loss: 3.015625, Token per second per gpu: 25173.57531095288
Epoch: 25, Global Step: 257500, Data Step: 515000, Loss: 2.828125, Token per second per gpu: 25171.461725699468
Epoch: 25, Global Step: 257600, Data Step: 515200, Loss: 3.03125, Token per second per gpu: 25212.327349911684
Epoch: 25, Global Step: 257700, Data Step: 515400, Loss: 3.453125, Token per second per gpu: 25254.517285958933
Epoch: 25, Global Step: 257800, Data Step: 515600, Loss: 2.921875, Token per second per gpu: 25212.607095517295
Epoch: 25, Global Step: 257900, Data Step: 515800, Loss: 3.09375, Token per second per gpu: 25183.868766635227
Epoch: 25, Global Step: 258000, Data Step: 516000, Loss: 3.25, Token per second per gpu: 25173.639628428387
Epoch: 25, Global Step: 258100, Data Step: 516200, Loss: 3.0625, Token per second per gpu: 25209.71879472364
Epoch: 25, Global Step: 258200, Data Step: 516400, Loss: 2.96875, Token per second per gpu: 24622.963523806426
Epoch: 25, Global Step: 258300, Data Step: 516600, Loss: 3.0625, Token per second per gpu: 25201.799463726973
Epoch: 25, Global Step: 258400, Data Step: 516800, Loss: 3.28125, Token per second per gpu: 25170.70334084262
Epoch: 25, Global Step: 258500, Data Step: 517000, Loss: 2.921875, Token per second per gpu: 25160.61229367102
Epoch: 25, Global Step: 258600, Data Step: 517200, Loss: 2.921875, Token per second per gpu: 25212.585730227634
Epoch: 25, Global Step: 258700, Data Step: 517400, Loss: 2.921875, Token per second per gpu: 25257.55079482361
Epoch: 25, Global Step: 258800, Data Step: 517600, Loss: 3.015625, Token per second per gpu: 25219.889032029543
Epoch: 25, Global Step: 258900, Data Step: 517800, Loss: 2.828125, Token per second per gpu: 24522.11236870565
Epoch: 25, Global Step: 259000, Data Step: 518000, Loss: 3.203125, Token per second per gpu: 25187.344381881685
Epoch: 25, Global Step: 259100, Data Step: 518200, Loss: 3.203125, Token per second per gpu: 25280.019959495196
Epoch: 25, Global Step: 259200, Data Step: 518400, Loss: 3.09375, Token per second per gpu: 25239.962393202655
Epoch: 25, Global Step: 259300, Data Step: 518600, Loss: 3.25, Token per second per gpu: 25161.702513128348
Epoch: 25, Global Step: 259400, Data Step: 518800, Loss: 2.9375, Token per second per gpu: 25173.42794857972
Epoch: 25, Global Step: 259500, Data Step: 519000, Loss: 3.046875, Token per second per gpu: 25250.871882435094
Epoch: 25, Global Step: 259600, Data Step: 519200, Loss: 3.0, Token per second per gpu: 25234.431951879837
Epoch: 25, Global Step: 259700, Data Step: 519400, Loss: 3.296875, Token per second per gpu: 25170.537340432853
Epoch: 25, Global Step: 259800, Data Step: 519600, Loss: 3.1875, Token per second per gpu: 25178.094649336083
Epoch: 25, Global Step: 259900, Data Step: 519800, Loss: 2.984375, Token per second per gpu: 25253.69634089283
Epoch: 25, Global Step: 260000, Data Step: 520000, Loss: 3.28125, Token per second per gpu: 25223.88460032182
I0330 10:05:41.814013 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0330 10:05:41.814429 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-30 10:05:41,814] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-30 10:05:41,819] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-30 10:05:41,819] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-30 10:05:42,232] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-30 10:05:42,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-30 10:05:42,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-30 10:05:42,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-30 10:05:42,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-30 10:05:42,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-30 10:05:42,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-30 10:05:44,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-30 10:05:44,430] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-30 10:05:44,430] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 10:05:44,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-30 10:05:44,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-30 10:05:44,481] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-30 10:05:44,481] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-30 10:05:44,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 10:05:44,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 10:05:44,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-30 10:05:44,496] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-30 10:05:44,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 10:05:44,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-30 10:05:44,496] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-30 10:05:44,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 10:05:44,503] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-30 10:05:44,503] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-30 10:05:44,504] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0330 10:05:44.504324 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0330 10:05:44.505034 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0330 10:05:44.505286 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0330 10:05:44.506796 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 25, Global Step: 260100, Data Step: 520200, Loss: 3.078125, Token per second per gpu: 24627.428696955772
Epoch: 25, Global Step: 260200, Data Step: 520400, Loss: 3.359375, Token per second per gpu: 25230.0305907864
Epoch: 25, Global Step: 260300, Data Step: 520600, Loss: 3.203125, Token per second per gpu: 25253.960268725434
Epoch: 25, Global Step: 260400, Data Step: 520800, Loss: 3.0, Token per second per gpu: 25213.77366372064
Epoch: 25, Global Step: 260500, Data Step: 521000, Loss: 2.984375, Token per second per gpu: 25187.679559451066
Epoch: 25, Global Step: 260600, Data Step: 521200, Loss: 2.9375, Token per second per gpu: 25180.267821293422
Epoch: 25, Global Step: 260700, Data Step: 521400, Loss: 3.390625, Token per second per gpu: 25178.249256189185
Epoch: 25, Global Step: 260800, Data Step: 521600, Loss: 3.1875, Token per second per gpu: 25182.464887578524
Epoch: 25, Global Step: 260900, Data Step: 521800, Loss: 2.84375, Token per second per gpu: 25168.79716903558
Epoch: 25, Global Step: 261000, Data Step: 522000, Loss: 2.84375, Token per second per gpu: 25227.43939285879
Epoch: 25, Global Step: 261100, Data Step: 522200, Loss: 3.046875, Token per second per gpu: 25268.640307549398
Epoch: 25, Global Step: 261200, Data Step: 522400, Loss: 2.953125, Token per second per gpu: 25227.040936105925
Epoch: 25, Global Step: 261300, Data Step: 522600, Loss: 3.234375, Token per second per gpu: 25192.064684376863
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0330 10:31:14.109839 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 26, Global Step: 261400, Data Step: 522800, Loss: 3.140625, Token per second per gpu: 24561.78322890498
Epoch: 26, Global Step: 261500, Data Step: 523000, Loss: 3.390625, Token per second per gpu: 25198.928619994607
Epoch: 26, Global Step: 261600, Data Step: 523200, Loss: 2.890625, Token per second per gpu: 25271.198421673784
Epoch: 26, Global Step: 261700, Data Step: 523400, Loss: 3.1875, Token per second per gpu: 25244.905401758253
Epoch: 26, Global Step: 261800, Data Step: 523600, Loss: 3.015625, Token per second per gpu: 25203.116421290673
Epoch: 26, Global Step: 261900, Data Step: 523800, Loss: 3.171875, Token per second per gpu: 25183.642528873537
Epoch: 26, Global Step: 262000, Data Step: 524000, Loss: 3.28125, Token per second per gpu: 25177.886410655767
Epoch: 26, Global Step: 262100, Data Step: 524200, Loss: 3.078125, Token per second per gpu: 25174.36183236886
Epoch: 26, Global Step: 262200, Data Step: 524400, Loss: 2.953125, Token per second per gpu: 25182.36020668272
Epoch: 26, Global Step: 262300, Data Step: 524600, Loss: 3.078125, Token per second per gpu: 25189.47571069663
Epoch: 26, Global Step: 262400, Data Step: 524800, Loss: 3.0, Token per second per gpu: 25196.16664115365
Epoch: 26, Global Step: 262500, Data Step: 525000, Loss: 2.875, Token per second per gpu: 25199.321510582606
Epoch: 26, Global Step: 262600, Data Step: 525200, Loss: 3.140625, Token per second per gpu: 25223.60918667925
Epoch: 26, Global Step: 262700, Data Step: 525400, Loss: 3.140625, Token per second per gpu: 25223.750500673235
Epoch: 26, Global Step: 262800, Data Step: 525600, Loss: 2.875, Token per second per gpu: 25221.298508791948
Epoch: 26, Global Step: 262900, Data Step: 525800, Loss: 2.921875, Token per second per gpu: 25216.597514025332
Epoch: 26, Global Step: 263000, Data Step: 526000, Loss: 3.015625, Token per second per gpu: 25213.852922913375
Epoch: 26, Global Step: 263100, Data Step: 526200, Loss: 3.078125, Token per second per gpu: 25202.34024833945
Epoch: 26, Global Step: 263200, Data Step: 526400, Loss: 2.828125, Token per second per gpu: 25192.29974244641
Epoch: 26, Global Step: 263300, Data Step: 526600, Loss: 3.3125, Token per second per gpu: 25205.342879478292
Epoch: 26, Global Step: 263400, Data Step: 526800, Loss: 2.578125, Token per second per gpu: 25212.457591873368
Epoch: 26, Global Step: 263500, Data Step: 527000, Loss: 3.09375, Token per second per gpu: 25231.463231071244
Epoch: 26, Global Step: 263600, Data Step: 527200, Loss: 3.203125, Token per second per gpu: 25218.60986038032
Epoch: 26, Global Step: 263700, Data Step: 527400, Loss: 2.75, Token per second per gpu: 25222.56673546164
Epoch: 26, Global Step: 263800, Data Step: 527600, Loss: 3.171875, Token per second per gpu: 25221.96994419776
Epoch: 26, Global Step: 263900, Data Step: 527800, Loss: 3.203125, Token per second per gpu: 25225.792909607862
Epoch: 26, Global Step: 264000, Data Step: 528000, Loss: 2.75, Token per second per gpu: 25206.48410689125
Epoch: 26, Global Step: 264100, Data Step: 528200, Loss: 3.078125, Token per second per gpu: 25267.92484088571
Epoch: 26, Global Step: 264200, Data Step: 528400, Loss: 2.953125, Token per second per gpu: 25262.513745108026
Epoch: 26, Global Step: 264300, Data Step: 528600, Loss: 3.078125, Token per second per gpu: 25206.106456367863
Epoch: 26, Global Step: 264400, Data Step: 528800, Loss: 2.875, Token per second per gpu: 23974.736466201568
Epoch: 26, Global Step: 264500, Data Step: 529000, Loss: 3.15625, Token per second per gpu: 25187.557136149004
Epoch: 26, Global Step: 264600, Data Step: 529200, Loss: 3.265625, Token per second per gpu: 25286.33620238682
Epoch: 26, Global Step: 264700, Data Step: 529400, Loss: 3.328125, Token per second per gpu: 25278.854130437616
Epoch: 26, Global Step: 264800, Data Step: 529600, Loss: 2.78125, Token per second per gpu: 25208.722944270412
Epoch: 26, Global Step: 264900, Data Step: 529800, Loss: 2.890625, Token per second per gpu: 25186.671690297084
Epoch: 26, Global Step: 265000, Data Step: 530000, Loss: 3.109375, Token per second per gpu: 25174.382398422753
Epoch: 26, Global Step: 265100, Data Step: 530200, Loss: 3.25, Token per second per gpu: 25175.70252607619
Epoch: 26, Global Step: 265200, Data Step: 530400, Loss: 2.84375, Token per second per gpu: 25254.010425668697
Epoch: 26, Global Step: 265300, Data Step: 530600, Loss: 3.203125, Token per second per gpu: 25270.2373566855
Epoch: 26, Global Step: 265400, Data Step: 530800, Loss: 2.875, Token per second per gpu: 25159.620109423893
Epoch: 26, Global Step: 265500, Data Step: 531000, Loss: 3.0, Token per second per gpu: 23348.551828517822
Epoch: 26, Global Step: 265600, Data Step: 531200, Loss: 2.96875, Token per second per gpu: 22995.162179373147
Epoch: 26, Global Step: 265700, Data Step: 531400, Loss: 3.09375, Token per second per gpu: 24572.986669517988
Epoch: 26, Global Step: 265800, Data Step: 531600, Loss: 3.046875, Token per second per gpu: 23667.40818995725
Epoch: 26, Global Step: 265900, Data Step: 531800, Loss: 3.109375, Token per second per gpu: 24043.07414785228
Epoch: 26, Global Step: 266000, Data Step: 532000, Loss: 2.984375, Token per second per gpu: 25289.149011383182
Epoch: 26, Global Step: 266100, Data Step: 532200, Loss: 2.734375, Token per second per gpu: 25269.168476488518
Epoch: 26, Global Step: 266200, Data Step: 532400, Loss: 3.1875, Token per second per gpu: 25264.158473451957
Epoch: 26, Global Step: 266300, Data Step: 532600, Loss: 2.875, Token per second per gpu: 25217.816675920225
Epoch: 26, Global Step: 266400, Data Step: 532800, Loss: 3.15625, Token per second per gpu: 25203.634965743764
Epoch: 26, Global Step: 266500, Data Step: 533000, Loss: 2.703125, Token per second per gpu: 25199.8639236884
Epoch: 26, Global Step: 266600, Data Step: 533200, Loss: 3.265625, Token per second per gpu: 25191.050790593905
Epoch: 26, Global Step: 266700, Data Step: 533400, Loss: 3.21875, Token per second per gpu: 25187.79300295545
Epoch: 26, Global Step: 266800, Data Step: 533600, Loss: 2.90625, Token per second per gpu: 25188.15550381774
Epoch: 26, Global Step: 266900, Data Step: 533800, Loss: 3.125, Token per second per gpu: 25200.877422960606
Epoch: 26, Global Step: 267000, Data Step: 534000, Loss: 3.3125, Token per second per gpu: 25218.546471177666
Epoch: 26, Global Step: 267100, Data Step: 534200, Loss: 3.1875, Token per second per gpu: 25249.896001121382
Epoch: 26, Global Step: 267200, Data Step: 534400, Loss: 2.8125, Token per second per gpu: 25253.18866885935
Epoch: 26, Global Step: 267300, Data Step: 534600, Loss: 2.90625, Token per second per gpu: 25241.937855774486
Epoch: 26, Global Step: 267400, Data Step: 534800, Loss: 3.15625, Token per second per gpu: 25240.2196525936
Epoch: 26, Global Step: 267500, Data Step: 535000, Loss: 3.390625, Token per second per gpu: 25212.15511685048
Epoch: 26, Global Step: 267600, Data Step: 535200, Loss: 3.0, Token per second per gpu: 25218.222475196926
Epoch: 26, Global Step: 267700, Data Step: 535400, Loss: 3.171875, Token per second per gpu: 25222.644522637074
Epoch: 26, Global Step: 267800, Data Step: 535600, Loss: 2.8125, Token per second per gpu: 25223.183779491443
Epoch: 26, Global Step: 267900, Data Step: 535800, Loss: 3.078125, Token per second per gpu: 25249.237750049113
Epoch: 26, Global Step: 268000, Data Step: 536000, Loss: 2.65625, Token per second per gpu: 25228.3085811913
Epoch: 26, Global Step: 268100, Data Step: 536200, Loss: 3.171875, Token per second per gpu: 24600.967188862287
Epoch: 26, Global Step: 268200, Data Step: 536400, Loss: 3.390625, Token per second per gpu: 25226.891471970535
Epoch: 26, Global Step: 268300, Data Step: 536600, Loss: 3.296875, Token per second per gpu: 25220.03525390437
Epoch: 26, Global Step: 268400, Data Step: 536800, Loss: 2.984375, Token per second per gpu: 25201.883274620715
Epoch: 26, Global Step: 268500, Data Step: 537000, Loss: 3.25, Token per second per gpu: 25202.042169827786
Epoch: 26, Global Step: 268600, Data Step: 537200, Loss: 2.484375, Token per second per gpu: 25202.504617809464
Epoch: 26, Global Step: 268700, Data Step: 537400, Loss: 2.796875, Token per second per gpu: 25273.13975376518
Epoch: 26, Global Step: 268800, Data Step: 537600, Loss: 3.203125, Token per second per gpu: 24617.435392918338
Epoch: 26, Global Step: 268900, Data Step: 537800, Loss: 3.140625, Token per second per gpu: 25217.72201937896
Epoch: 26, Global Step: 269000, Data Step: 538000, Loss: 3.0625, Token per second per gpu: 25196.055855060175
Epoch: 26, Global Step: 269100, Data Step: 538200, Loss: 3.0625, Token per second per gpu: 25287.810130704194
Epoch: 26, Global Step: 269200, Data Step: 538400, Loss: 3.0625, Token per second per gpu: 25267.446405732026
Epoch: 26, Global Step: 269300, Data Step: 538600, Loss: 3.078125, Token per second per gpu: 25200.108590236337
Epoch: 26, Global Step: 269400, Data Step: 538800, Loss: 3.15625, Token per second per gpu: 25175.995049550027
Epoch: 26, Global Step: 269500, Data Step: 539000, Loss: 3.40625, Token per second per gpu: 25181.33124085029
Epoch: 26, Global Step: 269600, Data Step: 539200, Loss: 3.140625, Token per second per gpu: 25210.067984438538
Epoch: 26, Global Step: 269700, Data Step: 539400, Loss: 3.375, Token per second per gpu: 25268.47375284846
Epoch: 26, Global Step: 269800, Data Step: 539600, Loss: 3.140625, Token per second per gpu: 25233.659699941476
Epoch: 26, Global Step: 269900, Data Step: 539800, Loss: 2.84375, Token per second per gpu: 25199.63703046521
Epoch: 26, Global Step: 270000, Data Step: 540000, Loss: 3.21875, Token per second per gpu: 25178.90396850719
I0330 13:16:55.933711 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0330 13:16:55.934138 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-30 13:16:55,934] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-30 13:16:55,938] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-30 13:16:55,938] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-30 13:16:56,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-30 13:16:56,561] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-30 13:16:56,562] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-30 13:16:56,562] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-30 13:16:56,562] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-30 13:16:56,562] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-30 13:16:56,562] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-30 13:16:58,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-30 13:16:58,686] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-30 13:16:58,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 13:16:58,779] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-30 13:16:58,791] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-30 13:16:58,791] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 13:16:58,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-30 13:16:58,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-30 13:16:58,816] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-30 13:16:58,817] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 13:16:58,817] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-30 13:16:58,817] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 13:16:58,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-30 13:16:58,823] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-30 13:16:58,823] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 13:16:58,847] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-30 13:16:58,847] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-30 13:16:58,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0330 13:16:58.848348 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0330 13:16:58.849041 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0330 13:16:58.849309 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0330 13:16:58.850695 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 26, Global Step: 270100, Data Step: 540200, Loss: 2.984375, Token per second per gpu: 24567.507036009632
Epoch: 26, Global Step: 270200, Data Step: 540400, Loss: 3.140625, Token per second per gpu: 25168.12321292268
Epoch: 26, Global Step: 270300, Data Step: 540600, Loss: 2.703125, Token per second per gpu: 25176.113267691562
Epoch: 26, Global Step: 270400, Data Step: 540800, Loss: 3.078125, Token per second per gpu: 25253.720785267244
Epoch: 26, Global Step: 270500, Data Step: 541000, Loss: 3.25, Token per second per gpu: 25269.72431462348
Epoch: 26, Global Step: 270600, Data Step: 541200, Loss: 3.1875, Token per second per gpu: 25253.102615826952
Epoch: 26, Global Step: 270700, Data Step: 541400, Loss: 3.09375, Token per second per gpu: 25244.867257186455
Epoch: 26, Global Step: 270800, Data Step: 541600, Loss: 2.875, Token per second per gpu: 23839.89474728747
Epoch: 26, Global Step: 270900, Data Step: 541800, Loss: 2.875, Token per second per gpu: 22266.130502901746
Epoch: 26, Global Step: 271000, Data Step: 542000, Loss: 2.9375, Token per second per gpu: 22170.203473341488
Epoch: 26, Global Step: 271100, Data Step: 542200, Loss: 3.015625, Token per second per gpu: 22574.51106555023
Epoch: 26, Global Step: 271200, Data Step: 542400, Loss: 3.0625, Token per second per gpu: 22277.401800838583
Epoch: 26, Global Step: 271300, Data Step: 542600, Loss: 2.6875, Token per second per gpu: 22667.243882590472
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0330 13:44:56.733540 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 27, Global Step: 271400, Data Step: 542800, Loss: 2.671875, Token per second per gpu: 22376.628118608474
Epoch: 27, Global Step: 271500, Data Step: 543000, Loss: 3.234375, Token per second per gpu: 22400.92932847804
Epoch: 27, Global Step: 271600, Data Step: 543200, Loss: 3.015625, Token per second per gpu: 22662.67583593713
Epoch: 27, Global Step: 271700, Data Step: 543400, Loss: 2.90625, Token per second per gpu: 22304.4810551073
Epoch: 27, Global Step: 271800, Data Step: 543600, Loss: 3.0, Token per second per gpu: 22763.28065662203
Epoch: 27, Global Step: 271900, Data Step: 543800, Loss: 2.9375, Token per second per gpu: 22609.626030702177
Epoch: 27, Global Step: 272000, Data Step: 544000, Loss: 3.265625, Token per second per gpu: 22438.70320267078
Epoch: 27, Global Step: 272100, Data Step: 544200, Loss: 3.25, Token per second per gpu: 22546.79761804892
Epoch: 27, Global Step: 272200, Data Step: 544400, Loss: 2.671875, Token per second per gpu: 22411.88013693249
Epoch: 27, Global Step: 272300, Data Step: 544600, Loss: 2.859375, Token per second per gpu: 22846.16720820484
Epoch: 27, Global Step: 272400, Data Step: 544800, Loss: 2.765625, Token per second per gpu: 22576.778621028967
Epoch: 27, Global Step: 272500, Data Step: 545000, Loss: 3.3125, Token per second per gpu: 22682.698395836935
Epoch: 27, Global Step: 272600, Data Step: 545200, Loss: 3.09375, Token per second per gpu: 22887.99478507619
Epoch: 27, Global Step: 272700, Data Step: 545400, Loss: 3.125, Token per second per gpu: 22536.364009376015
Epoch: 27, Global Step: 272800, Data Step: 545600, Loss: 2.921875, Token per second per gpu: 22387.69579077784
Epoch: 27, Global Step: 272900, Data Step: 545800, Loss: 3.171875, Token per second per gpu: 22542.418741367368
Epoch: 27, Global Step: 273000, Data Step: 546000, Loss: 2.953125, Token per second per gpu: 22549.939713013624
Epoch: 27, Global Step: 273100, Data Step: 546200, Loss: 2.9375, Token per second per gpu: 22696.06036671801
Epoch: 27, Global Step: 273200, Data Step: 546400, Loss: 3.140625, Token per second per gpu: 22939.12042610858
Epoch: 27, Global Step: 273300, Data Step: 546600, Loss: 3.0625, Token per second per gpu: 22349.1333447615
Epoch: 27, Global Step: 273400, Data Step: 546800, Loss: 3.28125, Token per second per gpu: 22574.03836394172
Epoch: 27, Global Step: 273500, Data Step: 547000, Loss: 3.265625, Token per second per gpu: 22687.308905023532
Epoch: 27, Global Step: 273600, Data Step: 547200, Loss: 3.0625, Token per second per gpu: 22692.866508406176
Epoch: 27, Global Step: 273700, Data Step: 547400, Loss: 2.703125, Token per second per gpu: 22698.930438313022
Epoch: 27, Global Step: 273800, Data Step: 547600, Loss: 3.1875, Token per second per gpu: 22853.44475623414
Epoch: 27, Global Step: 273900, Data Step: 547800, Loss: 3.25, Token per second per gpu: 22563.273216310354
Epoch: 27, Global Step: 274000, Data Step: 548000, Loss: 3.0625, Token per second per gpu: 22744.81632750942
Epoch: 27, Global Step: 274100, Data Step: 548200, Loss: 2.96875, Token per second per gpu: 22802.126882217442
Epoch: 27, Global Step: 274200, Data Step: 548400, Loss: 3.265625, Token per second per gpu: 22799.864616363473
Epoch: 27, Global Step: 274300, Data Step: 548600, Loss: 3.125, Token per second per gpu: 22785.6117059341
Epoch: 27, Global Step: 274400, Data Step: 548800, Loss: 3.34375, Token per second per gpu: 22542.69033362638
Epoch: 27, Global Step: 274500, Data Step: 549000, Loss: 2.953125, Token per second per gpu: 22685.929526267882
Epoch: 27, Global Step: 274600, Data Step: 549200, Loss: 2.859375, Token per second per gpu: 23044.651803272813
Epoch: 27, Global Step: 274700, Data Step: 549400, Loss: 2.953125, Token per second per gpu: 22475.446180873238
Epoch: 27, Global Step: 274800, Data Step: 549600, Loss: 2.875, Token per second per gpu: 22494.804336266025
Epoch: 27, Global Step: 274900, Data Step: 549800, Loss: 2.75, Token per second per gpu: 22802.97334602958
Epoch: 27, Global Step: 275000, Data Step: 550000, Loss: 3.25, Token per second per gpu: 22722.603470928814
Epoch: 27, Global Step: 275100, Data Step: 550200, Loss: 2.875, Token per second per gpu: 22770.219019386495
Epoch: 27, Global Step: 275200, Data Step: 550400, Loss: 2.90625, Token per second per gpu: 22770.839906010136
Epoch: 27, Global Step: 275300, Data Step: 550600, Loss: 3.34375, Token per second per gpu: 22929.919652522636
Epoch: 27, Global Step: 275400, Data Step: 550800, Loss: 3.234375, Token per second per gpu: 22893.922514562215
Epoch: 27, Global Step: 275500, Data Step: 551000, Loss: 3.1875, Token per second per gpu: 22288.624513127856
Epoch: 27, Global Step: 275600, Data Step: 551200, Loss: 3.03125, Token per second per gpu: 22799.567900502192
Epoch: 27, Global Step: 275700, Data Step: 551400, Loss: 3.046875, Token per second per gpu: 22864.018429651896
Epoch: 27, Global Step: 275800, Data Step: 551600, Loss: 3.203125, Token per second per gpu: 22692.153355498787
Epoch: 27, Global Step: 275900, Data Step: 551800, Loss: 2.96875, Token per second per gpu: 23150.23317732026
Epoch: 27, Global Step: 276000, Data Step: 552000, Loss: 2.859375, Token per second per gpu: 22734.27395124948
Epoch: 27, Global Step: 276100, Data Step: 552200, Loss: 2.90625, Token per second per gpu: 22994.498928246292
Epoch: 27, Global Step: 276200, Data Step: 552400, Loss: 3.09375, Token per second per gpu: 22796.274163158923
Epoch: 27, Global Step: 276300, Data Step: 552600, Loss: 2.890625, Token per second per gpu: 22852.62182153088
Epoch: 27, Global Step: 276400, Data Step: 552800, Loss: 2.8125, Token per second per gpu: 22808.48526947639
Epoch: 27, Global Step: 276500, Data Step: 553000, Loss: 3.078125, Token per second per gpu: 22445.788247678887
Epoch: 27, Global Step: 276600, Data Step: 553200, Loss: 3.015625, Token per second per gpu: 22713.771225305594
Epoch: 27, Global Step: 276700, Data Step: 553400, Loss: 3.28125, Token per second per gpu: 22828.124725741513
Epoch: 27, Global Step: 276800, Data Step: 553600, Loss: 2.921875, Token per second per gpu: 22732.96718919834
Epoch: 27, Global Step: 276900, Data Step: 553800, Loss: 3.203125, Token per second per gpu: 22253.92572855096
Epoch: 27, Global Step: 277000, Data Step: 554000, Loss: 2.921875, Token per second per gpu: 22775.02981201338
Epoch: 27, Global Step: 277100, Data Step: 554200, Loss: 3.125, Token per second per gpu: 22707.638792104834
Epoch: 27, Global Step: 277200, Data Step: 554400, Loss: 3.21875, Token per second per gpu: 22994.432570220597
Epoch: 27, Global Step: 277300, Data Step: 554600, Loss: 3.40625, Token per second per gpu: 22608.5928619838
Epoch: 27, Global Step: 277400, Data Step: 554800, Loss: 3.21875, Token per second per gpu: 22847.877246319913
Epoch: 27, Global Step: 277500, Data Step: 555000, Loss: 3.296875, Token per second per gpu: 22503.898847191358
Epoch: 27, Global Step: 277600, Data Step: 555200, Loss: 3.34375, Token per second per gpu: 22632.942373385096
Epoch: 27, Global Step: 277700, Data Step: 555400, Loss: 2.78125, Token per second per gpu: 22913.390111302066
Epoch: 27, Global Step: 277800, Data Step: 555600, Loss: 3.15625, Token per second per gpu: 23008.344173505622
Epoch: 27, Global Step: 277900, Data Step: 555800, Loss: 2.546875, Token per second per gpu: 22905.755482217264
Epoch: 27, Global Step: 278000, Data Step: 556000, Loss: 3.28125, Token per second per gpu: 21574.912854348062
Epoch: 27, Global Step: 278100, Data Step: 556200, Loss: 3.359375, Token per second per gpu: 22614.451033838428
Epoch: 27, Global Step: 278200, Data Step: 556400, Loss: 2.6875, Token per second per gpu: 22713.28387584572
Epoch: 27, Global Step: 278300, Data Step: 556600, Loss: 2.9375, Token per second per gpu: 22491.442817989733
Epoch: 27, Global Step: 278400, Data Step: 556800, Loss: 3.484375, Token per second per gpu: 22906.54602108013
Epoch: 27, Global Step: 278500, Data Step: 557000, Loss: 3.046875, Token per second per gpu: 22302.483094650248
Epoch: 27, Global Step: 278600, Data Step: 557200, Loss: 3.125, Token per second per gpu: 22717.833383952417
Epoch: 27, Global Step: 278700, Data Step: 557400, Loss: 2.671875, Token per second per gpu: 22900.893087576253
Epoch: 27, Global Step: 278800, Data Step: 557600, Loss: 3.046875, Token per second per gpu: 21637.75833722102
Epoch: 27, Global Step: 278900, Data Step: 557800, Loss: 3.25, Token per second per gpu: 22514.939171674418
Epoch: 27, Global Step: 279000, Data Step: 558000, Loss: 2.8125, Token per second per gpu: 22852.358792381903
Epoch: 27, Global Step: 279100, Data Step: 558200, Loss: 3.125, Token per second per gpu: 22360.764027657868
Epoch: 27, Global Step: 279200, Data Step: 558400, Loss: 2.921875, Token per second per gpu: 22771.74075616399
Epoch: 27, Global Step: 279300, Data Step: 558600, Loss: 3.15625, Token per second per gpu: 22591.875518966597
Epoch: 27, Global Step: 279400, Data Step: 558800, Loss: 2.65625, Token per second per gpu: 22592.104740983443
Epoch: 27, Global Step: 279500, Data Step: 559000, Loss: 3.234375, Token per second per gpu: 22531.62215277844
Epoch: 27, Global Step: 279600, Data Step: 559200, Loss: 2.765625, Token per second per gpu: 22683.707849508508
Epoch: 27, Global Step: 279700, Data Step: 559400, Loss: 3.59375, Token per second per gpu: 22624.92128024362
Epoch: 27, Global Step: 279800, Data Step: 559600, Loss: 2.796875, Token per second per gpu: 22618.345223154487
Epoch: 27, Global Step: 279900, Data Step: 559800, Loss: 3.0625, Token per second per gpu: 22633.475304999523
Epoch: 27, Global Step: 280000, Data Step: 560000, Loss: 3.203125, Token per second per gpu: 22532.866106420173
I0330 16:47:23.919710 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0330 16:47:23.921215 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-30 16:47:23,924] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-30 16:47:23,950] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-30 16:47:23,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-30 16:47:24,995] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-30 16:47:25,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-30 16:47:25,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-30 16:47:25,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-30 16:47:25,060] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-30 16:47:25,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-30 16:47:25,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-30 16:47:27,184] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-30 16:47:27,184] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-30 16:47:27,184] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 16:47:27,407] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-30 16:47:27,407] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-30 16:47:27,407] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 16:47:27,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-30 16:47:27,430] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-30 16:47:27,430] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 16:47:27,462] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-30 16:47:27,481] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-30 16:47:27,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 16:47:27,562] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-30 16:47:27,562] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-30 16:47:27,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 16:47:27,575] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-30 16:47:27,575] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-30 16:47:27,575] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0330 16:47:27.575950 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0330 16:47:27.576959 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0330 16:47:27.577243 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0330 16:47:27.578783 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 27, Global Step: 280100, Data Step: 560200, Loss: 3.09375, Token per second per gpu: 22319.248896502988
Epoch: 27, Global Step: 280200, Data Step: 560400, Loss: 2.96875, Token per second per gpu: 22773.740558241043
Epoch: 27, Global Step: 280300, Data Step: 560600, Loss: 3.296875, Token per second per gpu: 22814.40859282252
Epoch: 27, Global Step: 280400, Data Step: 560800, Loss: 2.421875, Token per second per gpu: 22626.393693911825
Epoch: 27, Global Step: 280500, Data Step: 561000, Loss: 3.203125, Token per second per gpu: 23002.622055380598
Epoch: 27, Global Step: 280600, Data Step: 561200, Loss: 2.859375, Token per second per gpu: 22603.413419453325
Epoch: 27, Global Step: 280700, Data Step: 561400, Loss: 3.25, Token per second per gpu: 22645.527860811922
Epoch: 27, Global Step: 280800, Data Step: 561600, Loss: 3.40625, Token per second per gpu: 22836.104116317052
Epoch: 27, Global Step: 280900, Data Step: 561800, Loss: 3.171875, Token per second per gpu: 22653.562767097825
Epoch: 27, Global Step: 281000, Data Step: 562000, Loss: 3.03125, Token per second per gpu: 22691.494851092662
Epoch: 27, Global Step: 281100, Data Step: 562200, Loss: 3.109375, Token per second per gpu: 22959.333908110108
Epoch: 27, Global Step: 281200, Data Step: 562400, Loss: 2.90625, Token per second per gpu: 22501.12922475618
Epoch: 27, Global Step: 281300, Data Step: 562600, Loss: 3.328125, Token per second per gpu: 22737.47866344166
Epoch: 27, Global Step: 281400, Data Step: 562800, Loss: 3.234375, Token per second per gpu: 22188.559849754467
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0330 17:17:57.189030 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 28, Global Step: 281500, Data Step: 563000, Loss: 3.3125, Token per second per gpu: 20891.54206290779
Epoch: 28, Global Step: 281600, Data Step: 563200, Loss: 2.828125, Token per second per gpu: 22522.78183882394
Epoch: 28, Global Step: 281700, Data Step: 563400, Loss: 3.296875, Token per second per gpu: 22656.527741076912
Epoch: 28, Global Step: 281800, Data Step: 563600, Loss: 2.96875, Token per second per gpu: 22876.58219659726
Epoch: 28, Global Step: 281900, Data Step: 563800, Loss: 2.984375, Token per second per gpu: 22424.641228765944
Epoch: 28, Global Step: 282000, Data Step: 564000, Loss: 2.921875, Token per second per gpu: 22319.158624981304
Epoch: 28, Global Step: 282100, Data Step: 564200, Loss: 2.953125, Token per second per gpu: 23046.41380155825
Epoch: 28, Global Step: 282200, Data Step: 564400, Loss: 3.328125, Token per second per gpu: 22826.49550071685
Epoch: 28, Global Step: 282300, Data Step: 564600, Loss: 2.875, Token per second per gpu: 22930.03478040871
Epoch: 28, Global Step: 282400, Data Step: 564800, Loss: 3.09375, Token per second per gpu: 22614.40725750283
Epoch: 28, Global Step: 282500, Data Step: 565000, Loss: 3.125, Token per second per gpu: 23061.16987313889
Epoch: 28, Global Step: 282600, Data Step: 565200, Loss: 2.953125, Token per second per gpu: 22603.686610333873
Epoch: 28, Global Step: 282700, Data Step: 565400, Loss: 3.078125, Token per second per gpu: 22578.24827770701
Epoch: 28, Global Step: 282800, Data Step: 565600, Loss: 3.09375, Token per second per gpu: 22712.474975720997
Epoch: 28, Global Step: 282900, Data Step: 565800, Loss: 3.0625, Token per second per gpu: 22480.967400031794
Epoch: 28, Global Step: 283000, Data Step: 566000, Loss: 3.078125, Token per second per gpu: 22632.572130666103
Epoch: 28, Global Step: 283100, Data Step: 566200, Loss: 2.828125, Token per second per gpu: 22724.27445574528
Epoch: 28, Global Step: 283200, Data Step: 566400, Loss: 3.15625, Token per second per gpu: 22235.48249106046
Epoch: 28, Global Step: 283300, Data Step: 566600, Loss: 3.09375, Token per second per gpu: 22695.100085191767
Epoch: 28, Global Step: 283400, Data Step: 566800, Loss: 2.9375, Token per second per gpu: 22908.539333402565
Epoch: 28, Global Step: 283500, Data Step: 567000, Loss: 2.96875, Token per second per gpu: 22317.36633592608
Epoch: 28, Global Step: 283600, Data Step: 567200, Loss: 3.046875, Token per second per gpu: 22406.967274656305
Epoch: 28, Global Step: 283700, Data Step: 567400, Loss: 2.90625, Token per second per gpu: 22491.303701317072
Epoch: 28, Global Step: 283800, Data Step: 567600, Loss: 3.03125, Token per second per gpu: 22315.22549172775
Epoch: 28, Global Step: 283900, Data Step: 567800, Loss: 3.0, Token per second per gpu: 22337.478487737797
Epoch: 28, Global Step: 284000, Data Step: 568000, Loss: 2.765625, Token per second per gpu: 22549.84853396127
Epoch: 28, Global Step: 284100, Data Step: 568200, Loss: 3.234375, Token per second per gpu: 22512.78261943058
Epoch: 28, Global Step: 284200, Data Step: 568400, Loss: 2.96875, Token per second per gpu: 22537.931475322785
Epoch: 28, Global Step: 284300, Data Step: 568600, Loss: 3.34375, Token per second per gpu: 22348.785932688053
Epoch: 28, Global Step: 284400, Data Step: 568800, Loss: 2.953125, Token per second per gpu: 22610.25583810832
Epoch: 28, Global Step: 284500, Data Step: 569000, Loss: 3.046875, Token per second per gpu: 22868.234213938253
Epoch: 28, Global Step: 284600, Data Step: 569200, Loss: 2.921875, Token per second per gpu: 22750.264562136712
Epoch: 28, Global Step: 284700, Data Step: 569400, Loss: 3.21875, Token per second per gpu: 22603.339909859315
Epoch: 28, Global Step: 284800, Data Step: 569600, Loss: 3.0625, Token per second per gpu: 22908.876647371937
Epoch: 28, Global Step: 284900, Data Step: 569800, Loss: 2.875, Token per second per gpu: 22961.54618192966
Epoch: 28, Global Step: 285000, Data Step: 570000, Loss: 3.140625, Token per second per gpu: 22929.20780047398
Epoch: 28, Global Step: 285100, Data Step: 570200, Loss: 2.921875, Token per second per gpu: 23128.851718201033
Epoch: 28, Global Step: 285200, Data Step: 570400, Loss: 2.546875, Token per second per gpu: 22687.653839839695
Epoch: 28, Global Step: 285300, Data Step: 570600, Loss: 3.0625, Token per second per gpu: 22299.921154031945
Epoch: 28, Global Step: 285400, Data Step: 570800, Loss: 3.0, Token per second per gpu: 22611.05688092222
Epoch: 28, Global Step: 285500, Data Step: 571000, Loss: 3.265625, Token per second per gpu: 22718.40732252628
Epoch: 28, Global Step: 285600, Data Step: 571200, Loss: 3.03125, Token per second per gpu: 22495.280176689954
Epoch: 28, Global Step: 285700, Data Step: 571400, Loss: 3.078125, Token per second per gpu: 23115.505619628813
Epoch: 28, Global Step: 285800, Data Step: 571600, Loss: 3.328125, Token per second per gpu: 22490.76542562365
Epoch: 28, Global Step: 285900, Data Step: 571800, Loss: 3.125, Token per second per gpu: 22868.065677398252
Epoch: 28, Global Step: 286000, Data Step: 572000, Loss: 3.3125, Token per second per gpu: 22577.36427487281
Epoch: 28, Global Step: 286100, Data Step: 572200, Loss: 2.890625, Token per second per gpu: 22598.614221167132
Epoch: 28, Global Step: 286200, Data Step: 572400, Loss: 3.046875, Token per second per gpu: 22347.329458359185
Epoch: 28, Global Step: 286300, Data Step: 572600, Loss: 2.734375, Token per second per gpu: 22461.31896087551
Epoch: 28, Global Step: 286400, Data Step: 572800, Loss: 3.296875, Token per second per gpu: 22581.432873728147
Epoch: 28, Global Step: 286500, Data Step: 573000, Loss: 3.328125, Token per second per gpu: 22594.371944414524
Epoch: 28, Global Step: 286600, Data Step: 573200, Loss: 2.890625, Token per second per gpu: 22502.221758982167
Epoch: 28, Global Step: 286700, Data Step: 573400, Loss: 2.90625, Token per second per gpu: 22177.612207709684
Epoch: 28, Global Step: 286800, Data Step: 573600, Loss: 3.375, Token per second per gpu: 22711.429141011395
Epoch: 28, Global Step: 286900, Data Step: 573800, Loss: 2.703125, Token per second per gpu: 22482.813765500367
Epoch: 28, Global Step: 287000, Data Step: 574000, Loss: 3.234375, Token per second per gpu: 22743.17901738484
Epoch: 28, Global Step: 287100, Data Step: 574200, Loss: 3.28125, Token per second per gpu: 25074.1198468688
Epoch: 28, Global Step: 287200, Data Step: 574400, Loss: 3.140625, Token per second per gpu: 25165.615230250318
Epoch: 28, Global Step: 287300, Data Step: 574600, Loss: 3.140625, Token per second per gpu: 25095.836209150002
Epoch: 28, Global Step: 287400, Data Step: 574800, Loss: 3.25, Token per second per gpu: 25041.010318531113
Epoch: 28, Global Step: 287500, Data Step: 575000, Loss: 2.71875, Token per second per gpu: 24986.822619281782
Epoch: 28, Global Step: 287600, Data Step: 575200, Loss: 3.1875, Token per second per gpu: 24985.966269702487
Epoch: 28, Global Step: 287700, Data Step: 575400, Loss: 2.96875, Token per second per gpu: 24951.868743239804
Epoch: 28, Global Step: 287800, Data Step: 575600, Loss: 2.96875, Token per second per gpu: 24995.28088607331
Epoch: 28, Global Step: 287900, Data Step: 575800, Loss: 2.53125, Token per second per gpu: 24434.929988516877
Epoch: 28, Global Step: 288000, Data Step: 576000, Loss: 2.984375, Token per second per gpu: 24486.735208955186
Epoch: 28, Global Step: 288100, Data Step: 576200, Loss: 2.984375, Token per second per gpu: 25029.314639400764
Epoch: 28, Global Step: 288200, Data Step: 576400, Loss: 3.109375, Token per second per gpu: 24981.333469439283
Epoch: 28, Global Step: 288300, Data Step: 576600, Loss: 2.984375, Token per second per gpu: 24969.84716612687
Epoch: 28, Global Step: 288400, Data Step: 576800, Loss: 2.9375, Token per second per gpu: 24968.76716040116
Epoch: 28, Global Step: 288500, Data Step: 577000, Loss: 3.15625, Token per second per gpu: 24995.908738346432
Epoch: 28, Global Step: 288600, Data Step: 577200, Loss: 3.328125, Token per second per gpu: 24994.55692192352
Epoch: 28, Global Step: 288700, Data Step: 577400, Loss: 2.921875, Token per second per gpu: 24385.15141238409
Epoch: 28, Global Step: 288800, Data Step: 577600, Loss: 3.015625, Token per second per gpu: 25018.477675728795
Epoch: 28, Global Step: 288900, Data Step: 577800, Loss: 3.15625, Token per second per gpu: 25010.0651180588
Epoch: 28, Global Step: 289000, Data Step: 578000, Loss: 3.53125, Token per second per gpu: 24996.215977989203
Epoch: 28, Global Step: 289100, Data Step: 578200, Loss: 3.359375, Token per second per gpu: 24992.56672797547
Epoch: 28, Global Step: 289200, Data Step: 578400, Loss: 3.078125, Token per second per gpu: 24978.21641527528
Epoch: 28, Global Step: 289300, Data Step: 578600, Loss: 3.296875, Token per second per gpu: 24975.268683602237
Epoch: 28, Global Step: 289400, Data Step: 578800, Loss: 2.875, Token per second per gpu: 23245.072295798214
Epoch: 28, Global Step: 289500, Data Step: 579000, Loss: 3.328125, Token per second per gpu: 25006.633283335712
Epoch: 28, Global Step: 289600, Data Step: 579200, Loss: 2.78125, Token per second per gpu: 25024.021627842587
Epoch: 28, Global Step: 289700, Data Step: 579400, Loss: 3.109375, Token per second per gpu: 25028.345799358704
Epoch: 28, Global Step: 289800, Data Step: 579600, Loss: 3.125, Token per second per gpu: 25027.62738563801
Epoch: 28, Global Step: 289900, Data Step: 579800, Loss: 2.953125, Token per second per gpu: 25037.07397670902
Epoch: 28, Global Step: 290000, Data Step: 580000, Loss: 3.328125, Token per second per gpu: 25017.248645067662
I0330 20:13:52.886081 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0330 20:13:52.886625 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-30 20:13:52,887] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-30 20:13:52,891] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-30 20:13:52,891] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-30 20:13:53,431] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-30 20:13:53,432] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-30 20:13:53,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-30 20:13:53,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-30 20:13:53,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-30 20:13:53,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-30 20:13:53,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-30 20:13:55,446] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-30 20:13:55,447] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-30 20:13:55,447] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 20:13:55,569] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-30 20:13:55,573] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-30 20:13:55,573] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 20:13:55,611] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-30 20:13:55,611] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-30 20:13:55,611] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 20:13:55,637] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-30 20:13:55,638] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-30 20:13:55,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 20:13:55,638] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-30 20:13:55,638] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-30 20:13:55,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 20:13:55,638] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-30 20:13:55,638] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-30 20:13:55,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0330 20:13:55.639560 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0330 20:13:55.640362 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0330 20:13:55.640634 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0330 20:13:55.642099 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 28, Global Step: 290100, Data Step: 580200, Loss: 3.1875, Token per second per gpu: 24451.481754453453
Epoch: 28, Global Step: 290200, Data Step: 580400, Loss: 3.078125, Token per second per gpu: 25021.347196596
Epoch: 28, Global Step: 290300, Data Step: 580600, Loss: 3.171875, Token per second per gpu: 25011.53720630078
Epoch: 28, Global Step: 290400, Data Step: 580800, Loss: 3.125, Token per second per gpu: 25001.722175459938
Epoch: 28, Global Step: 290500, Data Step: 581000, Loss: 3.109375, Token per second per gpu: 25006.656009334187
Epoch: 28, Global Step: 290600, Data Step: 581200, Loss: 3.203125, Token per second per gpu: 24959.450372515843
Epoch: 28, Global Step: 290700, Data Step: 581400, Loss: 3.515625, Token per second per gpu: 25079.962122673973
Epoch: 28, Global Step: 290800, Data Step: 581600, Loss: 3.078125, Token per second per gpu: 25360.249568495496
Epoch: 28, Global Step: 290900, Data Step: 581800, Loss: 2.953125, Token per second per gpu: 25349.25447969377
Epoch: 28, Global Step: 291000, Data Step: 582000, Loss: 2.921875, Token per second per gpu: 25203.508916619852
Epoch: 28, Global Step: 291100, Data Step: 582200, Loss: 3.265625, Token per second per gpu: 25226.67415417199
Epoch: 28, Global Step: 291200, Data Step: 582400, Loss: 3.125, Token per second per gpu: 25460.19160586513
Epoch: 28, Global Step: 291300, Data Step: 582600, Loss: 3.046875, Token per second per gpu: 25443.908212038114
Epoch: 28, Global Step: 291400, Data Step: 582800, Loss: 3.203125, Token per second per gpu: 25318.004883549762
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0330 20:42:23.525396 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 29, Global Step: 291500, Data Step: 583000, Loss: 3.125, Token per second per gpu: 24630.163358809583
Epoch: 29, Global Step: 291600, Data Step: 583200, Loss: 3.1875, Token per second per gpu: 25216.069329612274
Epoch: 29, Global Step: 291700, Data Step: 583400, Loss: 2.84375, Token per second per gpu: 25188.386339636316
Epoch: 29, Global Step: 291800, Data Step: 583600, Loss: 3.265625, Token per second per gpu: 25191.307211483796
Epoch: 29, Global Step: 291900, Data Step: 583800, Loss: 2.734375, Token per second per gpu: 25182.1469102388
Epoch: 29, Global Step: 292000, Data Step: 584000, Loss: 3.15625, Token per second per gpu: 25179.968112671555
Epoch: 29, Global Step: 292100, Data Step: 584200, Loss: 3.0, Token per second per gpu: 25134.620677523682
Epoch: 29, Global Step: 292200, Data Step: 584400, Loss: 3.0, Token per second per gpu: 25066.316676046306
Epoch: 29, Global Step: 292300, Data Step: 584600, Loss: 2.9375, Token per second per gpu: 25046.911491726143
Epoch: 29, Global Step: 292400, Data Step: 584800, Loss: 3.0625, Token per second per gpu: 25027.77610540501
Epoch: 29, Global Step: 292500, Data Step: 585000, Loss: 3.3125, Token per second per gpu: 24994.9139854413
Epoch: 29, Global Step: 292600, Data Step: 585200, Loss: 3.046875, Token per second per gpu: 24988.88318448883
Epoch: 29, Global Step: 292700, Data Step: 585400, Loss: 3.09375, Token per second per gpu: 25006.707000620037
Epoch: 29, Global Step: 292800, Data Step: 585600, Loss: 3.046875, Token per second per gpu: 25002.53002822129
Epoch: 29, Global Step: 292900, Data Step: 585800, Loss: 2.8125, Token per second per gpu: 25005.269126108866
Epoch: 29, Global Step: 293000, Data Step: 586000, Loss: 3.328125, Token per second per gpu: 25041.883164243412
Epoch: 29, Global Step: 293100, Data Step: 586200, Loss: 3.03125, Token per second per gpu: 25034.714124944057
Epoch: 29, Global Step: 293200, Data Step: 586400, Loss: 3.15625, Token per second per gpu: 25019.850063279555
Epoch: 29, Global Step: 293300, Data Step: 586600, Loss: 2.40625, Token per second per gpu: 25028.627803832158
Epoch: 29, Global Step: 293400, Data Step: 586800, Loss: 3.171875, Token per second per gpu: 25023.398945805133
Epoch: 29, Global Step: 293500, Data Step: 587000, Loss: 3.109375, Token per second per gpu: 25025.42583461958
Epoch: 29, Global Step: 293600, Data Step: 587200, Loss: 3.046875, Token per second per gpu: 25025.8894451277
Epoch: 29, Global Step: 293700, Data Step: 587400, Loss: 3.203125, Token per second per gpu: 25036.658263069974
Epoch: 29, Global Step: 293800, Data Step: 587600, Loss: 2.953125, Token per second per gpu: 25049.145861363577
Epoch: 29, Global Step: 293900, Data Step: 587800, Loss: 3.390625, Token per second per gpu: 25045.011196687035
Epoch: 29, Global Step: 294000, Data Step: 588000, Loss: 2.65625, Token per second per gpu: 25071.113117134828
Epoch: 29, Global Step: 294100, Data Step: 588200, Loss: 2.96875, Token per second per gpu: 25072.45688673662
Epoch: 29, Global Step: 294200, Data Step: 588400, Loss: 3.09375, Token per second per gpu: 25065.336335558437
Epoch: 29, Global Step: 294300, Data Step: 588600, Loss: 3.109375, Token per second per gpu: 25070.750491064864
Epoch: 29, Global Step: 294400, Data Step: 588800, Loss: 3.296875, Token per second per gpu: 25072.628725619015
Epoch: 29, Global Step: 294500, Data Step: 589000, Loss: 2.796875, Token per second per gpu: 25069.801229500747
Epoch: 29, Global Step: 294600, Data Step: 589200, Loss: 3.046875, Token per second per gpu: 25060.365803969264
Epoch: 29, Global Step: 294700, Data Step: 589400, Loss: 3.109375, Token per second per gpu: 25051.339586849263
Epoch: 29, Global Step: 294800, Data Step: 589600, Loss: 3.109375, Token per second per gpu: 25018.45658636827
Epoch: 29, Global Step: 294900, Data Step: 589800, Loss: 2.921875, Token per second per gpu: 25027.784765222743
Epoch: 29, Global Step: 295000, Data Step: 590000, Loss: 2.90625, Token per second per gpu: 25004.19231676252
Epoch: 29, Global Step: 295100, Data Step: 590200, Loss: 3.375, Token per second per gpu: 24995.093193383713
Epoch: 29, Global Step: 295200, Data Step: 590400, Loss: 3.015625, Token per second per gpu: 24397.73629576689
Epoch: 29, Global Step: 295300, Data Step: 590600, Loss: 2.828125, Token per second per gpu: 25008.512899209036
Epoch: 29, Global Step: 295400, Data Step: 590800, Loss: 2.890625, Token per second per gpu: 25003.247674609756
Epoch: 29, Global Step: 295500, Data Step: 591000, Loss: 3.28125, Token per second per gpu: 24990.826673375686
Epoch: 29, Global Step: 295600, Data Step: 591200, Loss: 2.921875, Token per second per gpu: 24977.70002909648
Epoch: 29, Global Step: 295700, Data Step: 591400, Loss: 2.9375, Token per second per gpu: 24974.133632508663
Epoch: 29, Global Step: 295800, Data Step: 591600, Loss: 2.84375, Token per second per gpu: 24968.735832597176
Epoch: 29, Global Step: 295900, Data Step: 591800, Loss: 3.203125, Token per second per gpu: 24965.02030367893
Epoch: 29, Global Step: 296000, Data Step: 592000, Loss: 3.125, Token per second per gpu: 24968.17493886039
Epoch: 29, Global Step: 296100, Data Step: 592200, Loss: 3.0625, Token per second per gpu: 24969.964023960518
Epoch: 29, Global Step: 296200, Data Step: 592400, Loss: 2.890625, Token per second per gpu: 24969.043849740498
Epoch: 29, Global Step: 296300, Data Step: 592600, Loss: 2.765625, Token per second per gpu: 24973.484466013877
Epoch: 29, Global Step: 296400, Data Step: 592800, Loss: 2.859375, Token per second per gpu: 24973.31893989524
Epoch: 29, Global Step: 296500, Data Step: 593000, Loss: 3.0625, Token per second per gpu: 24960.641907376932
Epoch: 29, Global Step: 296600, Data Step: 593200, Loss: 2.953125, Token per second per gpu: 24985.4256352585
Epoch: 29, Global Step: 296700, Data Step: 593400, Loss: 2.96875, Token per second per gpu: 24974.018646052995
Epoch: 29, Global Step: 296800, Data Step: 593600, Loss: 3.21875, Token per second per gpu: 24964.904627202643
Epoch: 29, Global Step: 296900, Data Step: 593800, Loss: 3.0625, Token per second per gpu: 24939.157916744713
Epoch: 29, Global Step: 297000, Data Step: 594000, Loss: 3.21875, Token per second per gpu: 24934.475289074555
Epoch: 29, Global Step: 297100, Data Step: 594200, Loss: 3.234375, Token per second per gpu: 24943.412631551535
Epoch: 29, Global Step: 297200, Data Step: 594400, Loss: 3.140625, Token per second per gpu: 24928.496404728186
Epoch: 29, Global Step: 297300, Data Step: 594600, Loss: 3.1875, Token per second per gpu: 25035.980520127974
Epoch: 29, Global Step: 297400, Data Step: 594800, Loss: 3.1875, Token per second per gpu: 25062.06647047322
Epoch: 29, Global Step: 297500, Data Step: 595000, Loss: 3.0, Token per second per gpu: 25045.6020317258
Epoch: 29, Global Step: 297600, Data Step: 595200, Loss: 3.1875, Token per second per gpu: 25029.764183421918
Epoch: 29, Global Step: 297700, Data Step: 595400, Loss: 3.125, Token per second per gpu: 25013.05810251288
Epoch: 29, Global Step: 297800, Data Step: 595600, Loss: 2.9375, Token per second per gpu: 25020.24951624793
Epoch: 29, Global Step: 297900, Data Step: 595800, Loss: 2.96875, Token per second per gpu: 24411.64277434547
Epoch: 29, Global Step: 298000, Data Step: 596000, Loss: 3.03125, Token per second per gpu: 25024.940726246397
Epoch: 29, Global Step: 298100, Data Step: 596200, Loss: 3.21875, Token per second per gpu: 24998.31870330174
Epoch: 29, Global Step: 298200, Data Step: 596400, Loss: 2.984375, Token per second per gpu: 24987.291571653688
Epoch: 29, Global Step: 298300, Data Step: 596600, Loss: 3.0625, Token per second per gpu: 24988.319886160654
Epoch: 29, Global Step: 298400, Data Step: 596800, Loss: 3.03125, Token per second per gpu: 25007.81043050787
Epoch: 29, Global Step: 298500, Data Step: 597000, Loss: 3.25, Token per second per gpu: 25019.948318820658
Epoch: 29, Global Step: 298600, Data Step: 597200, Loss: 2.875, Token per second per gpu: 25046.83675820104
Epoch: 29, Global Step: 298700, Data Step: 597400, Loss: 2.875, Token per second per gpu: 24411.592010315537
Epoch: 29, Global Step: 298800, Data Step: 597600, Loss: 3.28125, Token per second per gpu: 25004.804519430272
Epoch: 29, Global Step: 298900, Data Step: 597800, Loss: 3.03125, Token per second per gpu: 24992.814883752726
Epoch: 29, Global Step: 299000, Data Step: 598000, Loss: 3.28125, Token per second per gpu: 24990.606527433985
Epoch: 29, Global Step: 299100, Data Step: 598200, Loss: 2.84375, Token per second per gpu: 24382.742458674904
Epoch: 29, Global Step: 299200, Data Step: 598400, Loss: 2.953125, Token per second per gpu: 25016.633346073744
Epoch: 29, Global Step: 299300, Data Step: 598600, Loss: 3.15625, Token per second per gpu: 25034.046085263737
Epoch: 29, Global Step: 299400, Data Step: 598800, Loss: 3.15625, Token per second per gpu: 25044.987102757783
Epoch: 29, Global Step: 299500, Data Step: 599000, Loss: 3.328125, Token per second per gpu: 25037.442998347127
Epoch: 29, Global Step: 299600, Data Step: 599200, Loss: 2.984375, Token per second per gpu: 22612.338997656843
Epoch: 29, Global Step: 299700, Data Step: 599400, Loss: 2.921875, Token per second per gpu: 21913.134384532397
Epoch: 29, Global Step: 299800, Data Step: 599600, Loss: 3.046875, Token per second per gpu: 22391.799290941046
Epoch: 29, Global Step: 299900, Data Step: 599800, Loss: 2.90625, Token per second per gpu: 22257.458252307086
Epoch: 29, Global Step: 300000, Data Step: 600000, Loss: 2.90625, Token per second per gpu: 22328.054558653712
I0330 23:26:58.433650 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0330 23:26:58.434074 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-30 23:26:58,434] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-30 23:26:58,438] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-30 23:26:58,439] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-30 23:26:59,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-30 23:26:59,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-30 23:26:59,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-30 23:26:59,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-30 23:26:59,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-30 23:26:59,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-30 23:26:59,051] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-30 23:27:01,739] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-30 23:27:01,740] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-30 23:27:01,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 23:27:01,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-30 23:27:01,849] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-30 23:27:01,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 23:27:01,908] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-30 23:27:01,923] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-30 23:27:01,924] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 23:27:02,000] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-30 23:27:02,000] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-30 23:27:02,000] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 23:27:02,000] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-30 23:27:02,000] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-30 23:27:02,000] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-30 23:27:02,000] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-30 23:27:02,000] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-30 23:27:02,000] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0330 23:27:02.001338 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0330 23:27:02.005825 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0330 23:27:02.006137 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0330 23:27:02.007866 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 29, Global Step: 300100, Data Step: 600200, Loss: 3.296875, Token per second per gpu: 21833.961991442364
Epoch: 29, Global Step: 300200, Data Step: 600400, Loss: 3.0625, Token per second per gpu: 22097.91041930839
Epoch: 29, Global Step: 300300, Data Step: 600600, Loss: 2.84375, Token per second per gpu: 22353.681916255824
Epoch: 29, Global Step: 300400, Data Step: 600800, Loss: 2.6875, Token per second per gpu: 22628.605937815762
Epoch: 29, Global Step: 300500, Data Step: 601000, Loss: 3.015625, Token per second per gpu: 22638.208798197797
Epoch: 29, Global Step: 300600, Data Step: 601200, Loss: 2.8125, Token per second per gpu: 22581.270522059312
Epoch: 29, Global Step: 300700, Data Step: 601400, Loss: 2.90625, Token per second per gpu: 22834.514845121597
Epoch: 29, Global Step: 300800, Data Step: 601600, Loss: 2.875, Token per second per gpu: 22842.31002954816
Epoch: 29, Global Step: 300900, Data Step: 601800, Loss: 2.890625, Token per second per gpu: 22613.174138445298
Epoch: 29, Global Step: 301000, Data Step: 602000, Loss: 2.765625, Token per second per gpu: 22455.862073588636
Epoch: 29, Global Step: 301100, Data Step: 602200, Loss: 3.03125, Token per second per gpu: 22556.61015948896
Epoch: 29, Global Step: 301200, Data Step: 602400, Loss: 2.78125, Token per second per gpu: 22284.72001658216
Epoch: 29, Global Step: 301300, Data Step: 602600, Loss: 3.046875, Token per second per gpu: 22304.568489752226
Epoch: 29, Global Step: 301400, Data Step: 602800, Loss: 3.078125, Token per second per gpu: 22897.236459143092
Epoch: 29, Global Step: 301500, Data Step: 603000, Loss: 3.109375, Token per second per gpu: 22413.572273230373
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0330 23:59:56.719632 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 30, Global Step: 301600, Data Step: 603200, Loss: 2.90625, Token per second per gpu: 21439.849161776947
Epoch: 30, Global Step: 301700, Data Step: 603400, Loss: 3.296875, Token per second per gpu: 22813.955564465887
Epoch: 30, Global Step: 301800, Data Step: 603600, Loss: 3.5, Token per second per gpu: 22727.0928303574
Epoch: 30, Global Step: 301900, Data Step: 603800, Loss: 2.84375, Token per second per gpu: 22428.930528144414
Epoch: 30, Global Step: 302000, Data Step: 604000, Loss: 3.09375, Token per second per gpu: 22672.908006494203
Epoch: 30, Global Step: 302100, Data Step: 604200, Loss: 2.984375, Token per second per gpu: 22451.676640110956
Epoch: 30, Global Step: 302200, Data Step: 604400, Loss: 2.9375, Token per second per gpu: 22438.175735959332
Epoch: 30, Global Step: 302300, Data Step: 604600, Loss: 3.078125, Token per second per gpu: 22473.430018630912
Epoch: 30, Global Step: 302400, Data Step: 604800, Loss: 3.078125, Token per second per gpu: 22210.420535023004
Epoch: 30, Global Step: 302500, Data Step: 605000, Loss: 2.96875, Token per second per gpu: 22455.295563085623
Epoch: 30, Global Step: 302600, Data Step: 605200, Loss: 3.203125, Token per second per gpu: 22306.612505924517
Epoch: 30, Global Step: 302700, Data Step: 605400, Loss: 3.21875, Token per second per gpu: 22738.526128231963
Epoch: 30, Global Step: 302800, Data Step: 605600, Loss: 2.75, Token per second per gpu: 22639.396999710905
Epoch: 30, Global Step: 302900, Data Step: 605800, Loss: 2.953125, Token per second per gpu: 22257.98283441896
Epoch: 30, Global Step: 303000, Data Step: 606000, Loss: 3.25, Token per second per gpu: 22108.712041728544
Epoch: 30, Global Step: 303100, Data Step: 606200, Loss: 3.15625, Token per second per gpu: 22322.415459955264
Epoch: 30, Global Step: 303200, Data Step: 606400, Loss: 3.046875, Token per second per gpu: 22206.556124317347
Epoch: 30, Global Step: 303300, Data Step: 606600, Loss: 3.015625, Token per second per gpu: 22388.858462436994
Epoch: 30, Global Step: 303400, Data Step: 606800, Loss: 2.90625, Token per second per gpu: 22583.519845084727
Epoch: 30, Global Step: 303500, Data Step: 607000, Loss: 3.203125, Token per second per gpu: 22401.002815210486
Epoch: 30, Global Step: 303600, Data Step: 607200, Loss: 2.875, Token per second per gpu: 22926.88417559337
Epoch: 30, Global Step: 303700, Data Step: 607400, Loss: 2.90625, Token per second per gpu: 22862.749631330076
Epoch: 30, Global Step: 303800, Data Step: 607600, Loss: 3.03125, Token per second per gpu: 22586.08718763529
Epoch: 30, Global Step: 303900, Data Step: 607800, Loss: 2.8125, Token per second per gpu: 22141.40702197802
Epoch: 30, Global Step: 304000, Data Step: 608000, Loss: 3.046875, Token per second per gpu: 22456.749946681095
Epoch: 30, Global Step: 304100, Data Step: 608200, Loss: 3.046875, Token per second per gpu: 22453.34236255585
Epoch: 30, Global Step: 304200, Data Step: 608400, Loss: 2.921875, Token per second per gpu: 22516.85412208806
Epoch: 30, Global Step: 304300, Data Step: 608600, Loss: 3.171875, Token per second per gpu: 22358.114315761573
Epoch: 30, Global Step: 304400, Data Step: 608800, Loss: 2.96875, Token per second per gpu: 21941.814024398744
Epoch: 30, Global Step: 304500, Data Step: 609000, Loss: 3.1875, Token per second per gpu: 22245.51369482072
Epoch: 30, Global Step: 304600, Data Step: 609200, Loss: 2.5, Token per second per gpu: 23060.656055901178
Epoch: 30, Global Step: 304700, Data Step: 609400, Loss: 2.96875, Token per second per gpu: 22455.628344363602
Epoch: 30, Global Step: 304800, Data Step: 609600, Loss: 3.015625, Token per second per gpu: 22435.48647895271
Epoch: 30, Global Step: 304900, Data Step: 609800, Loss: 3.203125, Token per second per gpu: 22746.74907989379
Epoch: 30, Global Step: 305000, Data Step: 610000, Loss: 2.890625, Token per second per gpu: 22077.810731922575
Epoch: 30, Global Step: 305100, Data Step: 610200, Loss: 2.96875, Token per second per gpu: 22455.127464701924
Epoch: 30, Global Step: 305200, Data Step: 610400, Loss: 2.953125, Token per second per gpu: 22428.44541394027
Epoch: 30, Global Step: 305300, Data Step: 610600, Loss: 2.9375, Token per second per gpu: 22728.615265732526
Epoch: 30, Global Step: 305400, Data Step: 610800, Loss: 2.84375, Token per second per gpu: 22606.062240892494
Epoch: 30, Global Step: 305500, Data Step: 611000, Loss: 2.796875, Token per second per gpu: 22997.554876349783
Epoch: 30, Global Step: 305600, Data Step: 611200, Loss: 3.234375, Token per second per gpu: 22489.744179660855
Epoch: 30, Global Step: 305700, Data Step: 611400, Loss: 3.234375, Token per second per gpu: 22524.626343141314
Epoch: 30, Global Step: 305800, Data Step: 611600, Loss: 3.15625, Token per second per gpu: 22829.96974753842
Epoch: 30, Global Step: 305900, Data Step: 611800, Loss: 3.203125, Token per second per gpu: 22373.55576454704
Epoch: 30, Global Step: 306000, Data Step: 612000, Loss: 2.765625, Token per second per gpu: 22227.710082483907
Epoch: 30, Global Step: 306100, Data Step: 612200, Loss: 3.109375, Token per second per gpu: 22330.20958856775
Epoch: 30, Global Step: 306200, Data Step: 612400, Loss: 2.71875, Token per second per gpu: 22704.287022862758
Epoch: 30, Global Step: 306300, Data Step: 612600, Loss: 2.71875, Token per second per gpu: 22545.569210005135
Epoch: 30, Global Step: 306400, Data Step: 612800, Loss: 3.328125, Token per second per gpu: 22583.00703768204
Epoch: 30, Global Step: 306500, Data Step: 613000, Loss: 2.765625, Token per second per gpu: 22468.278002549407
Epoch: 30, Global Step: 306600, Data Step: 613200, Loss: 3.359375, Token per second per gpu: 22409.54634710197
Epoch: 30, Global Step: 306700, Data Step: 613400, Loss: 3.140625, Token per second per gpu: 22420.71681832723
Epoch: 30, Global Step: 306800, Data Step: 613600, Loss: 2.828125, Token per second per gpu: 22633.63705112545
Epoch: 30, Global Step: 306900, Data Step: 613800, Loss: 2.78125, Token per second per gpu: 22375.622143701577
Epoch: 30, Global Step: 307000, Data Step: 614000, Loss: 2.84375, Token per second per gpu: 22393.033706227583
Epoch: 30, Global Step: 307100, Data Step: 614200, Loss: 3.15625, Token per second per gpu: 22888.554020994205
Epoch: 30, Global Step: 307200, Data Step: 614400, Loss: 3.46875, Token per second per gpu: 22301.195854347276
Epoch: 30, Global Step: 307300, Data Step: 614600, Loss: 3.375, Token per second per gpu: 22220.083586107972
Epoch: 30, Global Step: 307400, Data Step: 614800, Loss: 3.078125, Token per second per gpu: 22496.011591763516
Epoch: 30, Global Step: 307500, Data Step: 615000, Loss: 3.015625, Token per second per gpu: 22395.338107446354
Epoch: 30, Global Step: 307600, Data Step: 615200, Loss: 3.125, Token per second per gpu: 22278.381788456685
Epoch: 30, Global Step: 307700, Data Step: 615400, Loss: 3.21875, Token per second per gpu: 22594.137900901722
Epoch: 30, Global Step: 307800, Data Step: 615600, Loss: 3.234375, Token per second per gpu: 22255.26106701606
Epoch: 30, Global Step: 307900, Data Step: 615800, Loss: 3.25, Token per second per gpu: 22564.138202763934
Epoch: 30, Global Step: 308000, Data Step: 616000, Loss: 2.859375, Token per second per gpu: 22329.232878509585
Epoch: 30, Global Step: 308100, Data Step: 616200, Loss: 3.109375, Token per second per gpu: 22444.261095199152
Epoch: 30, Global Step: 308200, Data Step: 616400, Loss: 3.28125, Token per second per gpu: 22295.961916217813
Epoch: 30, Global Step: 308300, Data Step: 616600, Loss: 3.125, Token per second per gpu: 22492.41651536255
Epoch: 30, Global Step: 308400, Data Step: 616800, Loss: 3.1875, Token per second per gpu: 22890.24429822963
Epoch: 30, Global Step: 308500, Data Step: 617000, Loss: 3.125, Token per second per gpu: 22607.31603569079
Epoch: 30, Global Step: 308600, Data Step: 617200, Loss: 3.09375, Token per second per gpu: 22585.466453737918
Epoch: 30, Global Step: 308700, Data Step: 617400, Loss: 2.8125, Token per second per gpu: 22141.786775768036
Epoch: 30, Global Step: 308800, Data Step: 617600, Loss: 3.203125, Token per second per gpu: 23002.23996953122
Epoch: 30, Global Step: 308900, Data Step: 617800, Loss: 3.125, Token per second per gpu: 22362.149062635443
Epoch: 30, Global Step: 309000, Data Step: 618000, Loss: 3.03125, Token per second per gpu: 22509.489339241125
Epoch: 30, Global Step: 309100, Data Step: 618200, Loss: 3.25, Token per second per gpu: 22215.71043887059
Epoch: 30, Global Step: 309200, Data Step: 618400, Loss: 3.140625, Token per second per gpu: 22379.24656439458
Epoch: 30, Global Step: 309300, Data Step: 618600, Loss: 2.65625, Token per second per gpu: 22766.74482537016
Epoch: 30, Global Step: 309400, Data Step: 618800, Loss: 3.078125, Token per second per gpu: 22500.0042328619
Epoch: 30, Global Step: 309500, Data Step: 619000, Loss: 2.59375, Token per second per gpu: 22431.75276058933
Epoch: 30, Global Step: 309600, Data Step: 619200, Loss: 2.875, Token per second per gpu: 22285.558884594928
Epoch: 30, Global Step: 309700, Data Step: 619400, Loss: 2.890625, Token per second per gpu: 22397.208306615084
Epoch: 30, Global Step: 309800, Data Step: 619600, Loss: 2.765625, Token per second per gpu: 22343.17173489103
Epoch: 30, Global Step: 309900, Data Step: 619800, Loss: 3.140625, Token per second per gpu: 22529.726498277258
Epoch: 30, Global Step: 310000, Data Step: 620000, Loss: 2.421875, Token per second per gpu: 22346.582999442377
I0331 03:00:36.910523 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0331 03:00:36.912871 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-31 03:00:36,920] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-31 03:00:36,936] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-31 03:00:36,944] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-31 03:00:38,026] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-31 03:00:38,063] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-31 03:00:38,063] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-31 03:00:38,063] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-31 03:00:38,063] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-31 03:00:38,071] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-31 03:00:38,073] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-31 03:00:38,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-31 03:00:38,592] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-31 03:00:38,592] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 03:00:39,722] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-31 03:00:39,732] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-31 03:00:39,733] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 03:00:39,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-31 03:00:39,994] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-31 03:00:39,995] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 03:00:40,017] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-31 03:00:40,018] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-31 03:00:40,018] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 03:00:40,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-31 03:00:40,046] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-31 03:00:40,046] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 03:00:40,103] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-31 03:00:40,103] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-31 03:00:40,103] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0331 03:00:40.104133 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0331 03:00:40.104813 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0331 03:00:40.105063 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0331 03:00:40.106341 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 30, Global Step: 310100, Data Step: 620200, Loss: 2.984375, Token per second per gpu: 21539.814191911875
Epoch: 30, Global Step: 310200, Data Step: 620400, Loss: 3.140625, Token per second per gpu: 22601.782913031562
Epoch: 30, Global Step: 310300, Data Step: 620600, Loss: 3.375, Token per second per gpu: 22554.54605645017
Epoch: 30, Global Step: 310400, Data Step: 620800, Loss: 2.96875, Token per second per gpu: 22405.54613705898
Epoch: 30, Global Step: 310500, Data Step: 621000, Loss: 2.96875, Token per second per gpu: 22622.66641442298
Epoch: 30, Global Step: 310600, Data Step: 621200, Loss: 2.796875, Token per second per gpu: 22285.367457279586
Epoch: 30, Global Step: 310700, Data Step: 621400, Loss: 3.203125, Token per second per gpu: 22245.23323935709
Epoch: 30, Global Step: 310800, Data Step: 621600, Loss: 2.875, Token per second per gpu: 22828.765039287977
Epoch: 30, Global Step: 310900, Data Step: 621800, Loss: 2.828125, Token per second per gpu: 22543.94623844256
Epoch: 30, Global Step: 311000, Data Step: 622000, Loss: 2.921875, Token per second per gpu: 22462.659255312847
Epoch: 30, Global Step: 311100, Data Step: 622200, Loss: 3.125, Token per second per gpu: 22306.38603357792
Epoch: 30, Global Step: 311200, Data Step: 622400, Loss: 3.421875, Token per second per gpu: 22757.632948547718
Epoch: 30, Global Step: 311300, Data Step: 622600, Loss: 2.84375, Token per second per gpu: 22654.692970451113
Epoch: 30, Global Step: 311400, Data Step: 622800, Loss: 3.015625, Token per second per gpu: 22629.15799598698
Epoch: 30, Global Step: 311500, Data Step: 623000, Loss: 3.125, Token per second per gpu: 22392.658983831672
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0331 03:34:45.408870 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 31, Global Step: 311600, Data Step: 623200, Loss: 3.078125, Token per second per gpu: 20913.59243516182
Epoch: 31, Global Step: 311700, Data Step: 623400, Loss: 3.0, Token per second per gpu: 22513.62586382461
Epoch: 31, Global Step: 311800, Data Step: 623600, Loss: 3.25, Token per second per gpu: 22328.07989933233
Epoch: 31, Global Step: 311900, Data Step: 623800, Loss: 3.125, Token per second per gpu: 22149.835895025626
Epoch: 31, Global Step: 312000, Data Step: 624000, Loss: 3.046875, Token per second per gpu: 23294.299599144902
Epoch: 31, Global Step: 312100, Data Step: 624200, Loss: 3.046875, Token per second per gpu: 22817.436511322598
Epoch: 31, Global Step: 312200, Data Step: 624400, Loss: 2.875, Token per second per gpu: 22784.15816344603
Epoch: 31, Global Step: 312300, Data Step: 624600, Loss: 2.625, Token per second per gpu: 23070.496347562217
Epoch: 31, Global Step: 312400, Data Step: 624800, Loss: 2.96875, Token per second per gpu: 22892.960255190155
Epoch: 31, Global Step: 312500, Data Step: 625000, Loss: 2.78125, Token per second per gpu: 23225.135812663964
Epoch: 31, Global Step: 312600, Data Step: 625200, Loss: 3.0625, Token per second per gpu: 22704.75734275012
Epoch: 31, Global Step: 312700, Data Step: 625400, Loss: 2.84375, Token per second per gpu: 22582.904571877785
Epoch: 31, Global Step: 312800, Data Step: 625600, Loss: 2.9375, Token per second per gpu: 22578.05896327442
Epoch: 31, Global Step: 312900, Data Step: 625800, Loss: 2.78125, Token per second per gpu: 22479.316777540025
Epoch: 31, Global Step: 313000, Data Step: 626000, Loss: 2.75, Token per second per gpu: 22521.470849102014
Epoch: 31, Global Step: 313100, Data Step: 626200, Loss: 3.328125, Token per second per gpu: 23260.704550511135
Epoch: 31, Global Step: 313200, Data Step: 626400, Loss: 2.984375, Token per second per gpu: 22995.153205617225
Epoch: 31, Global Step: 313300, Data Step: 626600, Loss: 3.03125, Token per second per gpu: 23105.313557493813
Epoch: 31, Global Step: 313400, Data Step: 626800, Loss: 2.828125, Token per second per gpu: 22923.572041224135
Epoch: 31, Global Step: 313500, Data Step: 627000, Loss: 3.109375, Token per second per gpu: 22858.184896557155
Epoch: 31, Global Step: 313600, Data Step: 627200, Loss: 3.46875, Token per second per gpu: 22706.069356996155
Epoch: 31, Global Step: 313700, Data Step: 627400, Loss: 3.0625, Token per second per gpu: 22677.303501658294
Epoch: 31, Global Step: 313800, Data Step: 627600, Loss: 3.21875, Token per second per gpu: 22782.859496888166
Epoch: 31, Global Step: 313900, Data Step: 627800, Loss: 3.046875, Token per second per gpu: 22164.261528599607
Epoch: 31, Global Step: 314000, Data Step: 628000, Loss: 2.9375, Token per second per gpu: 22799.336686955012
Epoch: 31, Global Step: 314100, Data Step: 628200, Loss: 3.234375, Token per second per gpu: 22468.78093182133
Epoch: 31, Global Step: 314200, Data Step: 628400, Loss: 3.046875, Token per second per gpu: 22903.0533807091
Epoch: 31, Global Step: 314300, Data Step: 628600, Loss: 2.640625, Token per second per gpu: 22182.77516088578
Epoch: 31, Global Step: 314400, Data Step: 628800, Loss: 3.078125, Token per second per gpu: 22471.67289859453
Epoch: 31, Global Step: 314500, Data Step: 629000, Loss: 3.4375, Token per second per gpu: 22841.136540628868
Epoch: 31, Global Step: 314600, Data Step: 629200, Loss: 3.375, Token per second per gpu: 22830.67126541679
Epoch: 31, Global Step: 314700, Data Step: 629400, Loss: 2.984375, Token per second per gpu: 22512.735711436464
Epoch: 31, Global Step: 314800, Data Step: 629600, Loss: 2.75, Token per second per gpu: 21747.58647447681
Epoch: 31, Global Step: 314900, Data Step: 629800, Loss: 3.09375, Token per second per gpu: 22803.251339056285
Epoch: 31, Global Step: 315000, Data Step: 630000, Loss: 2.90625, Token per second per gpu: 22875.178234235977
Epoch: 31, Global Step: 315100, Data Step: 630200, Loss: 3.3125, Token per second per gpu: 22515.87994458127
Epoch: 31, Global Step: 315200, Data Step: 630400, Loss: 3.203125, Token per second per gpu: 23674.150313546008
Epoch: 31, Global Step: 315300, Data Step: 630600, Loss: 3.34375, Token per second per gpu: 23009.633259804425
Epoch: 31, Global Step: 315400, Data Step: 630800, Loss: 2.71875, Token per second per gpu: 22725.713013537013
Epoch: 31, Global Step: 315500, Data Step: 631000, Loss: 2.96875, Token per second per gpu: 22538.916771936543
Epoch: 31, Global Step: 315600, Data Step: 631200, Loss: 2.8125, Token per second per gpu: 22862.272743244852
Epoch: 31, Global Step: 315700, Data Step: 631400, Loss: 2.9375, Token per second per gpu: 22757.302560380795
Epoch: 31, Global Step: 315800, Data Step: 631600, Loss: 2.90625, Token per second per gpu: 22684.877188507136
Epoch: 31, Global Step: 315900, Data Step: 631800, Loss: 2.828125, Token per second per gpu: 22648.18945004886
Epoch: 31, Global Step: 316000, Data Step: 632000, Loss: 3.03125, Token per second per gpu: 22696.502669361555
Epoch: 31, Global Step: 316100, Data Step: 632200, Loss: 3.125, Token per second per gpu: 22746.7940982732
Epoch: 31, Global Step: 316200, Data Step: 632400, Loss: 2.859375, Token per second per gpu: 22890.744346487074
Epoch: 31, Global Step: 316300, Data Step: 632600, Loss: 3.21875, Token per second per gpu: 23208.774770732904
Epoch: 31, Global Step: 316400, Data Step: 632800, Loss: 3.34375, Token per second per gpu: 22697.443408714273
Epoch: 31, Global Step: 316500, Data Step: 633000, Loss: 2.671875, Token per second per gpu: 22856.24431933301
Epoch: 31, Global Step: 316600, Data Step: 633200, Loss: 2.96875, Token per second per gpu: 22621.343389975784
Epoch: 31, Global Step: 316700, Data Step: 633400, Loss: 3.15625, Token per second per gpu: 22484.026095484536
Epoch: 31, Global Step: 316800, Data Step: 633600, Loss: 2.75, Token per second per gpu: 22376.539537787874
Epoch: 31, Global Step: 316900, Data Step: 633800, Loss: 3.046875, Token per second per gpu: 22504.98100066074
Epoch: 31, Global Step: 317000, Data Step: 634000, Loss: 3.078125, Token per second per gpu: 22474.755701690847
Epoch: 31, Global Step: 317100, Data Step: 634200, Loss: 3.296875, Token per second per gpu: 22563.113148615168
Epoch: 31, Global Step: 317200, Data Step: 634400, Loss: 3.0625, Token per second per gpu: 22551.611796474783
Epoch: 31, Global Step: 317300, Data Step: 634600, Loss: 2.875, Token per second per gpu: 22415.878283138674
Epoch: 31, Global Step: 317400, Data Step: 634800, Loss: 2.90625, Token per second per gpu: 22817.87967825123
Epoch: 31, Global Step: 317500, Data Step: 635000, Loss: 3.109375, Token per second per gpu: 22928.102482605012
Epoch: 31, Global Step: 317600, Data Step: 635200, Loss: 2.953125, Token per second per gpu: 22627.132981912328
Epoch: 31, Global Step: 317700, Data Step: 635400, Loss: 2.859375, Token per second per gpu: 21878.236506044843
Epoch: 31, Global Step: 317800, Data Step: 635600, Loss: 3.25, Token per second per gpu: 22322.30041270123
Epoch: 31, Global Step: 317900, Data Step: 635800, Loss: 2.78125, Token per second per gpu: 22455.770693607046
Epoch: 31, Global Step: 318000, Data Step: 636000, Loss: 2.75, Token per second per gpu: 22657.729171186445
Epoch: 31, Global Step: 318100, Data Step: 636200, Loss: 2.921875, Token per second per gpu: 22775.335766072032
Epoch: 31, Global Step: 318200, Data Step: 636400, Loss: 3.265625, Token per second per gpu: 22442.639291477015
Epoch: 31, Global Step: 318300, Data Step: 636600, Loss: 2.984375, Token per second per gpu: 22466.221927208862
Epoch: 31, Global Step: 318400, Data Step: 636800, Loss: 3.203125, Token per second per gpu: 22302.27120014205
Epoch: 31, Global Step: 318500, Data Step: 637000, Loss: 2.90625, Token per second per gpu: 22312.05210703568
Epoch: 31, Global Step: 318600, Data Step: 637200, Loss: 2.703125, Token per second per gpu: 21674.940960678432
Epoch: 31, Global Step: 318700, Data Step: 637400, Loss: 3.328125, Token per second per gpu: 22693.69972080517
Epoch: 31, Global Step: 318800, Data Step: 637600, Loss: 2.75, Token per second per gpu: 22422.155238905307
Epoch: 31, Global Step: 318900, Data Step: 637800, Loss: 3.171875, Token per second per gpu: 22185.029929057448
Epoch: 31, Global Step: 319000, Data Step: 638000, Loss: 2.796875, Token per second per gpu: 22601.36002550028
Epoch: 31, Global Step: 319100, Data Step: 638200, Loss: 3.203125, Token per second per gpu: 22397.016368332126
Epoch: 31, Global Step: 319200, Data Step: 638400, Loss: 2.90625, Token per second per gpu: 22462.040943234177
Epoch: 31, Global Step: 319300, Data Step: 638600, Loss: 3.09375, Token per second per gpu: 22371.22054417675
Epoch: 31, Global Step: 319400, Data Step: 638800, Loss: 2.9375, Token per second per gpu: 22235.625664785555
Epoch: 31, Global Step: 319500, Data Step: 639000, Loss: 3.140625, Token per second per gpu: 22400.24504018325
Epoch: 31, Global Step: 319600, Data Step: 639200, Loss: 3.109375, Token per second per gpu: 22536.822436337083
Epoch: 31, Global Step: 319700, Data Step: 639400, Loss: 2.9375, Token per second per gpu: 22536.041233829124
Epoch: 31, Global Step: 319800, Data Step: 639600, Loss: 2.859375, Token per second per gpu: 22560.536868511528
Epoch: 31, Global Step: 319900, Data Step: 639800, Loss: 3.265625, Token per second per gpu: 22342.873396943418
Epoch: 31, Global Step: 320000, Data Step: 640000, Loss: 2.65625, Token per second per gpu: 22275.99257267722
I0331 06:33:16.148208 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0331 06:33:16.148595 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-31 06:33:16,148] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-31 06:33:16,152] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-31 06:33:16,152] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-31 06:33:16,698] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-31 06:33:16,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-31 06:33:16,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-31 06:33:16,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-31 06:33:16,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-31 06:33:16,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-31 06:33:16,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-31 06:33:18,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-31 06:33:18,809] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-31 06:33:18,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 06:33:18,814] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-31 06:33:18,814] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-31 06:33:18,814] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 06:33:18,827] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-31 06:33:18,843] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-31 06:33:18,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 06:33:18,892] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-31 06:33:18,893] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-31 06:33:18,893] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 06:33:18,919] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-31 06:33:18,919] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-31 06:33:18,919] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-31 06:33:18,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 06:33:18,919] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-31 06:33:18,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0331 06:33:18.919890 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0331 06:33:18.920496 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0331 06:33:18.920715 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0331 06:33:18.933598 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 31, Global Step: 320100, Data Step: 640200, Loss: 3.015625, Token per second per gpu: 21972.500491219154
Epoch: 31, Global Step: 320200, Data Step: 640400, Loss: 3.328125, Token per second per gpu: 22229.240141626644
Epoch: 31, Global Step: 320300, Data Step: 640600, Loss: 3.1875, Token per second per gpu: 22337.479272556815
Epoch: 31, Global Step: 320400, Data Step: 640800, Loss: 3.125, Token per second per gpu: 22159.704260479346
Epoch: 31, Global Step: 320500, Data Step: 641000, Loss: 3.109375, Token per second per gpu: 22418.656581457155
Epoch: 31, Global Step: 320600, Data Step: 641200, Loss: 2.921875, Token per second per gpu: 22214.33972536441
Epoch: 31, Global Step: 320700, Data Step: 641400, Loss: 3.25, Token per second per gpu: 22340.236723521182
Epoch: 31, Global Step: 320800, Data Step: 641600, Loss: 3.25, Token per second per gpu: 22533.155500739624
Epoch: 31, Global Step: 320900, Data Step: 641800, Loss: 2.796875, Token per second per gpu: 22146.95230672146
Epoch: 31, Global Step: 321000, Data Step: 642000, Loss: 2.765625, Token per second per gpu: 22328.208832275297
Epoch: 31, Global Step: 321100, Data Step: 642200, Loss: 2.90625, Token per second per gpu: 22252.893000128995
Epoch: 31, Global Step: 321200, Data Step: 642400, Loss: 3.015625, Token per second per gpu: 21950.594723669357
Epoch: 31, Global Step: 321300, Data Step: 642600, Loss: 3.234375, Token per second per gpu: 22192.792287814056
Epoch: 31, Global Step: 321400, Data Step: 642800, Loss: 3.296875, Token per second per gpu: 22651.330349956977
Epoch: 31, Global Step: 321500, Data Step: 643000, Loss: 3.015625, Token per second per gpu: 22389.553301462875
Epoch: 31, Global Step: 321600, Data Step: 643200, Loss: 3.21875, Token per second per gpu: 22212.651722157385
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0331 07:08:47.872544 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 32, Global Step: 321700, Data Step: 643400, Loss: 3.203125, Token per second per gpu: 21804.240840857154
Epoch: 32, Global Step: 321800, Data Step: 643600, Loss: 3.203125, Token per second per gpu: 22114.37079511503
Epoch: 32, Global Step: 321900, Data Step: 643800, Loss: 2.796875, Token per second per gpu: 21999.759342256562
Epoch: 32, Global Step: 322000, Data Step: 644000, Loss: 2.890625, Token per second per gpu: 22544.753319068346
Epoch: 32, Global Step: 322100, Data Step: 644200, Loss: 3.046875, Token per second per gpu: 22217.5984037048
Epoch: 32, Global Step: 322200, Data Step: 644400, Loss: 3.28125, Token per second per gpu: 22251.58799086899
Epoch: 32, Global Step: 322300, Data Step: 644600, Loss: 2.828125, Token per second per gpu: 22282.926471779298
Epoch: 32, Global Step: 322400, Data Step: 644800, Loss: 3.1875, Token per second per gpu: 22718.104562362463
Epoch: 32, Global Step: 322500, Data Step: 645000, Loss: 3.0, Token per second per gpu: 22510.638687468196
Epoch: 32, Global Step: 322600, Data Step: 645200, Loss: 2.921875, Token per second per gpu: 22323.708699074563
Epoch: 32, Global Step: 322700, Data Step: 645400, Loss: 3.171875, Token per second per gpu: 22414.509792878394
Epoch: 32, Global Step: 322800, Data Step: 645600, Loss: 3.0, Token per second per gpu: 21996.480052017152
Epoch: 32, Global Step: 322900, Data Step: 645800, Loss: 3.265625, Token per second per gpu: 22214.32489606065
Epoch: 32, Global Step: 323000, Data Step: 646000, Loss: 3.21875, Token per second per gpu: 22725.023702353745
Epoch: 32, Global Step: 323100, Data Step: 646200, Loss: 2.75, Token per second per gpu: 22469.31355821699
Epoch: 32, Global Step: 323200, Data Step: 646400, Loss: 3.171875, Token per second per gpu: 22298.149775835638
Epoch: 32, Global Step: 323300, Data Step: 646600, Loss: 2.8125, Token per second per gpu: 22244.82399845486
Epoch: 32, Global Step: 323400, Data Step: 646800, Loss: 2.984375, Token per second per gpu: 22259.1966328344
Epoch: 32, Global Step: 323500, Data Step: 647000, Loss: 3.21875, Token per second per gpu: 22142.979247564876
Epoch: 32, Global Step: 323600, Data Step: 647200, Loss: 2.921875, Token per second per gpu: 22477.20585682635
Epoch: 32, Global Step: 323700, Data Step: 647400, Loss: 3.015625, Token per second per gpu: 22830.26130069295
Epoch: 32, Global Step: 323800, Data Step: 647600, Loss: 2.65625, Token per second per gpu: 22335.535899239574
Epoch: 32, Global Step: 323900, Data Step: 647800, Loss: 3.078125, Token per second per gpu: 22391.296440010174
Epoch: 32, Global Step: 324000, Data Step: 648000, Loss: 3.390625, Token per second per gpu: 22537.029897169716
Epoch: 32, Global Step: 324100, Data Step: 648200, Loss: 2.9375, Token per second per gpu: 22403.389879923823
Epoch: 32, Global Step: 324200, Data Step: 648400, Loss: 2.875, Token per second per gpu: 22197.202140457466
Epoch: 32, Global Step: 324300, Data Step: 648600, Loss: 3.265625, Token per second per gpu: 22367.747206069744
Epoch: 32, Global Step: 324400, Data Step: 648800, Loss: 3.140625, Token per second per gpu: 22147.616862022736
Epoch: 32, Global Step: 324500, Data Step: 649000, Loss: 3.125, Token per second per gpu: 22507.037928365873
Epoch: 32, Global Step: 324600, Data Step: 649200, Loss: 3.046875, Token per second per gpu: 22183.492180015848
Epoch: 32, Global Step: 324700, Data Step: 649400, Loss: 3.0625, Token per second per gpu: 21258.976461211507
Epoch: 32, Global Step: 324800, Data Step: 649600, Loss: 2.671875, Token per second per gpu: 22440.64464381503
Epoch: 32, Global Step: 324900, Data Step: 649800, Loss: 3.015625, Token per second per gpu: 22305.19208379879
Epoch: 32, Global Step: 325000, Data Step: 650000, Loss: 3.0625, Token per second per gpu: 22030.96103314908
Epoch: 32, Global Step: 325100, Data Step: 650200, Loss: 3.171875, Token per second per gpu: 22213.95858177603
Epoch: 32, Global Step: 325200, Data Step: 650400, Loss: 2.96875, Token per second per gpu: 22205.73613333978
Epoch: 32, Global Step: 325300, Data Step: 650600, Loss: 3.203125, Token per second per gpu: 22058.79167596991
Epoch: 32, Global Step: 325400, Data Step: 650800, Loss: 3.0, Token per second per gpu: 22252.23317985932
Epoch: 32, Global Step: 325500, Data Step: 651000, Loss: 2.6875, Token per second per gpu: 22440.89944799112
Epoch: 32, Global Step: 325600, Data Step: 651200, Loss: 2.578125, Token per second per gpu: 22503.625254223098
Epoch: 32, Global Step: 325700, Data Step: 651400, Loss: 2.796875, Token per second per gpu: 22250.661222273135
Epoch: 32, Global Step: 325800, Data Step: 651600, Loss: 3.0625, Token per second per gpu: 22545.604767193334
Epoch: 32, Global Step: 325900, Data Step: 651800, Loss: 2.78125, Token per second per gpu: 22243.201275064235
Epoch: 32, Global Step: 326000, Data Step: 652000, Loss: 3.25, Token per second per gpu: 22320.721097552814
Epoch: 32, Global Step: 326100, Data Step: 652200, Loss: 3.09375, Token per second per gpu: 22400.68465312106
Epoch: 32, Global Step: 326200, Data Step: 652400, Loss: 3.046875, Token per second per gpu: 22242.748121385113
Epoch: 32, Global Step: 326300, Data Step: 652600, Loss: 3.265625, Token per second per gpu: 22684.62252219008
Epoch: 32, Global Step: 326400, Data Step: 652800, Loss: 3.40625, Token per second per gpu: 22332.598723379695
Epoch: 32, Global Step: 326500, Data Step: 653000, Loss: 2.9375, Token per second per gpu: 22332.8550022859
Epoch: 32, Global Step: 326600, Data Step: 653200, Loss: 2.828125, Token per second per gpu: 22785.0164449478
Epoch: 32, Global Step: 326700, Data Step: 653400, Loss: 3.125, Token per second per gpu: 22627.09428496548
Epoch: 32, Global Step: 326800, Data Step: 653600, Loss: 3.1875, Token per second per gpu: 22404.852549062056
Epoch: 32, Global Step: 326900, Data Step: 653800, Loss: 2.953125, Token per second per gpu: 22792.155222400826
Epoch: 32, Global Step: 327000, Data Step: 654000, Loss: 2.984375, Token per second per gpu: 22407.256312065565
Epoch: 32, Global Step: 327100, Data Step: 654200, Loss: 3.078125, Token per second per gpu: 22364.49303083418
Epoch: 32, Global Step: 327200, Data Step: 654400, Loss: 3.25, Token per second per gpu: 22245.572933031814
Epoch: 32, Global Step: 327300, Data Step: 654600, Loss: 2.703125, Token per second per gpu: 22408.00458533582
Epoch: 32, Global Step: 327400, Data Step: 654800, Loss: 3.0, Token per second per gpu: 22164.860626928166
Epoch: 32, Global Step: 327500, Data Step: 655000, Loss: 3.25, Token per second per gpu: 22163.50423266925
Epoch: 32, Global Step: 327600, Data Step: 655200, Loss: 2.796875, Token per second per gpu: 21659.15251797908
Epoch: 32, Global Step: 327700, Data Step: 655400, Loss: 3.03125, Token per second per gpu: 22223.93606560007
Epoch: 32, Global Step: 327800, Data Step: 655600, Loss: 2.90625, Token per second per gpu: 22540.14529632788
Epoch: 32, Global Step: 327900, Data Step: 655800, Loss: 3.015625, Token per second per gpu: 22240.85284978745
Epoch: 32, Global Step: 328000, Data Step: 656000, Loss: 2.953125, Token per second per gpu: 22206.25346315637
Epoch: 32, Global Step: 328100, Data Step: 656200, Loss: 3.0625, Token per second per gpu: 25164.376419246484
Epoch: 32, Global Step: 328200, Data Step: 656400, Loss: 2.828125, Token per second per gpu: 25269.29132439071
Epoch: 32, Global Step: 328300, Data Step: 656600, Loss: 2.890625, Token per second per gpu: 25191.121028700025
Epoch: 32, Global Step: 328400, Data Step: 656800, Loss: 2.6875, Token per second per gpu: 25101.455240139047
Epoch: 32, Global Step: 328500, Data Step: 657000, Loss: 3.125, Token per second per gpu: 25041.11875905891
Epoch: 32, Global Step: 328600, Data Step: 657200, Loss: 2.84375, Token per second per gpu: 24424.635092073717
Epoch: 32, Global Step: 328700, Data Step: 657400, Loss: 3.09375, Token per second per gpu: 25052.572021427095
Epoch: 32, Global Step: 328800, Data Step: 657600, Loss: 2.75, Token per second per gpu: 25047.920308119643
Epoch: 32, Global Step: 328900, Data Step: 657800, Loss: 3.359375, Token per second per gpu: 25038.965022673812
Epoch: 32, Global Step: 329000, Data Step: 658000, Loss: 3.1875, Token per second per gpu: 25034.824275016123
Epoch: 32, Global Step: 329100, Data Step: 658200, Loss: 2.921875, Token per second per gpu: 25032.930586206294
Epoch: 32, Global Step: 329200, Data Step: 658400, Loss: 3.109375, Token per second per gpu: 25015.90218681698
Epoch: 32, Global Step: 329300, Data Step: 658600, Loss: 3.25, Token per second per gpu: 25027.235578702785
Epoch: 32, Global Step: 329400, Data Step: 658800, Loss: 3.0, Token per second per gpu: 25026.224383558594
Epoch: 32, Global Step: 329500, Data Step: 659000, Loss: 3.03125, Token per second per gpu: 25029.582662780173
Epoch: 32, Global Step: 329600, Data Step: 659200, Loss: 3.078125, Token per second per gpu: 25018.59286468166
Epoch: 32, Global Step: 329700, Data Step: 659400, Loss: 3.203125, Token per second per gpu: 25006.343077519876
Epoch: 32, Global Step: 329800, Data Step: 659600, Loss: 2.9375, Token per second per gpu: 25015.755835761374
Epoch: 32, Global Step: 329900, Data Step: 659800, Loss: 3.15625, Token per second per gpu: 25026.11379066733
Epoch: 32, Global Step: 330000, Data Step: 660000, Loss: 3.046875, Token per second per gpu: 25020.691841856402
I0331 10:03:47.493688 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0331 10:03:47.494088 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-31 10:03:47,494] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-31 10:03:47,498] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-31 10:03:47,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-31 10:03:48,044] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-31 10:03:48,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-31 10:03:48,047] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-31 10:03:48,047] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-31 10:03:48,047] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-31 10:03:48,047] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-31 10:03:48,047] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-31 10:03:50,099] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-31 10:03:50,099] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-31 10:03:50,100] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 10:03:50,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-31 10:03:50,213] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-31 10:03:50,213] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 10:03:50,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-31 10:03:50,250] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-31 10:03:50,250] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 10:03:50,259] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-31 10:03:50,260] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-31 10:03:50,260] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-31 10:03:50,260] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-31 10:03:50,260] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 10:03:50,260] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 10:03:50,277] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-31 10:03:50,278] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-31 10:03:50,278] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0331 10:03:50.279249 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0331 10:03:50.279968 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0331 10:03:50.280191 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0331 10:03:50.281510 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 32, Global Step: 330100, Data Step: 660200, Loss: 2.71875, Token per second per gpu: 24446.71694541158
Epoch: 32, Global Step: 330200, Data Step: 660400, Loss: 3.359375, Token per second per gpu: 25010.29466894303
Epoch: 32, Global Step: 330300, Data Step: 660600, Loss: 3.125, Token per second per gpu: 25004.014893320484
Epoch: 32, Global Step: 330400, Data Step: 660800, Loss: 3.0, Token per second per gpu: 24995.385827634567
Epoch: 32, Global Step: 330500, Data Step: 661000, Loss: 3.046875, Token per second per gpu: 24990.95474033099
Epoch: 32, Global Step: 330600, Data Step: 661200, Loss: 2.75, Token per second per gpu: 24994.580556896122
Epoch: 32, Global Step: 330700, Data Step: 661400, Loss: 2.8125, Token per second per gpu: 24991.011561738
Epoch: 32, Global Step: 330800, Data Step: 661600, Loss: 3.09375, Token per second per gpu: 24982.641797753855
Epoch: 32, Global Step: 330900, Data Step: 661800, Loss: 3.15625, Token per second per gpu: 24987.551924573527
Epoch: 32, Global Step: 331000, Data Step: 662000, Loss: 3.171875, Token per second per gpu: 24992.82088216133
Epoch: 32, Global Step: 331100, Data Step: 662200, Loss: 2.53125, Token per second per gpu: 24987.859217261703
Epoch: 32, Global Step: 331200, Data Step: 662400, Loss: 2.859375, Token per second per gpu: 25064.9702359178
Epoch: 32, Global Step: 331300, Data Step: 662600, Loss: 3.046875, Token per second per gpu: 25089.710078318614
Epoch: 32, Global Step: 331400, Data Step: 662800, Loss: 2.734375, Token per second per gpu: 25066.90274281958
Epoch: 32, Global Step: 331500, Data Step: 663000, Loss: 3.203125, Token per second per gpu: 25041.960100511344
Epoch: 32, Global Step: 331600, Data Step: 663200, Loss: 3.015625, Token per second per gpu: 25043.87493797909
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0331 10:36:26.420918 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 33, Global Step: 331700, Data Step: 663400, Loss: 2.96875, Token per second per gpu: 24408.592670011793
Epoch: 33, Global Step: 331800, Data Step: 663600, Loss: 2.96875, Token per second per gpu: 25059.159062969495
Epoch: 33, Global Step: 331900, Data Step: 663800, Loss: 3.1875, Token per second per gpu: 25021.254216538393
Epoch: 33, Global Step: 332000, Data Step: 664000, Loss: 3.125, Token per second per gpu: 24975.256032341393
Epoch: 33, Global Step: 332100, Data Step: 664200, Loss: 2.609375, Token per second per gpu: 24981.048706741054
Epoch: 33, Global Step: 332200, Data Step: 664400, Loss: 2.8125, Token per second per gpu: 24984.384485024893
Epoch: 33, Global Step: 332300, Data Step: 664600, Loss: 3.109375, Token per second per gpu: 24361.14581581435
Epoch: 33, Global Step: 332400, Data Step: 664800, Loss: 3.078125, Token per second per gpu: 24990.474431390518
Epoch: 33, Global Step: 332500, Data Step: 665000, Loss: 3.15625, Token per second per gpu: 24977.820730775096
Epoch: 33, Global Step: 332600, Data Step: 665200, Loss: 2.828125, Token per second per gpu: 25002.27753951617
Epoch: 33, Global Step: 332700, Data Step: 665400, Loss: 3.0625, Token per second per gpu: 25056.139704438003
Epoch: 33, Global Step: 332800, Data Step: 665600, Loss: 3.046875, Token per second per gpu: 25074.107928020887
Epoch: 33, Global Step: 332900, Data Step: 665800, Loss: 3.125, Token per second per gpu: 25063.833413612214
Epoch: 33, Global Step: 333000, Data Step: 666000, Loss: 2.65625, Token per second per gpu: 25056.621865178113
Epoch: 33, Global Step: 333100, Data Step: 666200, Loss: 3.09375, Token per second per gpu: 25045.89403229995
Epoch: 33, Global Step: 333200, Data Step: 666400, Loss: 3.390625, Token per second per gpu: 25052.777413000167
Epoch: 33, Global Step: 333300, Data Step: 666600, Loss: 3.109375, Token per second per gpu: 25055.24019188908
Epoch: 33, Global Step: 333400, Data Step: 666800, Loss: 2.859375, Token per second per gpu: 25053.974549697956
Epoch: 33, Global Step: 333500, Data Step: 667000, Loss: 3.28125, Token per second per gpu: 25038.484579214768
Epoch: 33, Global Step: 333600, Data Step: 667200, Loss: 3.09375, Token per second per gpu: 25022.479390481043
Epoch: 33, Global Step: 333700, Data Step: 667400, Loss: 3.09375, Token per second per gpu: 25031.2129680928
Epoch: 33, Global Step: 333800, Data Step: 667600, Loss: 3.15625, Token per second per gpu: 25039.797291742063
Epoch: 33, Global Step: 333900, Data Step: 667800, Loss: 3.25, Token per second per gpu: 25037.215387877954
Epoch: 33, Global Step: 334000, Data Step: 668000, Loss: 3.21875, Token per second per gpu: 25047.92773535386
Epoch: 33, Global Step: 334100, Data Step: 668200, Loss: 2.90625, Token per second per gpu: 25069.290674724925
Epoch: 33, Global Step: 334200, Data Step: 668400, Loss: 2.796875, Token per second per gpu: 25081.854913865496
Epoch: 33, Global Step: 334300, Data Step: 668600, Loss: 2.9375, Token per second per gpu: 25077.422741180973
Epoch: 33, Global Step: 334400, Data Step: 668800, Loss: 3.328125, Token per second per gpu: 25083.268639832342
Epoch: 33, Global Step: 334500, Data Step: 669000, Loss: 3.109375, Token per second per gpu: 24463.35622859495
Epoch: 33, Global Step: 334600, Data Step: 669200, Loss: 2.953125, Token per second per gpu: 25100.164896982067
Epoch: 33, Global Step: 334700, Data Step: 669400, Loss: 3.046875, Token per second per gpu: 25073.39250561508
Epoch: 33, Global Step: 334800, Data Step: 669600, Loss: 2.890625, Token per second per gpu: 25065.224096458263
Epoch: 33, Global Step: 334900, Data Step: 669800, Loss: 2.890625, Token per second per gpu: 25069.92225062851
Epoch: 33, Global Step: 335000, Data Step: 670000, Loss: 3.328125, Token per second per gpu: 25058.350925486295
Epoch: 33, Global Step: 335100, Data Step: 670200, Loss: 3.140625, Token per second per gpu: 25065.389022748703
Epoch: 33, Global Step: 335200, Data Step: 670400, Loss: 3.15625, Token per second per gpu: 25072.727760424008
Epoch: 33, Global Step: 335300, Data Step: 670600, Loss: 2.765625, Token per second per gpu: 25029.97864048712
Epoch: 33, Global Step: 335400, Data Step: 670800, Loss: 2.703125, Token per second per gpu: 25046.250694559105
Epoch: 33, Global Step: 335500, Data Step: 671000, Loss: 2.859375, Token per second per gpu: 25048.56529932352
Epoch: 33, Global Step: 335600, Data Step: 671200, Loss: 3.03125, Token per second per gpu: 25027.770142060952
Epoch: 33, Global Step: 335700, Data Step: 671400, Loss: 3.15625, Token per second per gpu: 25040.298807930965
Epoch: 33, Global Step: 335800, Data Step: 671600, Loss: 3.0, Token per second per gpu: 25034.084062332622
Epoch: 33, Global Step: 335900, Data Step: 671800, Loss: 2.9375, Token per second per gpu: 25038.64344478035
Epoch: 33, Global Step: 336000, Data Step: 672000, Loss: 2.953125, Token per second per gpu: 25030.36218290235
Epoch: 33, Global Step: 336100, Data Step: 672200, Loss: 2.984375, Token per second per gpu: 25031.619060376048
Epoch: 33, Global Step: 336200, Data Step: 672400, Loss: 2.890625, Token per second per gpu: 25014.307492992866
Epoch: 33, Global Step: 336300, Data Step: 672600, Loss: 3.265625, Token per second per gpu: 25020.259103690496
Epoch: 33, Global Step: 336400, Data Step: 672800, Loss: 3.125, Token per second per gpu: 25006.505832146122
Epoch: 33, Global Step: 336500, Data Step: 673000, Loss: 3.015625, Token per second per gpu: 25013.027906526422
Epoch: 33, Global Step: 336600, Data Step: 673200, Loss: 3.140625, Token per second per gpu: 25023.86014846395
Epoch: 33, Global Step: 336700, Data Step: 673400, Loss: 2.9375, Token per second per gpu: 24994.56778262748
Epoch: 33, Global Step: 336800, Data Step: 673600, Loss: 2.90625, Token per second per gpu: 24997.598339473137
Epoch: 33, Global Step: 336900, Data Step: 673800, Loss: 3.125, Token per second per gpu: 25013.616663718287
Epoch: 33, Global Step: 337000, Data Step: 674000, Loss: 2.609375, Token per second per gpu: 25000.78020361673
Epoch: 33, Global Step: 337100, Data Step: 674200, Loss: 3.234375, Token per second per gpu: 24987.083479296158
Epoch: 33, Global Step: 337200, Data Step: 674400, Loss: 2.875, Token per second per gpu: 24981.66370310724
Epoch: 33, Global Step: 337300, Data Step: 674600, Loss: 3.265625, Token per second per gpu: 24980.683463654732
Epoch: 33, Global Step: 337400, Data Step: 674800, Loss: 2.796875, Token per second per gpu: 25057.96080701226
Epoch: 33, Global Step: 337500, Data Step: 675000, Loss: 2.96875, Token per second per gpu: 25084.86286639935
Epoch: 33, Global Step: 337600, Data Step: 675200, Loss: 2.9375, Token per second per gpu: 24467.40904702777
Epoch: 33, Global Step: 337700, Data Step: 675400, Loss: 2.8125, Token per second per gpu: 25062.524211665514
Epoch: 33, Global Step: 337800, Data Step: 675600, Loss: 2.75, Token per second per gpu: 25052.55394006361
Epoch: 33, Global Step: 337900, Data Step: 675800, Loss: 3.09375, Token per second per gpu: 25043.752091342532
Epoch: 33, Global Step: 338000, Data Step: 676000, Loss: 3.125, Token per second per gpu: 25042.8506125138
Epoch: 33, Global Step: 338100, Data Step: 676200, Loss: 3.078125, Token per second per gpu: 25051.61073179497
Epoch: 33, Global Step: 338200, Data Step: 676400, Loss: 2.875, Token per second per gpu: 25075.7954165408
Epoch: 33, Global Step: 338300, Data Step: 676600, Loss: 3.109375, Token per second per gpu: 25069.382607388183
Epoch: 33, Global Step: 338400, Data Step: 676800, Loss: 2.8125, Token per second per gpu: 25047.344892642916
Epoch: 33, Global Step: 338500, Data Step: 677000, Loss: 3.015625, Token per second per gpu: 24416.82413434695
Epoch: 33, Global Step: 338600, Data Step: 677200, Loss: 2.921875, Token per second per gpu: 25044.793366403963
Epoch: 33, Global Step: 338700, Data Step: 677400, Loss: 2.859375, Token per second per gpu: 25050.906671092973
Epoch: 33, Global Step: 338800, Data Step: 677600, Loss: 2.921875, Token per second per gpu: 25047.86759046489
Epoch: 33, Global Step: 338900, Data Step: 677800, Loss: 3.0625, Token per second per gpu: 25046.83146091459
Epoch: 33, Global Step: 339000, Data Step: 678000, Loss: 2.984375, Token per second per gpu: 25050.781002082545
Epoch: 33, Global Step: 339100, Data Step: 678200, Loss: 2.84375, Token per second per gpu: 25067.221717809345
Epoch: 33, Global Step: 339200, Data Step: 678400, Loss: 3.046875, Token per second per gpu: 25054.303380916637
Epoch: 33, Global Step: 339300, Data Step: 678600, Loss: 2.9375, Token per second per gpu: 25049.085398963456
Epoch: 33, Global Step: 339400, Data Step: 678800, Loss: 2.78125, Token per second per gpu: 25067.413252354578
Epoch: 33, Global Step: 339500, Data Step: 679000, Loss: 3.125, Token per second per gpu: 25035.02226740836
Epoch: 33, Global Step: 339600, Data Step: 679200, Loss: 3.03125, Token per second per gpu: 25031.390622119765
Epoch: 33, Global Step: 339700, Data Step: 679400, Loss: 3.046875, Token per second per gpu: 25030.789150231787
Epoch: 33, Global Step: 339800, Data Step: 679600, Loss: 2.6875, Token per second per gpu: 25040.717860822893
Epoch: 33, Global Step: 339900, Data Step: 679800, Loss: 3.03125, Token per second per gpu: 25043.100079368553
Epoch: 33, Global Step: 340000, Data Step: 680000, Loss: 3.125, Token per second per gpu: 25022.933094985507
I0331 13:15:48.244354 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0331 13:15:48.244772 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-31 13:15:48,245] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-31 13:15:48,249] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-31 13:15:48,249] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-31 13:15:48,795] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-31 13:15:48,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-31 13:15:48,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-31 13:15:48,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-31 13:15:48,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-31 13:15:48,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-31 13:15:48,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-31 13:15:50,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-31 13:15:50,898] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-31 13:15:50,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 13:15:51,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-31 13:15:51,018] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-31 13:15:51,018] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 13:15:51,043] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-31 13:15:51,043] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-31 13:15:51,043] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 13:15:51,070] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-31 13:15:51,071] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-31 13:15:51,071] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 13:15:51,073] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-31 13:15:51,073] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-31 13:15:51,073] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 13:15:51,073] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-31 13:15:51,073] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-31 13:15:51,073] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0331 13:15:51.074399 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0331 13:15:51.075361 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0331 13:15:51.075656 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0331 13:15:51.077202 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 33, Global Step: 340100, Data Step: 680200, Loss: 3.03125, Token per second per gpu: 24444.820554722224
Epoch: 33, Global Step: 340200, Data Step: 680400, Loss: 2.984375, Token per second per gpu: 25033.590628820795
Epoch: 33, Global Step: 340300, Data Step: 680600, Loss: 2.859375, Token per second per gpu: 25008.33877979915
Epoch: 33, Global Step: 340400, Data Step: 680800, Loss: 3.203125, Token per second per gpu: 25018.508610309666
Epoch: 33, Global Step: 340500, Data Step: 681000, Loss: 3.296875, Token per second per gpu: 24991.287193176035
Epoch: 33, Global Step: 340600, Data Step: 681200, Loss: 2.921875, Token per second per gpu: 24999.304414523125
Epoch: 33, Global Step: 340700, Data Step: 681400, Loss: 2.921875, Token per second per gpu: 24961.67024756365
Epoch: 33, Global Step: 340800, Data Step: 681600, Loss: 2.734375, Token per second per gpu: 24965.104094980652
Epoch: 33, Global Step: 340900, Data Step: 681800, Loss: 3.34375, Token per second per gpu: 24958.231826926884
Epoch: 33, Global Step: 341000, Data Step: 682000, Loss: 3.046875, Token per second per gpu: 24961.352508892414
Epoch: 33, Global Step: 341100, Data Step: 682200, Loss: 2.640625, Token per second per gpu: 24953.898081748423
Epoch: 33, Global Step: 341200, Data Step: 682400, Loss: 2.984375, Token per second per gpu: 24914.848837953166
Epoch: 33, Global Step: 341300, Data Step: 682600, Loss: 2.890625, Token per second per gpu: 24939.40048195147
Epoch: 33, Global Step: 341400, Data Step: 682800, Loss: 2.84375, Token per second per gpu: 25061.470438817018
Epoch: 33, Global Step: 341500, Data Step: 683000, Loss: 3.109375, Token per second per gpu: 25064.017667620075
Epoch: 33, Global Step: 341600, Data Step: 683200, Loss: 2.9375, Token per second per gpu: 25234.336274415717
Epoch: 33, Global Step: 341700, Data Step: 683400, Loss: 2.6875, Token per second per gpu: 25464.05836188279
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0331 13:49:24.352038 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 34, Global Step: 341800, Data Step: 683600, Loss: 3.359375, Token per second per gpu: 24816.44398719574
Epoch: 34, Global Step: 341900, Data Step: 683800, Loss: 3.203125, Token per second per gpu: 25339.444807935008
Epoch: 34, Global Step: 342000, Data Step: 684000, Loss: 3.359375, Token per second per gpu: 25284.048523639463
Epoch: 34, Global Step: 342100, Data Step: 684200, Loss: 2.6875, Token per second per gpu: 25290.056026091268
Epoch: 34, Global Step: 342200, Data Step: 684400, Loss: 2.953125, Token per second per gpu: 25404.92419060845
Epoch: 34, Global Step: 342300, Data Step: 684600, Loss: 2.984375, Token per second per gpu: 25323.339179223472
Epoch: 34, Global Step: 342400, Data Step: 684800, Loss: 3.203125, Token per second per gpu: 25311.392772832383
Epoch: 34, Global Step: 342500, Data Step: 685000, Loss: 2.9375, Token per second per gpu: 25315.091006031827
Epoch: 34, Global Step: 342600, Data Step: 685200, Loss: 2.96875, Token per second per gpu: 25322.288946265453
Epoch: 34, Global Step: 342700, Data Step: 685400, Loss: 3.09375, Token per second per gpu: 25317.541689348072
Epoch: 34, Global Step: 342800, Data Step: 685600, Loss: 3.1875, Token per second per gpu: 25322.560308262688
Epoch: 34, Global Step: 342900, Data Step: 685800, Loss: 3.09375, Token per second per gpu: 25325.572381660997
Epoch: 34, Global Step: 343000, Data Step: 686000, Loss: 2.703125, Token per second per gpu: 25329.261694314104
Epoch: 34, Global Step: 343100, Data Step: 686200, Loss: 3.078125, Token per second per gpu: 25323.81591095969
Epoch: 34, Global Step: 343200, Data Step: 686400, Loss: 2.84375, Token per second per gpu: 25296.006365861016
Epoch: 34, Global Step: 343300, Data Step: 686600, Loss: 3.125, Token per second per gpu: 25292.589033527907
Epoch: 34, Global Step: 343400, Data Step: 686800, Loss: 3.3125, Token per second per gpu: 24661.101924531704
Epoch: 34, Global Step: 343500, Data Step: 687000, Loss: 3.03125, Token per second per gpu: 25290.16578717275
Epoch: 34, Global Step: 343600, Data Step: 687200, Loss: 3.171875, Token per second per gpu: 25285.680601942768
Epoch: 34, Global Step: 343700, Data Step: 687400, Loss: 2.671875, Token per second per gpu: 25275.526366599366
Epoch: 34, Global Step: 343800, Data Step: 687600, Loss: 2.875, Token per second per gpu: 25272.128734961007
Epoch: 34, Global Step: 343900, Data Step: 687800, Loss: 2.9375, Token per second per gpu: 25276.197254599192
Epoch: 34, Global Step: 344000, Data Step: 688000, Loss: 3.203125, Token per second per gpu: 25273.668322335678
Epoch: 34, Global Step: 344100, Data Step: 688200, Loss: 3.203125, Token per second per gpu: 25272.58281362596
Epoch: 34, Global Step: 344200, Data Step: 688400, Loss: 3.203125, Token per second per gpu: 25273.806707735148
Epoch: 34, Global Step: 344300, Data Step: 688600, Loss: 2.75, Token per second per gpu: 25271.509981231302
Epoch: 34, Global Step: 344400, Data Step: 688800, Loss: 3.0, Token per second per gpu: 24640.10125952054
Epoch: 34, Global Step: 344500, Data Step: 689000, Loss: 2.953125, Token per second per gpu: 25258.148267622662
Epoch: 34, Global Step: 344600, Data Step: 689200, Loss: 2.765625, Token per second per gpu: 25240.102888456247
Epoch: 34, Global Step: 344700, Data Step: 689400, Loss: 2.984375, Token per second per gpu: 25240.66472688867
Epoch: 34, Global Step: 344800, Data Step: 689600, Loss: 2.765625, Token per second per gpu: 25260.72411989171
Epoch: 34, Global Step: 344900, Data Step: 689800, Loss: 3.296875, Token per second per gpu: 25383.079149356126
Epoch: 34, Global Step: 345000, Data Step: 690000, Loss: 2.921875, Token per second per gpu: 25320.03721641885
Epoch: 34, Global Step: 345100, Data Step: 690200, Loss: 2.859375, Token per second per gpu: 25280.305124413673
Epoch: 34, Global Step: 345200, Data Step: 690400, Loss: 3.265625, Token per second per gpu: 25275.082230306005
Epoch: 34, Global Step: 345300, Data Step: 690600, Loss: 2.984375, Token per second per gpu: 25271.335511162197
Epoch: 34, Global Step: 345400, Data Step: 690800, Loss: 2.828125, Token per second per gpu: 25272.202175315713
Epoch: 34, Global Step: 345500, Data Step: 691000, Loss: 3.0, Token per second per gpu: 25270.20468630718
Epoch: 34, Global Step: 345600, Data Step: 691200, Loss: 3.140625, Token per second per gpu: 25269.69592743291
Epoch: 34, Global Step: 345700, Data Step: 691400, Loss: 2.90625, Token per second per gpu: 25273.60248801004
Epoch: 34, Global Step: 345800, Data Step: 691600, Loss: 3.234375, Token per second per gpu: 25269.177621321327
Epoch: 34, Global Step: 345900, Data Step: 691800, Loss: 2.859375, Token per second per gpu: 25273.532529500128
Epoch: 34, Global Step: 346000, Data Step: 692000, Loss: 2.953125, Token per second per gpu: 25271.457111258995
Epoch: 34, Global Step: 346100, Data Step: 692200, Loss: 2.953125, Token per second per gpu: 25270.979441087427
Epoch: 34, Global Step: 346200, Data Step: 692400, Loss: 3.09375, Token per second per gpu: 25272.249020862128
Epoch: 34, Global Step: 346300, Data Step: 692600, Loss: 3.25, Token per second per gpu: 25277.045422241383
Epoch: 34, Global Step: 346400, Data Step: 692800, Loss: 3.0, Token per second per gpu: 25273.936845330285
Epoch: 34, Global Step: 346500, Data Step: 693000, Loss: 2.703125, Token per second per gpu: 25291.96043391712
Epoch: 34, Global Step: 346600, Data Step: 693200, Loss: 3.109375, Token per second per gpu: 25282.528890766873
Epoch: 34, Global Step: 346700, Data Step: 693400, Loss: 2.953125, Token per second per gpu: 25297.199895514877
Epoch: 34, Global Step: 346800, Data Step: 693600, Loss: 3.109375, Token per second per gpu: 25296.347567169843
Epoch: 34, Global Step: 346900, Data Step: 693800, Loss: 2.796875, Token per second per gpu: 25282.548099361306
Epoch: 34, Global Step: 347000, Data Step: 694000, Loss: 2.859375, Token per second per gpu: 25271.69211997966
Epoch: 34, Global Step: 347100, Data Step: 694200, Loss: 3.125, Token per second per gpu: 25275.167322607118
Epoch: 34, Global Step: 347200, Data Step: 694400, Loss: 2.984375, Token per second per gpu: 25270.497929765424
Epoch: 34, Global Step: 347300, Data Step: 694600, Loss: 3.25, Token per second per gpu: 25272.215076329227
Epoch: 34, Global Step: 347400, Data Step: 694800, Loss: 3.265625, Token per second per gpu: 25270.96574831831
Epoch: 34, Global Step: 347500, Data Step: 695000, Loss: 2.96875, Token per second per gpu: 24631.66077804805
Epoch: 34, Global Step: 347600, Data Step: 695200, Loss: 3.140625, Token per second per gpu: 25271.141852206558
Epoch: 34, Global Step: 347700, Data Step: 695400, Loss: 3.21875, Token per second per gpu: 25270.101759308374
Epoch: 34, Global Step: 347800, Data Step: 695600, Loss: 3.046875, Token per second per gpu: 25273.145834610415
Epoch: 34, Global Step: 347900, Data Step: 695800, Loss: 3.203125, Token per second per gpu: 25261.831326813477
Epoch: 34, Global Step: 348000, Data Step: 696000, Loss: 2.703125, Token per second per gpu: 25265.419811169006
Epoch: 34, Global Step: 348100, Data Step: 696200, Loss: 2.921875, Token per second per gpu: 25266.50586730292
Epoch: 34, Global Step: 348200, Data Step: 696400, Loss: 2.984375, Token per second per gpu: 25258.291658962768
Epoch: 34, Global Step: 348300, Data Step: 696600, Loss: 3.09375, Token per second per gpu: 25253.33152874677
Epoch: 34, Global Step: 348400, Data Step: 696800, Loss: 3.09375, Token per second per gpu: 25245.313761658123
Epoch: 34, Global Step: 348500, Data Step: 697000, Loss: 3.09375, Token per second per gpu: 24607.632813598313
Epoch: 34, Global Step: 348600, Data Step: 697200, Loss: 2.953125, Token per second per gpu: 25229.95881812775
Epoch: 34, Global Step: 348700, Data Step: 697400, Loss: 3.171875, Token per second per gpu: 25209.926718487448
Epoch: 34, Global Step: 348800, Data Step: 697600, Loss: 3.125, Token per second per gpu: 25212.6797167233
Epoch: 34, Global Step: 348900, Data Step: 697800, Loss: 2.828125, Token per second per gpu: 25233.31533825761
Epoch: 34, Global Step: 349000, Data Step: 698000, Loss: 3.046875, Token per second per gpu: 25287.038948960446
Epoch: 34, Global Step: 349100, Data Step: 698200, Loss: 3.03125, Token per second per gpu: 25313.414814327996
Epoch: 34, Global Step: 349200, Data Step: 698400, Loss: 3.140625, Token per second per gpu: 25302.279721669795
Epoch: 34, Global Step: 349300, Data Step: 698600, Loss: 3.078125, Token per second per gpu: 25298.592913001903
Epoch: 34, Global Step: 349400, Data Step: 698800, Loss: 2.828125, Token per second per gpu: 25281.757449057255
Epoch: 34, Global Step: 349500, Data Step: 699000, Loss: 3.09375, Token per second per gpu: 25279.617776792194
Epoch: 34, Global Step: 349600, Data Step: 699200, Loss: 3.03125, Token per second per gpu: 25272.609145190207
Epoch: 34, Global Step: 349700, Data Step: 699400, Loss: 3.046875, Token per second per gpu: 25274.526315089093
Epoch: 34, Global Step: 349800, Data Step: 699600, Loss: 2.921875, Token per second per gpu: 25276.264900702772
Epoch: 34, Global Step: 349900, Data Step: 699800, Loss: 3.0625, Token per second per gpu: 25273.99950856745
Epoch: 34, Global Step: 350000, Data Step: 700000, Loss: 2.8125, Token per second per gpu: 25277.042671792857
I0331 16:26:15.866638 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0331 16:26:15.867002 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-31 16:26:15,867] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-31 16:26:15,871] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-31 16:26:15,871] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-31 16:26:16,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-31 16:26:16,424] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-31 16:26:16,424] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-31 16:26:16,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-31 16:26:16,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-31 16:26:16,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-31 16:26:16,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-31 16:26:18,452] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-31 16:26:18,453] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-31 16:26:18,453] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 16:26:18,642] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-31 16:26:18,642] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-31 16:26:18,643] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-31 16:26:18,643] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-31 16:26:18,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 16:26:18,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 16:26:18,656] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-31 16:26:18,656] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-31 16:26:18,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 16:26:18,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-31 16:26:18,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-31 16:26:18,670] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-31 16:26:18,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 16:26:18,670] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-31 16:26:18,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0331 16:26:18.671221 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0331 16:26:18.671874 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0331 16:26:18.672125 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0331 16:26:18.673401 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 34, Global Step: 350100, Data Step: 700200, Loss: 2.90625, Token per second per gpu: 24674.466039033723
Epoch: 34, Global Step: 350200, Data Step: 700400, Loss: 2.84375, Token per second per gpu: 25294.009243523593
Epoch: 34, Global Step: 350300, Data Step: 700600, Loss: 3.28125, Token per second per gpu: 25289.577864353145
Epoch: 34, Global Step: 350400, Data Step: 700800, Loss: 3.078125, Token per second per gpu: 25291.635184278835
Epoch: 34, Global Step: 350500, Data Step: 701000, Loss: 3.046875, Token per second per gpu: 25289.88331183719
Epoch: 34, Global Step: 350600, Data Step: 701200, Loss: 3.046875, Token per second per gpu: 25294.850873704923
Epoch: 34, Global Step: 350700, Data Step: 701400, Loss: 2.96875, Token per second per gpu: 25283.71871455229
Epoch: 34, Global Step: 350800, Data Step: 701600, Loss: 3.28125, Token per second per gpu: 25273.485784951983
Epoch: 34, Global Step: 350900, Data Step: 701800, Loss: 3.328125, Token per second per gpu: 25267.179183377502
Epoch: 34, Global Step: 351000, Data Step: 702000, Loss: 3.171875, Token per second per gpu: 25270.779337810192
Epoch: 34, Global Step: 351100, Data Step: 702200, Loss: 2.8125, Token per second per gpu: 25258.38102190317
Epoch: 34, Global Step: 351200, Data Step: 702400, Loss: 2.921875, Token per second per gpu: 25262.467780962
Epoch: 34, Global Step: 351300, Data Step: 702600, Loss: 2.96875, Token per second per gpu: 25249.174523441354
Epoch: 34, Global Step: 351400, Data Step: 702800, Loss: 3.4375, Token per second per gpu: 25240.022884022877
Epoch: 34, Global Step: 351500, Data Step: 703000, Loss: 3.0625, Token per second per gpu: 25236.169401604147
Epoch: 34, Global Step: 351600, Data Step: 703200, Loss: 3.03125, Token per second per gpu: 25215.533061359394
Epoch: 34, Global Step: 351700, Data Step: 703400, Loss: 3.109375, Token per second per gpu: 25222.406422714997
Epoch: 34, Global Step: 351800, Data Step: 703600, Loss: 3.171875, Token per second per gpu: 25210.163267506676
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0331 17:00:33.318313 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 35, Global Step: 351900, Data Step: 703800, Loss: 3.375, Token per second per gpu: 24592.06524493372
Epoch: 35, Global Step: 352000, Data Step: 704000, Loss: 2.953125, Token per second per gpu: 25188.647118295798
Epoch: 35, Global Step: 352100, Data Step: 704200, Loss: 2.6875, Token per second per gpu: 25183.767329289756
Epoch: 35, Global Step: 352200, Data Step: 704400, Loss: 2.890625, Token per second per gpu: 25207.694979164986
Epoch: 35, Global Step: 352300, Data Step: 704600, Loss: 2.984375, Token per second per gpu: 25269.535807956545
Epoch: 35, Global Step: 352400, Data Step: 704800, Loss: 2.75, Token per second per gpu: 25287.469477284936
Epoch: 35, Global Step: 352500, Data Step: 705000, Loss: 2.6875, Token per second per gpu: 25285.7780453285
Epoch: 35, Global Step: 352600, Data Step: 705200, Loss: 3.078125, Token per second per gpu: 25278.65670613226
Epoch: 35, Global Step: 352700, Data Step: 705400, Loss: 2.90625, Token per second per gpu: 25282.632289440444
Epoch: 35, Global Step: 352800, Data Step: 705600, Loss: 2.515625, Token per second per gpu: 25273.31430162541
Epoch: 35, Global Step: 352900, Data Step: 705800, Loss: 3.125, Token per second per gpu: 25273.905487388176
Epoch: 35, Global Step: 353000, Data Step: 706000, Loss: 2.921875, Token per second per gpu: 25275.492201714947
Epoch: 35, Global Step: 353100, Data Step: 706200, Loss: 3.109375, Token per second per gpu: 25285.06870005281
Epoch: 35, Global Step: 353200, Data Step: 706400, Loss: 3.390625, Token per second per gpu: 25276.90086582499
Epoch: 35, Global Step: 353300, Data Step: 706600, Loss: 2.78125, Token per second per gpu: 25289.286401584563
Epoch: 35, Global Step: 353400, Data Step: 706800, Loss: 3.03125, Token per second per gpu: 25301.744444033673
Epoch: 35, Global Step: 353500, Data Step: 707000, Loss: 3.234375, Token per second per gpu: 25297.063319926245
Epoch: 35, Global Step: 353600, Data Step: 707200, Loss: 2.921875, Token per second per gpu: 25284.66873832485
Epoch: 35, Global Step: 353700, Data Step: 707400, Loss: 3.140625, Token per second per gpu: 25270.729801504094
Epoch: 35, Global Step: 353800, Data Step: 707600, Loss: 2.96875, Token per second per gpu: 25269.595277647342
Epoch: 35, Global Step: 353900, Data Step: 707800, Loss: 3.15625, Token per second per gpu: 25271.226653633476
Epoch: 35, Global Step: 354000, Data Step: 708000, Loss: 3.3125, Token per second per gpu: 25263.424345169093
Epoch: 35, Global Step: 354100, Data Step: 708200, Loss: 3.15625, Token per second per gpu: 25161.333173803472
Epoch: 35, Global Step: 354200, Data Step: 708400, Loss: 2.90625, Token per second per gpu: 24475.661539511173
Epoch: 35, Global Step: 354300, Data Step: 708600, Loss: 2.984375, Token per second per gpu: 24990.229992766886
Epoch: 35, Global Step: 354400, Data Step: 708800, Loss: 2.75, Token per second per gpu: 24319.27402313664
Epoch: 35, Global Step: 354500, Data Step: 709000, Loss: 2.9375, Token per second per gpu: 24993.114755998482
Epoch: 35, Global Step: 354600, Data Step: 709200, Loss: 2.703125, Token per second per gpu: 24982.11515368336
Epoch: 35, Global Step: 354700, Data Step: 709400, Loss: 3.40625, Token per second per gpu: 24969.355900326245
Epoch: 35, Global Step: 354800, Data Step: 709600, Loss: 3.109375, Token per second per gpu: 24932.81304515909
Epoch: 35, Global Step: 354900, Data Step: 709800, Loss: 2.765625, Token per second per gpu: 24995.74746641873
Epoch: 35, Global Step: 355000, Data Step: 710000, Loss: 3.5625, Token per second per gpu: 25086.994964981586
Epoch: 35, Global Step: 355100, Data Step: 710200, Loss: 3.125, Token per second per gpu: 25077.22824238866
Epoch: 35, Global Step: 355200, Data Step: 710400, Loss: 2.96875, Token per second per gpu: 25064.147317914885
Epoch: 35, Global Step: 355300, Data Step: 710600, Loss: 3.140625, Token per second per gpu: 25041.68630922432
Epoch: 35, Global Step: 355400, Data Step: 710800, Loss: 2.828125, Token per second per gpu: 25013.87476844385
Epoch: 35, Global Step: 355500, Data Step: 711000, Loss: 3.09375, Token per second per gpu: 25042.095804365657
Epoch: 35, Global Step: 355600, Data Step: 711200, Loss: 2.96875, Token per second per gpu: 25009.95383943763
Epoch: 35, Global Step: 355700, Data Step: 711400, Loss: 2.90625, Token per second per gpu: 25006.394067529527
Epoch: 35, Global Step: 355800, Data Step: 711600, Loss: 2.984375, Token per second per gpu: 24987.728700940945
Epoch: 35, Global Step: 355900, Data Step: 711800, Loss: 2.84375, Token per second per gpu: 24990.61552345799
Epoch: 35, Global Step: 356000, Data Step: 712000, Loss: 3.328125, Token per second per gpu: 24980.65944167614
Epoch: 35, Global Step: 356100, Data Step: 712200, Loss: 2.421875, Token per second per gpu: 25105.507808278046
Epoch: 35, Global Step: 356200, Data Step: 712400, Loss: 2.671875, Token per second per gpu: 25247.126954745097
Epoch: 35, Global Step: 356300, Data Step: 712600, Loss: 2.90625, Token per second per gpu: 25130.2441814505
Epoch: 35, Global Step: 356400, Data Step: 712800, Loss: 2.734375, Token per second per gpu: 25035.984671263155
Epoch: 35, Global Step: 356500, Data Step: 713000, Loss: 2.78125, Token per second per gpu: 24981.74739951
Epoch: 35, Global Step: 356600, Data Step: 713200, Loss: 3.21875, Token per second per gpu: 24974.291062920704
Epoch: 35, Global Step: 356700, Data Step: 713400, Loss: 3.28125, Token per second per gpu: 24980.493252177843
Epoch: 35, Global Step: 356800, Data Step: 713600, Loss: 3.125, Token per second per gpu: 24967.326472407472
Epoch: 35, Global Step: 356900, Data Step: 713800, Loss: 3.0, Token per second per gpu: 24982.57571414903
Epoch: 35, Global Step: 357000, Data Step: 714000, Loss: 3.234375, Token per second per gpu: 25013.506544800206
Epoch: 35, Global Step: 357100, Data Step: 714200, Loss: 3.171875, Token per second per gpu: 25004.59349556127
Epoch: 35, Global Step: 357200, Data Step: 714400, Loss: 3.1875, Token per second per gpu: 24991.537443454108
Epoch: 35, Global Step: 357300, Data Step: 714600, Loss: 2.890625, Token per second per gpu: 25008.206133811593
Epoch: 35, Global Step: 357400, Data Step: 714800, Loss: 3.171875, Token per second per gpu: 24398.300582880576
Epoch: 35, Global Step: 357500, Data Step: 715000, Loss: 3.078125, Token per second per gpu: 22223.589591117758
Epoch: 35, Global Step: 357600, Data Step: 715200, Loss: 2.921875, Token per second per gpu: 22051.090472976495
Epoch: 35, Global Step: 357700, Data Step: 715400, Loss: 3.171875, Token per second per gpu: 22417.9011895693
Epoch: 35, Global Step: 357800, Data Step: 715600, Loss: 3.078125, Token per second per gpu: 22068.643492746945
Epoch: 35, Global Step: 357900, Data Step: 715800, Loss: 3.203125, Token per second per gpu: 22402.43259989492
Epoch: 35, Global Step: 358000, Data Step: 716000, Loss: 3.046875, Token per second per gpu: 22231.90088289184
Epoch: 35, Global Step: 358100, Data Step: 716200, Loss: 3.015625, Token per second per gpu: 22246.701346003098
Epoch: 35, Global Step: 358200, Data Step: 716400, Loss: 3.140625, Token per second per gpu: 22167.741802359305
Epoch: 35, Global Step: 358300, Data Step: 716600, Loss: 2.75, Token per second per gpu: 22047.557955899538
Epoch: 35, Global Step: 358400, Data Step: 716800, Loss: 2.765625, Token per second per gpu: 21525.57238805554
Epoch: 35, Global Step: 358500, Data Step: 717000, Loss: 2.859375, Token per second per gpu: 22309.742974733283
Epoch: 35, Global Step: 358600, Data Step: 717200, Loss: 3.015625, Token per second per gpu: 22437.577817513888
Epoch: 35, Global Step: 358700, Data Step: 717400, Loss: 3.359375, Token per second per gpu: 22315.119175529086
Epoch: 35, Global Step: 358800, Data Step: 717600, Loss: 3.0, Token per second per gpu: 22417.827633528577
Epoch: 35, Global Step: 358900, Data Step: 717800, Loss: 3.125, Token per second per gpu: 22661.892601544812
Epoch: 35, Global Step: 359000, Data Step: 718000, Loss: 2.703125, Token per second per gpu: 22452.047748383728
Epoch: 35, Global Step: 359100, Data Step: 718200, Loss: 3.484375, Token per second per gpu: 22296.378843650884
Epoch: 35, Global Step: 359200, Data Step: 718400, Loss: 2.859375, Token per second per gpu: 22682.276137849276
Epoch: 35, Global Step: 359300, Data Step: 718600, Loss: 2.828125, Token per second per gpu: 22417.828340795902
Epoch: 35, Global Step: 359400, Data Step: 718800, Loss: 2.859375, Token per second per gpu: 22600.963286806196
Epoch: 35, Global Step: 359500, Data Step: 719000, Loss: 2.875, Token per second per gpu: 22191.544299808662
Epoch: 35, Global Step: 359600, Data Step: 719200, Loss: 3.140625, Token per second per gpu: 22398.410341974173
Epoch: 35, Global Step: 359700, Data Step: 719400, Loss: 2.859375, Token per second per gpu: 21156.083875997905
Epoch: 35, Global Step: 359800, Data Step: 719600, Loss: 3.3125, Token per second per gpu: 22285.873373593116
Epoch: 35, Global Step: 359900, Data Step: 719800, Loss: 3.109375, Token per second per gpu: 22592.194191647883
Epoch: 35, Global Step: 360000, Data Step: 720000, Loss: 2.953125, Token per second per gpu: 22097.36501867427
I0331 19:43:48.300337 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0331 19:43:48.301881 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-31 19:43:48,311] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-31 19:43:48,326] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-31 19:43:48,332] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-31 19:43:48,879] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-31 19:43:48,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-31 19:43:48,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-31 19:43:48,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-31 19:43:48,882] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-31 19:43:48,882] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-31 19:43:48,882] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-31 19:43:51,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-31 19:43:51,547] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-31 19:43:51,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 19:43:51,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-31 19:43:51,594] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-31 19:43:51,594] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 19:43:51,606] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-31 19:43:51,606] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-31 19:43:51,606] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 19:43:51,624] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-31 19:43:51,624] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-31 19:43:51,624] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-31 19:43:51,624] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 19:43:51,624] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-31 19:43:51,624] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 19:43:51,648] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-31 19:43:51,649] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-31 19:43:51,649] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0331 19:43:51.649802 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0331 19:43:51.650434 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0331 19:43:51.650657 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0331 19:43:51.651925 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 35, Global Step: 360100, Data Step: 720200, Loss: 3.109375, Token per second per gpu: 21837.813394459467
Epoch: 35, Global Step: 360200, Data Step: 720400, Loss: 2.875, Token per second per gpu: 22547.104414815807
Epoch: 35, Global Step: 360300, Data Step: 720600, Loss: 3.125, Token per second per gpu: 22300.796532479253
Epoch: 35, Global Step: 360400, Data Step: 720800, Loss: 2.984375, Token per second per gpu: 22493.971171789148
Epoch: 35, Global Step: 360500, Data Step: 721000, Loss: 3.15625, Token per second per gpu: 22890.637984636352
Epoch: 35, Global Step: 360600, Data Step: 721200, Loss: 2.90625, Token per second per gpu: 22915.727738531594
Epoch: 35, Global Step: 360700, Data Step: 721400, Loss: 3.03125, Token per second per gpu: 22742.489716703905
Epoch: 35, Global Step: 360800, Data Step: 721600, Loss: 2.828125, Token per second per gpu: 22381.51499474329
Epoch: 35, Global Step: 360900, Data Step: 721800, Loss: 2.875, Token per second per gpu: 22760.562787098763
Epoch: 35, Global Step: 361000, Data Step: 722000, Loss: 3.296875, Token per second per gpu: 22341.4526506086
Epoch: 35, Global Step: 361100, Data Step: 722200, Loss: 2.890625, Token per second per gpu: 22281.034416165152
Epoch: 35, Global Step: 361200, Data Step: 722400, Loss: 2.8125, Token per second per gpu: 22344.748359455334
Epoch: 35, Global Step: 361300, Data Step: 722600, Loss: 3.390625, Token per second per gpu: 22256.65537090073
Epoch: 35, Global Step: 361400, Data Step: 722800, Loss: 3.015625, Token per second per gpu: 22838.82982388436
Epoch: 35, Global Step: 361500, Data Step: 723000, Loss: 2.90625, Token per second per gpu: 22365.707790067823
Epoch: 35, Global Step: 361600, Data Step: 723200, Loss: 2.796875, Token per second per gpu: 22676.81358647034
Epoch: 35, Global Step: 361700, Data Step: 723400, Loss: 2.90625, Token per second per gpu: 22533.072065303924
Epoch: 35, Global Step: 361800, Data Step: 723600, Loss: 2.84375, Token per second per gpu: 21944.941723846372
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0331 20:23:24.937899 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 36, Global Step: 361900, Data Step: 723800, Loss: 2.828125, Token per second per gpu: 21580.130405734366
Epoch: 36, Global Step: 362000, Data Step: 724000, Loss: 2.75, Token per second per gpu: 22331.804614044715
Epoch: 36, Global Step: 362100, Data Step: 724200, Loss: 2.921875, Token per second per gpu: 22615.422243154087
Epoch: 36, Global Step: 362200, Data Step: 724400, Loss: 3.21875, Token per second per gpu: 22359.85396941957
Epoch: 36, Global Step: 362300, Data Step: 724600, Loss: 3.015625, Token per second per gpu: 22625.801128619492
Epoch: 36, Global Step: 362400, Data Step: 724800, Loss: 3.25, Token per second per gpu: 22035.75387217644
Epoch: 36, Global Step: 362500, Data Step: 725000, Loss: 3.15625, Token per second per gpu: 22688.952073350083
Epoch: 36, Global Step: 362600, Data Step: 725200, Loss: 2.921875, Token per second per gpu: 22070.173303690248
Epoch: 36, Global Step: 362700, Data Step: 725400, Loss: 3.125, Token per second per gpu: 22413.818269931322
Epoch: 36, Global Step: 362800, Data Step: 725600, Loss: 2.890625, Token per second per gpu: 22188.513264212037
Epoch: 36, Global Step: 362900, Data Step: 725800, Loss: 2.90625, Token per second per gpu: 22068.445250965135
Epoch: 36, Global Step: 363000, Data Step: 726000, Loss: 2.953125, Token per second per gpu: 22619.48322408709
Epoch: 36, Global Step: 363100, Data Step: 726200, Loss: 3.34375, Token per second per gpu: 22201.431510958722
Epoch: 36, Global Step: 363200, Data Step: 726400, Loss: 2.78125, Token per second per gpu: 22321.64921562184
Epoch: 36, Global Step: 363300, Data Step: 726600, Loss: 3.03125, Token per second per gpu: 22523.05665371253
Epoch: 36, Global Step: 363400, Data Step: 726800, Loss: 2.984375, Token per second per gpu: 22667.352219405308
Epoch: 36, Global Step: 363500, Data Step: 727000, Loss: 3.1875, Token per second per gpu: 22588.609874561473
Epoch: 36, Global Step: 363600, Data Step: 727200, Loss: 3.0, Token per second per gpu: 22584.71367121883
Epoch: 36, Global Step: 363700, Data Step: 727400, Loss: 2.953125, Token per second per gpu: 22430.92475993098
Epoch: 36, Global Step: 363800, Data Step: 727600, Loss: 3.140625, Token per second per gpu: 22364.570170912622
Epoch: 36, Global Step: 363900, Data Step: 727800, Loss: 3.140625, Token per second per gpu: 22469.176177978472
Epoch: 36, Global Step: 364000, Data Step: 728000, Loss: 3.140625, Token per second per gpu: 21974.20467610543
Epoch: 36, Global Step: 364100, Data Step: 728200, Loss: 2.953125, Token per second per gpu: 25124.510458884204
Epoch: 36, Global Step: 364200, Data Step: 728400, Loss: 2.421875, Token per second per gpu: 25070.9002441095
Epoch: 36, Global Step: 364300, Data Step: 728600, Loss: 3.078125, Token per second per gpu: 25066.620863491913
Epoch: 36, Global Step: 364400, Data Step: 728800, Loss: 3.0, Token per second per gpu: 25055.89106913508
Epoch: 36, Global Step: 364500, Data Step: 729000, Loss: 2.765625, Token per second per gpu: 25032.285829304274
Epoch: 36, Global Step: 364600, Data Step: 729200, Loss: 2.875, Token per second per gpu: 25012.813480863708
Epoch: 36, Global Step: 364700, Data Step: 729400, Loss: 2.796875, Token per second per gpu: 25021.631168396703
Epoch: 36, Global Step: 364800, Data Step: 729600, Loss: 3.15625, Token per second per gpu: 25025.0856294708
Epoch: 36, Global Step: 364900, Data Step: 729800, Loss: 3.1875, Token per second per gpu: 25036.923589629605
Epoch: 36, Global Step: 365000, Data Step: 730000, Loss: 2.75, Token per second per gpu: 25038.798938805245
Epoch: 36, Global Step: 365100, Data Step: 730200, Loss: 2.71875, Token per second per gpu: 25035.926036606295
Epoch: 36, Global Step: 365200, Data Step: 730400, Loss: 3.078125, Token per second per gpu: 25026.815888058565
Epoch: 36, Global Step: 365300, Data Step: 730600, Loss: 3.25, Token per second per gpu: 25023.126130429868
Epoch: 36, Global Step: 365400, Data Step: 730800, Loss: 3.203125, Token per second per gpu: 25013.527315039348
Epoch: 36, Global Step: 365500, Data Step: 731000, Loss: 3.0, Token per second per gpu: 24381.6088041272
Epoch: 36, Global Step: 365600, Data Step: 731200, Loss: 2.75, Token per second per gpu: 25009.398444986928
Epoch: 36, Global Step: 365700, Data Step: 731400, Loss: 2.9375, Token per second per gpu: 25008.646480417712
Epoch: 36, Global Step: 365800, Data Step: 731600, Loss: 3.09375, Token per second per gpu: 24998.290094883712
Epoch: 36, Global Step: 365900, Data Step: 731800, Loss: 2.875, Token per second per gpu: 24968.506940731168
Epoch: 36, Global Step: 366000, Data Step: 732000, Loss: 3.21875, Token per second per gpu: 24980.43575536239
Epoch: 36, Global Step: 366100, Data Step: 732200, Loss: 2.84375, Token per second per gpu: 24971.276117273803
Epoch: 36, Global Step: 366200, Data Step: 732400, Loss: 2.84375, Token per second per gpu: 24959.296997034653
Epoch: 36, Global Step: 366300, Data Step: 732600, Loss: 3.125, Token per second per gpu: 24953.013781527603
Epoch: 36, Global Step: 366400, Data Step: 732800, Loss: 3.390625, Token per second per gpu: 24954.997526232382
Epoch: 36, Global Step: 366500, Data Step: 733000, Loss: 2.9375, Token per second per gpu: 24966.34018522003
Epoch: 36, Global Step: 366600, Data Step: 733200, Loss: 3.15625, Token per second per gpu: 24924.40131589977
Epoch: 36, Global Step: 366700, Data Step: 733400, Loss: 3.28125, Token per second per gpu: 24957.379292222337
Epoch: 36, Global Step: 366800, Data Step: 733600, Loss: 3.171875, Token per second per gpu: 24955.906921010665
Epoch: 36, Global Step: 366900, Data Step: 733800, Loss: 3.03125, Token per second per gpu: 24970.26510309152
Epoch: 36, Global Step: 367000, Data Step: 734000, Loss: 3.0625, Token per second per gpu: 24980.6905411067
Epoch: 36, Global Step: 367100, Data Step: 734200, Loss: 2.671875, Token per second per gpu: 24977.797695619272
Epoch: 36, Global Step: 367200, Data Step: 734400, Loss: 3.09375, Token per second per gpu: 24942.43492804132
Epoch: 36, Global Step: 367300, Data Step: 734600, Loss: 3.28125, Token per second per gpu: 24446.86730186288
Epoch: 36, Global Step: 367400, Data Step: 734800, Loss: 2.796875, Token per second per gpu: 25303.64276798418
Epoch: 36, Global Step: 367500, Data Step: 735000, Loss: 2.6875, Token per second per gpu: 25333.49372824036
Epoch: 36, Global Step: 367600, Data Step: 735200, Loss: 3.125, Token per second per gpu: 25299.497214089653
Epoch: 36, Global Step: 367700, Data Step: 735400, Loss: 2.796875, Token per second per gpu: 25160.208608358324
Epoch: 36, Global Step: 367800, Data Step: 735600, Loss: 2.90625, Token per second per gpu: 25117.395150410495
Epoch: 36, Global Step: 367900, Data Step: 735800, Loss: 2.734375, Token per second per gpu: 25172.149813690157
Epoch: 36, Global Step: 368000, Data Step: 736000, Loss: 3.125, Token per second per gpu: 25229.636215082224
Epoch: 36, Global Step: 368100, Data Step: 736200, Loss: 3.046875, Token per second per gpu: 25268.98431259386
Epoch: 36, Global Step: 368200, Data Step: 736400, Loss: 3.125, Token per second per gpu: 25271.74493806422
Epoch: 36, Global Step: 368300, Data Step: 736600, Loss: 2.90625, Token per second per gpu: 25273.60851619909
Epoch: 36, Global Step: 368400, Data Step: 736800, Loss: 3.0625, Token per second per gpu: 24612.662561866524
Epoch: 36, Global Step: 368500, Data Step: 737000, Loss: 2.90625, Token per second per gpu: 25271.00190994921
Epoch: 36, Global Step: 368600, Data Step: 737200, Loss: 3.265625, Token per second per gpu: 25272.465115377006
Epoch: 36, Global Step: 368700, Data Step: 737400, Loss: 3.09375, Token per second per gpu: 25275.302657036944
Epoch: 36, Global Step: 368800, Data Step: 737600, Loss: 2.796875, Token per second per gpu: 25282.178749134586
Epoch: 36, Global Step: 368900, Data Step: 737800, Loss: 3.125, Token per second per gpu: 25298.199093051393
Epoch: 36, Global Step: 369000, Data Step: 738000, Loss: 2.828125, Token per second per gpu: 25298.42718181507
Epoch: 36, Global Step: 369100, Data Step: 738200, Loss: 3.046875, Token per second per gpu: 25301.151849210004
Epoch: 36, Global Step: 369200, Data Step: 738400, Loss: 3.28125, Token per second per gpu: 25271.99914470666
Epoch: 36, Global Step: 369300, Data Step: 738600, Loss: 3.25, Token per second per gpu: 25195.904760796297
Epoch: 36, Global Step: 369400, Data Step: 738800, Loss: 2.90625, Token per second per gpu: 25109.966252322552
Epoch: 36, Global Step: 369500, Data Step: 739000, Loss: 2.9375, Token per second per gpu: 25071.478615698456
Epoch: 36, Global Step: 369600, Data Step: 739200, Loss: 2.890625, Token per second per gpu: 25077.513952574576
Epoch: 36, Global Step: 369700, Data Step: 739400, Loss: 2.859375, Token per second per gpu: 25060.831177889646
Epoch: 36, Global Step: 369800, Data Step: 739600, Loss: 3.09375, Token per second per gpu: 25086.442603824762
Epoch: 36, Global Step: 369900, Data Step: 739800, Loss: 3.125, Token per second per gpu: 25065.538399678906
Epoch: 36, Global Step: 370000, Data Step: 740000, Loss: 3.0, Token per second per gpu: 25042.213235590356
I0331 23:04:25.356380 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0331 23:04:25.356809 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-03-31 23:04:25,357] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-03-31 23:04:25,361] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-03-31 23:04:25,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-03-31 23:04:25,901] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-03-31 23:04:25,903] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-03-31 23:04:25,903] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-31 23:04:25,903] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-03-31 23:04:25,903] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-03-31 23:04:25,903] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-03-31 23:04:25,904] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-03-31 23:04:27,991] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-03-31 23:04:27,992] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-03-31 23:04:27,992] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 23:04:28,131] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-03-31 23:04:28,131] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-03-31 23:04:28,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 23:04:28,151] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-03-31 23:04:28,151] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-03-31 23:04:28,151] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 23:04:28,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-03-31 23:04:28,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-31 23:04:28,164] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-03-31 23:04:28,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 23:04:28,164] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-31 23:04:28,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-03-31 23:04:28,164] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-03-31 23:04:28,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-03-31 23:04:28,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0331 23:04:28.165353 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0331 23:04:28.166014 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0331 23:04:28.166258 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0331 23:04:28.167659 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 36, Global Step: 370100, Data Step: 740200, Loss: 3.109375, Token per second per gpu: 24454.89488469614
Epoch: 36, Global Step: 370200, Data Step: 740400, Loss: 2.734375, Token per second per gpu: 25008.434356195572
Epoch: 36, Global Step: 370300, Data Step: 740600, Loss: 2.9375, Token per second per gpu: 24987.54820299267
Epoch: 36, Global Step: 370400, Data Step: 740800, Loss: 2.921875, Token per second per gpu: 25004.649964930424
Epoch: 36, Global Step: 370500, Data Step: 741000, Loss: 3.203125, Token per second per gpu: 25037.29660272097
Epoch: 36, Global Step: 370600, Data Step: 741200, Loss: 3.125, Token per second per gpu: 25025.29767334959
Epoch: 36, Global Step: 370700, Data Step: 741400, Loss: 2.8125, Token per second per gpu: 25029.955042250425
Epoch: 36, Global Step: 370800, Data Step: 741600, Loss: 3.34375, Token per second per gpu: 25061.205215664544
Epoch: 36, Global Step: 370900, Data Step: 741800, Loss: 3.078125, Token per second per gpu: 25042.216454322013
Epoch: 36, Global Step: 371000, Data Step: 742000, Loss: 2.875, Token per second per gpu: 25051.740253764874
Epoch: 36, Global Step: 371100, Data Step: 742200, Loss: 2.796875, Token per second per gpu: 25055.628042886758
Epoch: 36, Global Step: 371200, Data Step: 742400, Loss: 3.1875, Token per second per gpu: 25034.87652270435
Epoch: 36, Global Step: 371300, Data Step: 742600, Loss: 3.0625, Token per second per gpu: 25003.371366293915
Epoch: 36, Global Step: 371400, Data Step: 742800, Loss: 3.0625, Token per second per gpu: 24986.15532414779
Epoch: 36, Global Step: 371500, Data Step: 743000, Loss: 2.5, Token per second per gpu: 25008.842609212134
Epoch: 36, Global Step: 371600, Data Step: 743200, Loss: 2.84375, Token per second per gpu: 25011.15014945884
Epoch: 36, Global Step: 371700, Data Step: 743400, Loss: 3.171875, Token per second per gpu: 24987.808406191387
Epoch: 36, Global Step: 371800, Data Step: 743600, Loss: 2.953125, Token per second per gpu: 24990.6899218588
Epoch: 36, Global Step: 371900, Data Step: 743800, Loss: 2.984375, Token per second per gpu: 24989.698892925062
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0331 23:41:01.394042 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 37, Global Step: 372000, Data Step: 744000, Loss: 3.3125, Token per second per gpu: 24378.329501616437
Epoch: 37, Global Step: 372100, Data Step: 744200, Loss: 2.984375, Token per second per gpu: 24951.613411196846
Epoch: 37, Global Step: 372200, Data Step: 744400, Loss: 2.671875, Token per second per gpu: 24967.95483056439
Epoch: 37, Global Step: 372300, Data Step: 744600, Loss: 3.046875, Token per second per gpu: 24953.147079781666
Epoch: 37, Global Step: 372400, Data Step: 744800, Loss: 3.15625, Token per second per gpu: 24943.434985254138
Epoch: 37, Global Step: 372500, Data Step: 745000, Loss: 3.0625, Token per second per gpu: 25003.79994933208
Epoch: 37, Global Step: 372600, Data Step: 745200, Loss: 2.953125, Token per second per gpu: 25009.53850810834
Epoch: 37, Global Step: 372700, Data Step: 745400, Loss: 2.84375, Token per second per gpu: 25011.500592302727
Epoch: 37, Global Step: 372800, Data Step: 745600, Loss: 2.9375, Token per second per gpu: 24992.449399943678
Epoch: 37, Global Step: 372900, Data Step: 745800, Loss: 2.984375, Token per second per gpu: 24987.50731736491
Epoch: 37, Global Step: 373000, Data Step: 746000, Loss: 3.234375, Token per second per gpu: 24988.416395138676
Epoch: 37, Global Step: 373100, Data Step: 746200, Loss: 3.3125, Token per second per gpu: 24971.614241479205
Epoch: 37, Global Step: 373200, Data Step: 746400, Loss: 2.828125, Token per second per gpu: 24957.30663893073
Epoch: 37, Global Step: 373300, Data Step: 746600, Loss: 3.296875, Token per second per gpu: 24965.43823386857
Epoch: 37, Global Step: 373400, Data Step: 746800, Loss: 3.109375, Token per second per gpu: 24974.951579741944
Epoch: 37, Global Step: 373500, Data Step: 747000, Loss: 2.6875, Token per second per gpu: 25010.962321936975
Epoch: 37, Global Step: 373600, Data Step: 747200, Loss: 3.234375, Token per second per gpu: 25084.286221164864
Epoch: 37, Global Step: 373700, Data Step: 747400, Loss: 2.421875, Token per second per gpu: 25148.022343683464
Epoch: 37, Global Step: 373800, Data Step: 747600, Loss: 3.125, Token per second per gpu: 25198.116015626925
Epoch: 37, Global Step: 373900, Data Step: 747800, Loss: 3.28125, Token per second per gpu: 24581.226911441805
Epoch: 37, Global Step: 374000, Data Step: 748000, Loss: 2.90625, Token per second per gpu: 25224.86347613929
Epoch: 37, Global Step: 374100, Data Step: 748200, Loss: 2.859375, Token per second per gpu: 25265.2642904576
Epoch: 37, Global Step: 374200, Data Step: 748400, Loss: 2.890625, Token per second per gpu: 25262.805647755395
Epoch: 37, Global Step: 374300, Data Step: 748600, Loss: 3.1875, Token per second per gpu: 25269.106682965146
Epoch: 37, Global Step: 374400, Data Step: 748800, Loss: 2.953125, Token per second per gpu: 25271.133498979583
Epoch: 37, Global Step: 374500, Data Step: 749000, Loss: 3.5, Token per second per gpu: 25278.52937649717
Epoch: 37, Global Step: 374600, Data Step: 749200, Loss: 3.0, Token per second per gpu: 25277.433188581075
Epoch: 37, Global Step: 374700, Data Step: 749400, Loss: 2.859375, Token per second per gpu: 25295.809732988782
Epoch: 37, Global Step: 374800, Data Step: 749600, Loss: 2.9375, Token per second per gpu: 25277.636782573576
Epoch: 37, Global Step: 374900, Data Step: 749800, Loss: 2.953125, Token per second per gpu: 25185.266973634338
Epoch: 37, Global Step: 375000, Data Step: 750000, Loss: 2.734375, Token per second per gpu: 25095.911965296527
Epoch: 37, Global Step: 375100, Data Step: 750200, Loss: 2.78125, Token per second per gpu: 25090.224956810445
Epoch: 37, Global Step: 375200, Data Step: 750400, Loss: 3.125, Token per second per gpu: 25077.76010170153
Epoch: 37, Global Step: 375300, Data Step: 750600, Loss: 3.34375, Token per second per gpu: 25050.863759581796
Epoch: 37, Global Step: 375400, Data Step: 750800, Loss: 3.15625, Token per second per gpu: 25078.069929520978
Epoch: 37, Global Step: 375500, Data Step: 751000, Loss: 3.0, Token per second per gpu: 25066.624140517244
Epoch: 37, Global Step: 375600, Data Step: 751200, Loss: 2.8125, Token per second per gpu: 25053.677216384356
Epoch: 37, Global Step: 375700, Data Step: 751400, Loss: 3.328125, Token per second per gpu: 25030.62903577626
Epoch: 37, Global Step: 375800, Data Step: 751600, Loss: 3.0, Token per second per gpu: 25007.936445352138
Epoch: 37, Global Step: 375900, Data Step: 751800, Loss: 3.0, Token per second per gpu: 25004.31446505303
Epoch: 37, Global Step: 376000, Data Step: 752000, Loss: 3.0, Token per second per gpu: 25029.19473606309
Epoch: 37, Global Step: 376100, Data Step: 752200, Loss: 2.953125, Token per second per gpu: 25026.911605738358
Epoch: 37, Global Step: 376200, Data Step: 752400, Loss: 3.09375, Token per second per gpu: 25041.493817910534
Epoch: 37, Global Step: 376300, Data Step: 752600, Loss: 3.15625, Token per second per gpu: 25026.875309673927
Epoch: 37, Global Step: 376400, Data Step: 752800, Loss: 2.828125, Token per second per gpu: 25017.88822335764
Epoch: 37, Global Step: 376500, Data Step: 753000, Loss: 3.15625, Token per second per gpu: 25019.014922337217
Epoch: 37, Global Step: 376600, Data Step: 753200, Loss: 2.546875, Token per second per gpu: 24392.778088036615
Epoch: 37, Global Step: 376700, Data Step: 753400, Loss: 3.296875, Token per second per gpu: 24997.32443105209
Epoch: 37, Global Step: 376800, Data Step: 753600, Loss: 3.125, Token per second per gpu: 24983.24013121422
Epoch: 37, Global Step: 376900, Data Step: 753800, Loss: 3.203125, Token per second per gpu: 24971.42473593028
Epoch: 37, Global Step: 377000, Data Step: 754000, Loss: 3.09375, Token per second per gpu: 24970.752223517877
Epoch: 37, Global Step: 377100, Data Step: 754200, Loss: 3.15625, Token per second per gpu: 24954.793477026607
Epoch: 37, Global Step: 377200, Data Step: 754400, Loss: 3.28125, Token per second per gpu: 24955.249112464116
Epoch: 37, Global Step: 377300, Data Step: 754600, Loss: 2.9375, Token per second per gpu: 24364.568171613435
Epoch: 37, Global Step: 377400, Data Step: 754800, Loss: 2.75, Token per second per gpu: 24980.67101354823
Epoch: 37, Global Step: 377500, Data Step: 755000, Loss: 2.9375, Token per second per gpu: 25007.377827172775
Epoch: 37, Global Step: 377600, Data Step: 755200, Loss: 2.78125, Token per second per gpu: 25009.108848099742
Epoch: 37, Global Step: 377700, Data Step: 755400, Loss: 2.953125, Token per second per gpu: 25016.570242852802
Epoch: 37, Global Step: 377800, Data Step: 755600, Loss: 2.9375, Token per second per gpu: 25017.214967596756
Epoch: 37, Global Step: 377900, Data Step: 755800, Loss: 3.046875, Token per second per gpu: 25062.65249451528
Epoch: 37, Global Step: 378000, Data Step: 756000, Loss: 3.140625, Token per second per gpu: 25133.185783137997
Epoch: 37, Global Step: 378100, Data Step: 756200, Loss: 2.875, Token per second per gpu: 25193.780593557898
Epoch: 37, Global Step: 378200, Data Step: 756400, Loss: 2.90625, Token per second per gpu: 25214.350067302443
Epoch: 37, Global Step: 378300, Data Step: 756600, Loss: 2.875, Token per second per gpu: 24617.783368239685
Epoch: 37, Global Step: 378400, Data Step: 756800, Loss: 3.234375, Token per second per gpu: 25270.258978390095
Epoch: 37, Global Step: 378500, Data Step: 757000, Loss: 2.625, Token per second per gpu: 25271.129851053083
Epoch: 37, Global Step: 378600, Data Step: 757200, Loss: 3.40625, Token per second per gpu: 25271.529860398114
Epoch: 37, Global Step: 378700, Data Step: 757400, Loss: 3.046875, Token per second per gpu: 25284.26540214761
Epoch: 37, Global Step: 378800, Data Step: 757600, Loss: 3.0625, Token per second per gpu: 25290.380970369348
Epoch: 37, Global Step: 378900, Data Step: 757800, Loss: 3.1875, Token per second per gpu: 25277.76944573952
Epoch: 37, Global Step: 379000, Data Step: 758000, Loss: 3.359375, Token per second per gpu: 25213.303697469866
Epoch: 37, Global Step: 379100, Data Step: 758200, Loss: 3.171875, Token per second per gpu: 25119.24266707116
Epoch: 37, Global Step: 379200, Data Step: 758400, Loss: 2.8125, Token per second per gpu: 25095.152495426275
Epoch: 37, Global Step: 379300, Data Step: 758600, Loss: 3.0, Token per second per gpu: 25071.630771146392
Epoch: 37, Global Step: 379400, Data Step: 758800, Loss: 3.328125, Token per second per gpu: 25068.82040833678
Epoch: 37, Global Step: 379500, Data Step: 759000, Loss: 3.28125, Token per second per gpu: 25085.16172179002
Epoch: 37, Global Step: 379600, Data Step: 759200, Loss: 3.15625, Token per second per gpu: 25067.921077557094
Epoch: 37, Global Step: 379700, Data Step: 759400, Loss: 3.03125, Token per second per gpu: 25078.23325471127
Epoch: 37, Global Step: 379800, Data Step: 759600, Loss: 3.25, Token per second per gpu: 25040.7656689375
Epoch: 37, Global Step: 379900, Data Step: 759800, Loss: 3.234375, Token per second per gpu: 25038.038199044808
Epoch: 37, Global Step: 380000, Data Step: 760000, Loss: 2.890625, Token per second per gpu: 25017.06709866679
I0401 02:16:09.471254 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0401 02:16:09.471647 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-01 02:16:09,472] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-01 02:16:09,475] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-01 02:16:09,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-01 02:16:10,047] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-01 02:16:10,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-01 02:16:10,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-01 02:16:10,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-01 02:16:10,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-01 02:16:10,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-01 02:16:10,051] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-01 02:16:12,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-01 02:16:12,169] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-01 02:16:12,169] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 02:16:12,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-01 02:16:12,229] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-01 02:16:12,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 02:16:12,243] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-01 02:16:12,245] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-01 02:16:12,245] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 02:16:12,263] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-01 02:16:12,263] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-01 02:16:12,263] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-01 02:16:12,263] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-01 02:16:12,263] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-01 02:16:12,263] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 02:16:12,263] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-01 02:16:12,263] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 02:16:12,263] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0401 02:16:12.264108 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0401 02:16:12.264739 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0401 02:16:12.264990 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0401 02:16:12.267242 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 37, Global Step: 380100, Data Step: 760200, Loss: 3.046875, Token per second per gpu: 24446.048700401905
Epoch: 37, Global Step: 380200, Data Step: 760400, Loss: 3.0, Token per second per gpu: 25068.21323463066
Epoch: 37, Global Step: 380300, Data Step: 760600, Loss: 2.8125, Token per second per gpu: 25049.90597758715
Epoch: 37, Global Step: 380400, Data Step: 760800, Loss: 3.21875, Token per second per gpu: 25055.238996601067
Epoch: 37, Global Step: 380500, Data Step: 761000, Loss: 2.875, Token per second per gpu: 25035.88328018011
Epoch: 37, Global Step: 380600, Data Step: 761200, Loss: 3.28125, Token per second per gpu: 25029.377547623473
Epoch: 37, Global Step: 380700, Data Step: 761400, Loss: 3.0625, Token per second per gpu: 25009.578171369714
Epoch: 37, Global Step: 380800, Data Step: 761600, Loss: 3.21875, Token per second per gpu: 25022.905363231584
Epoch: 37, Global Step: 380900, Data Step: 761800, Loss: 2.9375, Token per second per gpu: 25004.658971067878
Epoch: 37, Global Step: 381000, Data Step: 762000, Loss: 2.90625, Token per second per gpu: 24996.92695120719
Epoch: 37, Global Step: 381100, Data Step: 762200, Loss: 2.953125, Token per second per gpu: 24992.453588364813
Epoch: 37, Global Step: 381200, Data Step: 762400, Loss: 3.0, Token per second per gpu: 24966.09967568736
Epoch: 37, Global Step: 381300, Data Step: 762600, Loss: 2.96875, Token per second per gpu: 24946.610854328184
Epoch: 37, Global Step: 381400, Data Step: 762800, Loss: 2.859375, Token per second per gpu: 24951.875495126955
Epoch: 37, Global Step: 381500, Data Step: 763000, Loss: 2.5, Token per second per gpu: 24951.814315947035
Epoch: 37, Global Step: 381600, Data Step: 763200, Loss: 2.8125, Token per second per gpu: 24944.073782415348
Epoch: 37, Global Step: 381700, Data Step: 763400, Loss: 3.03125, Token per second per gpu: 24905.556369915714
Epoch: 37, Global Step: 381800, Data Step: 763600, Loss: 2.90625, Token per second per gpu: 24959.37394240022
Epoch: 37, Global Step: 381900, Data Step: 763800, Loss: 2.875, Token per second per gpu: 25007.217805006563
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0401 02:53:46.655521 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 38, Global Step: 382000, Data Step: 764000, Loss: 2.875, Token per second per gpu: 24466.1589325806
Epoch: 38, Global Step: 382100, Data Step: 764200, Loss: 3.015625, Token per second per gpu: 25180.94683730559
Epoch: 38, Global Step: 382200, Data Step: 764400, Loss: 3.1875, Token per second per gpu: 25203.732145904025
Epoch: 38, Global Step: 382300, Data Step: 764600, Loss: 3.015625, Token per second per gpu: 25222.760019308
Epoch: 38, Global Step: 382400, Data Step: 764800, Loss: 3.171875, Token per second per gpu: 25258.060015419393
Epoch: 38, Global Step: 382500, Data Step: 765000, Loss: 3.171875, Token per second per gpu: 25269.883432086484
Epoch: 38, Global Step: 382600, Data Step: 765200, Loss: 3.296875, Token per second per gpu: 25271.218987645672
Epoch: 38, Global Step: 382700, Data Step: 765400, Loss: 2.765625, Token per second per gpu: 25273.83008058117
Epoch: 38, Global Step: 382800, Data Step: 765600, Loss: 3.40625, Token per second per gpu: 25275.95576254969
Epoch: 38, Global Step: 382900, Data Step: 765800, Loss: 2.75, Token per second per gpu: 25288.7002667371
Epoch: 38, Global Step: 383000, Data Step: 766000, Loss: 3.3125, Token per second per gpu: 25294.426026195826
Epoch: 38, Global Step: 383100, Data Step: 766200, Loss: 2.890625, Token per second per gpu: 25310.379857972293
Epoch: 38, Global Step: 383200, Data Step: 766400, Loss: 2.5625, Token per second per gpu: 25242.567400005773
Epoch: 38, Global Step: 383300, Data Step: 766600, Loss: 2.796875, Token per second per gpu: 25148.274905064372
Epoch: 38, Global Step: 383400, Data Step: 766800, Loss: 2.59375, Token per second per gpu: 25103.440381686487
Epoch: 38, Global Step: 383500, Data Step: 767000, Loss: 2.796875, Token per second per gpu: 25081.941939032837
Epoch: 38, Global Step: 383600, Data Step: 767200, Loss: 3.140625, Token per second per gpu: 25079.64985286213
Epoch: 38, Global Step: 383700, Data Step: 767400, Loss: 2.96875, Token per second per gpu: 24447.487745873357
Epoch: 38, Global Step: 383800, Data Step: 767600, Loss: 2.859375, Token per second per gpu: 25073.474423730615
Epoch: 38, Global Step: 383900, Data Step: 767800, Loss: 2.640625, Token per second per gpu: 25063.248114378188
Epoch: 38, Global Step: 384000, Data Step: 768000, Loss: 3.109375, Token per second per gpu: 25055.061575549666
Epoch: 38, Global Step: 384100, Data Step: 768200, Loss: 3.03125, Token per second per gpu: 25043.019761401647
Epoch: 38, Global Step: 384200, Data Step: 768400, Loss: 3.03125, Token per second per gpu: 25001.885697908103
Epoch: 38, Global Step: 384300, Data Step: 768600, Loss: 3.15625, Token per second per gpu: 25001.66468437325
Epoch: 38, Global Step: 384400, Data Step: 768800, Loss: 3.0625, Token per second per gpu: 24975.609704458326
Epoch: 38, Global Step: 384500, Data Step: 769000, Loss: 3.203125, Token per second per gpu: 25009.875287372935
Epoch: 38, Global Step: 384600, Data Step: 769200, Loss: 2.875, Token per second per gpu: 25019.313661746033
Epoch: 38, Global Step: 384700, Data Step: 769400, Loss: 2.5625, Token per second per gpu: 25040.576461995333
Epoch: 38, Global Step: 384800, Data Step: 769600, Loss: 3.234375, Token per second per gpu: 25040.33602536462
Epoch: 38, Global Step: 384900, Data Step: 769800, Loss: 2.984375, Token per second per gpu: 25026.443239038777
Epoch: 38, Global Step: 385000, Data Step: 770000, Loss: 2.796875, Token per second per gpu: 25033.07438879977
Epoch: 38, Global Step: 385100, Data Step: 770200, Loss: 2.265625, Token per second per gpu: 25009.17921440043
Epoch: 38, Global Step: 385200, Data Step: 770400, Loss: 2.859375, Token per second per gpu: 25008.432854718907
Epoch: 38, Global Step: 385300, Data Step: 770600, Loss: 3.140625, Token per second per gpu: 25002.166019688775
Epoch: 38, Global Step: 385400, Data Step: 770800, Loss: 2.890625, Token per second per gpu: 24979.887921622696
Epoch: 38, Global Step: 385500, Data Step: 771000, Loss: 3.1875, Token per second per gpu: 24992.401827851547
Epoch: 38, Global Step: 385600, Data Step: 771200, Loss: 3.171875, Token per second per gpu: 24983.098502371726
Epoch: 38, Global Step: 385700, Data Step: 771400, Loss: 2.859375, Token per second per gpu: 24972.371776180054
Epoch: 38, Global Step: 385800, Data Step: 771600, Loss: 3.140625, Token per second per gpu: 24979.448070622704
Epoch: 38, Global Step: 385900, Data Step: 771800, Loss: 3.328125, Token per second per gpu: 24956.510007094003
Epoch: 38, Global Step: 386000, Data Step: 772000, Loss: 3.109375, Token per second per gpu: 24966.242505076283
Epoch: 38, Global Step: 386100, Data Step: 772200, Loss: 3.015625, Token per second per gpu: 24938.225183949413
Epoch: 38, Global Step: 386200, Data Step: 772400, Loss: 2.78125, Token per second per gpu: 24927.518789932157
Epoch: 38, Global Step: 386300, Data Step: 772600, Loss: 3.09375, Token per second per gpu: 24896.286583595047
Epoch: 38, Global Step: 386400, Data Step: 772800, Loss: 3.0625, Token per second per gpu: 24929.748783487103
Epoch: 38, Global Step: 386500, Data Step: 773000, Loss: 3.015625, Token per second per gpu: 24940.7427339258
Epoch: 38, Global Step: 386600, Data Step: 773200, Loss: 3.109375, Token per second per gpu: 25027.998514240193
Epoch: 38, Global Step: 386700, Data Step: 773400, Loss: 2.921875, Token per second per gpu: 25128.54360681173
Epoch: 38, Global Step: 386800, Data Step: 773600, Loss: 2.9375, Token per second per gpu: 25168.49495409264
Epoch: 38, Global Step: 386900, Data Step: 773800, Loss: 2.921875, Token per second per gpu: 25199.73780714085
Epoch: 38, Global Step: 387000, Data Step: 774000, Loss: 2.921875, Token per second per gpu: 25245.33080332095
Epoch: 38, Global Step: 387100, Data Step: 774200, Loss: 2.984375, Token per second per gpu: 25261.971484502876
Epoch: 38, Global Step: 387200, Data Step: 774400, Loss: 2.609375, Token per second per gpu: 24596.83184315388
Epoch: 38, Global Step: 387300, Data Step: 774600, Loss: 3.03125, Token per second per gpu: 25270.955333364862
Epoch: 38, Global Step: 387400, Data Step: 774800, Loss: 3.03125, Token per second per gpu: 25278.564343086815
Epoch: 38, Global Step: 387500, Data Step: 775000, Loss: 2.984375, Token per second per gpu: 25271.649506150432
Epoch: 38, Global Step: 387600, Data Step: 775200, Loss: 3.109375, Token per second per gpu: 25216.917783319124
Epoch: 38, Global Step: 387700, Data Step: 775400, Loss: 2.9375, Token per second per gpu: 24495.059407182154
Epoch: 38, Global Step: 387800, Data Step: 775600, Loss: 2.625, Token per second per gpu: 25102.21128357201
Epoch: 38, Global Step: 387900, Data Step: 775800, Loss: 3.140625, Token per second per gpu: 25073.331197490006
Epoch: 38, Global Step: 388000, Data Step: 776000, Loss: 2.921875, Token per second per gpu: 25052.449920772604
Epoch: 38, Global Step: 388100, Data Step: 776200, Loss: 3.296875, Token per second per gpu: 25074.726109590512
Epoch: 38, Global Step: 388200, Data Step: 776400, Loss: 2.96875, Token per second per gpu: 25062.643498553127
Epoch: 38, Global Step: 388300, Data Step: 776600, Loss: 2.90625, Token per second per gpu: 24427.454221650518
Epoch: 38, Global Step: 388400, Data Step: 776800, Loss: 3.015625, Token per second per gpu: 25034.84414675268
Epoch: 38, Global Step: 388500, Data Step: 777000, Loss: 3.0625, Token per second per gpu: 25028.97220224701
Epoch: 38, Global Step: 388600, Data Step: 777200, Loss: 2.484375, Token per second per gpu: 25008.064118219165
Epoch: 38, Global Step: 388700, Data Step: 777400, Loss: 3.203125, Token per second per gpu: 24987.400839619673
Epoch: 38, Global Step: 388800, Data Step: 777600, Loss: 2.75, Token per second per gpu: 25019.537578476626
Epoch: 38, Global Step: 388900, Data Step: 777800, Loss: 3.109375, Token per second per gpu: 25043.798509160755
Epoch: 38, Global Step: 389000, Data Step: 778000, Loss: 2.65625, Token per second per gpu: 25045.09220247876
Epoch: 38, Global Step: 389100, Data Step: 778200, Loss: 2.703125, Token per second per gpu: 25036.472958143117
Epoch: 38, Global Step: 389200, Data Step: 778400, Loss: 3.265625, Token per second per gpu: 25024.510485022285
Epoch: 38, Global Step: 389300, Data Step: 778600, Loss: 2.890625, Token per second per gpu: 25022.61115125013
Epoch: 38, Global Step: 389400, Data Step: 778800, Loss: 3.140625, Token per second per gpu: 25028.163262153394
Epoch: 38, Global Step: 389500, Data Step: 779000, Loss: 3.3125, Token per second per gpu: 25012.217148241718
Epoch: 38, Global Step: 389600, Data Step: 779200, Loss: 2.875, Token per second per gpu: 24999.620171032846
Epoch: 38, Global Step: 389700, Data Step: 779400, Loss: 3.046875, Token per second per gpu: 25007.19885719237
Epoch: 38, Global Step: 389800, Data Step: 779600, Loss: 2.9375, Token per second per gpu: 24993.36292265885
Epoch: 38, Global Step: 389900, Data Step: 779800, Loss: 2.828125, Token per second per gpu: 24986.473642893827
Epoch: 38, Global Step: 390000, Data Step: 780000, Loss: 3.015625, Token per second per gpu: 24970.404057198113
I0401 05:27:56.996766 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0401 05:27:56.997163 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-01 05:27:56,997] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-01 05:27:57,001] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-01 05:27:57,001] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-01 05:27:57,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-01 05:27:57,566] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-01 05:27:57,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-01 05:27:57,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-01 05:27:57,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-01 05:27:57,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-01 05:27:57,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-01 05:27:59,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-01 05:27:59,691] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-01 05:27:59,691] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 05:27:59,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-01 05:27:59,782] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-01 05:27:59,782] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 05:27:59,791] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-01 05:27:59,791] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-01 05:27:59,791] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 05:27:59,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-01 05:27:59,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-01 05:27:59,797] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-01 05:27:59,797] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 05:27:59,810] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-01 05:27:59,810] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-01 05:27:59,811] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-01 05:27:59,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 05:27:59,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0401 05:27:59.811577 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0401 05:27:59.812180 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0401 05:27:59.812397 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0401 05:27:59.813713 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 38, Global Step: 390100, Data Step: 780200, Loss: 3.390625, Token per second per gpu: 24396.68427133532
Epoch: 38, Global Step: 390200, Data Step: 780400, Loss: 2.96875, Token per second per gpu: 24964.945748467708
Epoch: 38, Global Step: 390300, Data Step: 780600, Loss: 2.890625, Token per second per gpu: 24947.2732567141
Epoch: 38, Global Step: 390400, Data Step: 780800, Loss: 3.125, Token per second per gpu: 24964.70851623247
Epoch: 38, Global Step: 390500, Data Step: 781000, Loss: 2.96875, Token per second per gpu: 24956.517844259834
Epoch: 38, Global Step: 390600, Data Step: 781200, Loss: 2.875, Token per second per gpu: 24999.81212260454
Epoch: 38, Global Step: 390700, Data Step: 781400, Loss: 3.328125, Token per second per gpu: 25111.2903861355
Epoch: 38, Global Step: 390800, Data Step: 781600, Loss: 3.21875, Token per second per gpu: 25189.20635309488
Epoch: 38, Global Step: 390900, Data Step: 781800, Loss: 2.609375, Token per second per gpu: 25213.754243715553
Epoch: 38, Global Step: 391000, Data Step: 782000, Loss: 3.140625, Token per second per gpu: 25231.973350367378
Epoch: 38, Global Step: 391100, Data Step: 782200, Loss: 2.875, Token per second per gpu: 25253.3296809552
Epoch: 38, Global Step: 391200, Data Step: 782400, Loss: 2.828125, Token per second per gpu: 25262.460225951705
Epoch: 38, Global Step: 391300, Data Step: 782600, Loss: 3.078125, Token per second per gpu: 25272.41287589451
Epoch: 38, Global Step: 391400, Data Step: 782800, Loss: 2.90625, Token per second per gpu: 25277.41795486784
Epoch: 38, Global Step: 391500, Data Step: 783000, Loss: 3.0, Token per second per gpu: 25268.485698621895
Epoch: 38, Global Step: 391600, Data Step: 783200, Loss: 3.390625, Token per second per gpu: 25198.746740087205
Epoch: 38, Global Step: 391700, Data Step: 783400, Loss: 3.21875, Token per second per gpu: 25118.463033290467
Epoch: 38, Global Step: 391800, Data Step: 783600, Loss: 3.34375, Token per second per gpu: 25093.226107938914
Epoch: 38, Global Step: 391900, Data Step: 783800, Loss: 2.953125, Token per second per gpu: 25062.890135854148
Epoch: 38, Global Step: 392000, Data Step: 784000, Loss: 3.0625, Token per second per gpu: 25065.72132642976
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0401 06:06:22.192676 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 39, Global Step: 392100, Data Step: 784200, Loss: 2.90625, Token per second per gpu: 24466.305861234105
Epoch: 39, Global Step: 392200, Data Step: 784400, Loss: 2.921875, Token per second per gpu: 25077.869589636186
Epoch: 39, Global Step: 392300, Data Step: 784600, Loss: 2.796875, Token per second per gpu: 25065.049966609247
Epoch: 39, Global Step: 392400, Data Step: 784800, Loss: 2.65625, Token per second per gpu: 25035.061285146443
Epoch: 39, Global Step: 392500, Data Step: 785000, Loss: 3.0625, Token per second per gpu: 25039.4284602197
Epoch: 39, Global Step: 392600, Data Step: 785200, Loss: 2.796875, Token per second per gpu: 25024.865190597662
Epoch: 39, Global Step: 392700, Data Step: 785400, Loss: 3.046875, Token per second per gpu: 24996.5611402194
Epoch: 39, Global Step: 392800, Data Step: 785600, Loss: 2.890625, Token per second per gpu: 25028.702117580528
Epoch: 39, Global Step: 392900, Data Step: 785800, Loss: 2.765625, Token per second per gpu: 25055.799755109107
Epoch: 39, Global Step: 393000, Data Step: 786000, Loss: 3.0, Token per second per gpu: 25051.51669542375
Epoch: 39, Global Step: 393100, Data Step: 786200, Loss: 3.203125, Token per second per gpu: 25055.981812133257
Epoch: 39, Global Step: 393200, Data Step: 786400, Loss: 3.0625, Token per second per gpu: 25030.312132413907
Epoch: 39, Global Step: 393300, Data Step: 786600, Loss: 2.96875, Token per second per gpu: 25042.671497858286
Epoch: 39, Global Step: 393400, Data Step: 786800, Loss: 3.046875, Token per second per gpu: 25031.80284104602
Epoch: 39, Global Step: 393500, Data Step: 787000, Loss: 3.15625, Token per second per gpu: 24400.668498088384
Epoch: 39, Global Step: 393600, Data Step: 787200, Loss: 2.875, Token per second per gpu: 25040.021108117944
Epoch: 39, Global Step: 393700, Data Step: 787400, Loss: 2.96875, Token per second per gpu: 25019.257903424143
Epoch: 39, Global Step: 393800, Data Step: 787600, Loss: 3.125, Token per second per gpu: 25010.81955005031
Epoch: 39, Global Step: 393900, Data Step: 787800, Loss: 3.0625, Token per second per gpu: 25003.240170341935
Epoch: 39, Global Step: 394000, Data Step: 788000, Loss: 3.09375, Token per second per gpu: 25008.58719710828
Epoch: 39, Global Step: 394100, Data Step: 788200, Loss: 3.03125, Token per second per gpu: 24996.369135116907
Epoch: 39, Global Step: 394200, Data Step: 788400, Loss: 2.890625, Token per second per gpu: 25001.486365366796
Epoch: 39, Global Step: 394300, Data Step: 788600, Loss: 3.078125, Token per second per gpu: 24983.76444576073
Epoch: 39, Global Step: 394400, Data Step: 788800, Loss: 2.84375, Token per second per gpu: 24995.92291051396
Epoch: 39, Global Step: 394500, Data Step: 789000, Loss: 3.078125, Token per second per gpu: 24969.139538918942
Epoch: 39, Global Step: 394600, Data Step: 789200, Loss: 3.25, Token per second per gpu: 24960.555412444268
Epoch: 39, Global Step: 394700, Data Step: 789400, Loss: 3.25, Token per second per gpu: 24957.87354177398
Epoch: 39, Global Step: 394800, Data Step: 789600, Loss: 3.09375, Token per second per gpu: 25023.861288920954
Epoch: 39, Global Step: 394900, Data Step: 789800, Loss: 2.859375, Token per second per gpu: 25116.689006193716
Epoch: 39, Global Step: 395000, Data Step: 790000, Loss: 3.109375, Token per second per gpu: 25188.290853453556
Epoch: 39, Global Step: 395100, Data Step: 790200, Loss: 3.015625, Token per second per gpu: 25227.393608942493
Epoch: 39, Global Step: 395200, Data Step: 790400, Loss: 2.65625, Token per second per gpu: 25245.061621601533
Epoch: 39, Global Step: 395300, Data Step: 790600, Loss: 3.078125, Token per second per gpu: 25271.48254168806
Epoch: 39, Global Step: 395400, Data Step: 790800, Loss: 2.78125, Token per second per gpu: 25271.37204393787
Epoch: 39, Global Step: 395500, Data Step: 791000, Loss: 2.65625, Token per second per gpu: 25274.564337810836
Epoch: 39, Global Step: 395600, Data Step: 791200, Loss: 2.609375, Token per second per gpu: 25277.282650679008
Epoch: 39, Global Step: 395700, Data Step: 791400, Loss: 2.859375, Token per second per gpu: 25292.07979659579
Epoch: 39, Global Step: 395800, Data Step: 791600, Loss: 2.828125, Token per second per gpu: 25292.140007921982
Epoch: 39, Global Step: 395900, Data Step: 791800, Loss: 3.0, Token per second per gpu: 25226.958696367794
Epoch: 39, Global Step: 396000, Data Step: 792000, Loss: 3.046875, Token per second per gpu: 25142.731344112857
Epoch: 39, Global Step: 396100, Data Step: 792200, Loss: 3.109375, Token per second per gpu: 25107.05132201116
Epoch: 39, Global Step: 396200, Data Step: 792400, Loss: 3.09375, Token per second per gpu: 25079.441781446585
Epoch: 39, Global Step: 396300, Data Step: 792600, Loss: 2.890625, Token per second per gpu: 25067.174744955264
Epoch: 39, Global Step: 396400, Data Step: 792800, Loss: 2.984375, Token per second per gpu: 25060.68747215549
Epoch: 39, Global Step: 396500, Data Step: 793000, Loss: 2.65625, Token per second per gpu: 25088.29234348902
Epoch: 39, Global Step: 396600, Data Step: 793200, Loss: 3.078125, Token per second per gpu: 25080.24352104127
Epoch: 39, Global Step: 396700, Data Step: 793400, Loss: 2.46875, Token per second per gpu: 25072.491493711586
Epoch: 39, Global Step: 396800, Data Step: 793600, Loss: 3.09375, Token per second per gpu: 25055.154443255848
Epoch: 39, Global Step: 396900, Data Step: 793800, Loss: 3.296875, Token per second per gpu: 25043.319749303082
Epoch: 39, Global Step: 397000, Data Step: 794000, Loss: 2.828125, Token per second per gpu: 25029.556783337404
Epoch: 39, Global Step: 397100, Data Step: 794200, Loss: 2.828125, Token per second per gpu: 24402.777909705786
Epoch: 39, Global Step: 397200, Data Step: 794400, Loss: 3.21875, Token per second per gpu: 25035.58834783978
Epoch: 39, Global Step: 397300, Data Step: 794600, Loss: 2.984375, Token per second per gpu: 25039.615573643387
Epoch: 39, Global Step: 397400, Data Step: 794800, Loss: 3.21875, Token per second per gpu: 25050.768741758788
Epoch: 39, Global Step: 397500, Data Step: 795000, Loss: 3.0625, Token per second per gpu: 25046.840393594997
Epoch: 39, Global Step: 397600, Data Step: 795200, Loss: 2.921875, Token per second per gpu: 25041.56400318161
Epoch: 39, Global Step: 397700, Data Step: 795400, Loss: 3.140625, Token per second per gpu: 25039.752342089676
Epoch: 39, Global Step: 397800, Data Step: 795600, Loss: 3.046875, Token per second per gpu: 25017.010780441495
Epoch: 39, Global Step: 397900, Data Step: 795800, Loss: 3.140625, Token per second per gpu: 25008.08726100738
Epoch: 39, Global Step: 398000, Data Step: 796000, Loss: 3.109375, Token per second per gpu: 25021.720056678412
Epoch: 39, Global Step: 398100, Data Step: 796200, Loss: 2.921875, Token per second per gpu: 24998.367798189796
Epoch: 39, Global Step: 398200, Data Step: 796400, Loss: 2.875, Token per second per gpu: 24344.150445776977
Epoch: 39, Global Step: 398300, Data Step: 796600, Loss: 3.078125, Token per second per gpu: 25000.40900217386
Epoch: 39, Global Step: 398400, Data Step: 796800, Loss: 2.796875, Token per second per gpu: 24973.465724175378
Epoch: 39, Global Step: 398500, Data Step: 797000, Loss: 2.984375, Token per second per gpu: 24956.06845262294
Epoch: 39, Global Step: 398600, Data Step: 797200, Loss: 2.71875, Token per second per gpu: 24947.12111280376
Epoch: 39, Global Step: 398700, Data Step: 797400, Loss: 2.875, Token per second per gpu: 24306.6510820133
Epoch: 39, Global Step: 398800, Data Step: 797600, Loss: 3.265625, Token per second per gpu: 24987.865885260835
Epoch: 39, Global Step: 398900, Data Step: 797800, Loss: 2.796875, Token per second per gpu: 24964.60723729308
Epoch: 39, Global Step: 399000, Data Step: 798000, Loss: 3.078125, Token per second per gpu: 24935.0243239502
Epoch: 39, Global Step: 399100, Data Step: 798200, Loss: 2.546875, Token per second per gpu: 25012.693217644704
Epoch: 39, Global Step: 399200, Data Step: 798400, Loss: 2.9375, Token per second per gpu: 25141.510430934446
Epoch: 39, Global Step: 399300, Data Step: 798600, Loss: 3.296875, Token per second per gpu: 25207.25779982702
Epoch: 39, Global Step: 399400, Data Step: 798800, Loss: 3.140625, Token per second per gpu: 25227.964839735858
Epoch: 39, Global Step: 399500, Data Step: 799000, Loss: 3.1875, Token per second per gpu: 25270.603503305756
Epoch: 39, Global Step: 399600, Data Step: 799200, Loss: 3.15625, Token per second per gpu: 25267.299633576957
Epoch: 39, Global Step: 399700, Data Step: 799400, Loss: 3.0625, Token per second per gpu: 25271.166436102787
Epoch: 39, Global Step: 399800, Data Step: 799600, Loss: 2.734375, Token per second per gpu: 25272.83323007982
Epoch: 39, Global Step: 399900, Data Step: 799800, Loss: 3.21875, Token per second per gpu: 25272.26361386692
Epoch: 39, Global Step: 400000, Data Step: 800000, Loss: 2.796875, Token per second per gpu: 25293.890233379087
I0401 08:39:34.343169 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0401 08:39:34.343603 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-01 08:39:34,344] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-01 08:39:34,348] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-01 08:39:34,348] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-01 08:39:34,795] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-01 08:39:34,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-01 08:39:34,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-01 08:39:34,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-01 08:39:34,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-01 08:39:34,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-01 08:39:34,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-01 08:39:36,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-01 08:39:36,870] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-01 08:39:36,870] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 08:39:36,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-01 08:39:36,932] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-01 08:39:36,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 08:39:36,947] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-01 08:39:36,948] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-01 08:39:36,947] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-01 08:39:36,948] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 08:39:36,948] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-01 08:39:36,948] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 08:39:36,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-01 08:39:36,977] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-01 08:39:36,978] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 08:39:36,989] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-01 08:39:36,989] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-01 08:39:36,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0401 08:39:36.989743 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0401 08:39:36.990534 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0401 08:39:36.990854 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0401 08:39:36.992686 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 39, Global Step: 400100, Data Step: 800200, Loss: 2.84375, Token per second per gpu: 24704.2071047568
Epoch: 39, Global Step: 400200, Data Step: 800400, Loss: 2.859375, Token per second per gpu: 25290.490310675184
Epoch: 39, Global Step: 400300, Data Step: 800600, Loss: 2.859375, Token per second per gpu: 25303.294108416027
Epoch: 39, Global Step: 400400, Data Step: 800800, Loss: 3.09375, Token per second per gpu: 25299.4478830552
Epoch: 39, Global Step: 400500, Data Step: 801000, Loss: 2.71875, Token per second per gpu: 25304.489014999588
Epoch: 39, Global Step: 400600, Data Step: 801200, Loss: 2.84375, Token per second per gpu: 25308.967459973413
Epoch: 39, Global Step: 400700, Data Step: 801400, Loss: 3.125, Token per second per gpu: 25295.65659242485
Epoch: 39, Global Step: 400800, Data Step: 801600, Loss: 2.84375, Token per second per gpu: 25237.920747567565
Epoch: 39, Global Step: 400900, Data Step: 801800, Loss: 3.359375, Token per second per gpu: 25154.789327603336
Epoch: 39, Global Step: 401000, Data Step: 802000, Loss: 3.25, Token per second per gpu: 25107.34773176533
Epoch: 39, Global Step: 401100, Data Step: 802200, Loss: 2.921875, Token per second per gpu: 25108.09510158272
Epoch: 39, Global Step: 401200, Data Step: 802400, Loss: 2.734375, Token per second per gpu: 25099.18613143079
Epoch: 39, Global Step: 401300, Data Step: 802600, Loss: 2.90625, Token per second per gpu: 25061.04465985581
Epoch: 39, Global Step: 401400, Data Step: 802800, Loss: 2.796875, Token per second per gpu: 25047.46372345906
Epoch: 39, Global Step: 401500, Data Step: 803000, Loss: 2.703125, Token per second per gpu: 25070.797581388153
Epoch: 39, Global Step: 401600, Data Step: 803200, Loss: 3.15625, Token per second per gpu: 25077.21840299567
Epoch: 39, Global Step: 401700, Data Step: 803400, Loss: 2.953125, Token per second per gpu: 25050.271221711384
Epoch: 39, Global Step: 401800, Data Step: 803600, Loss: 3.09375, Token per second per gpu: 25035.209574414752
Epoch: 39, Global Step: 401900, Data Step: 803800, Loss: 2.8125, Token per second per gpu: 25030.66886969623
Epoch: 39, Global Step: 402000, Data Step: 804000, Loss: 3.0625, Token per second per gpu: 24993.1605208903
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0401 09:18:55.560925 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 40, Global Step: 402100, Data Step: 804200, Loss: 2.875, Token per second per gpu: 24390.088840339948
Epoch: 40, Global Step: 402200, Data Step: 804400, Loss: 3.234375, Token per second per gpu: 25049.18004040962
Epoch: 40, Global Step: 402300, Data Step: 804600, Loss: 3.296875, Token per second per gpu: 25062.748590513973
Epoch: 40, Global Step: 402400, Data Step: 804800, Loss: 2.796875, Token per second per gpu: 25058.36797557806
Epoch: 40, Global Step: 402500, Data Step: 805000, Loss: 3.03125, Token per second per gpu: 25054.05789990448
Epoch: 40, Global Step: 402600, Data Step: 805200, Loss: 3.03125, Token per second per gpu: 25029.486198596205
Epoch: 40, Global Step: 402700, Data Step: 805400, Loss: 3.125, Token per second per gpu: 25029.165693904994
Epoch: 40, Global Step: 402800, Data Step: 805600, Loss: 3.125, Token per second per gpu: 25019.68915595098
Epoch: 40, Global Step: 402900, Data Step: 805800, Loss: 3.046875, Token per second per gpu: 25022.619703826374
Epoch: 40, Global Step: 403000, Data Step: 806000, Loss: 3.296875, Token per second per gpu: 25017.753766198417
Epoch: 40, Global Step: 403100, Data Step: 806200, Loss: 3.359375, Token per second per gpu: 25012.337821212575
Epoch: 40, Global Step: 403200, Data Step: 806400, Loss: 2.71875, Token per second per gpu: 25097.596393351312
Epoch: 40, Global Step: 403300, Data Step: 806600, Loss: 2.96875, Token per second per gpu: 25152.947367810837
Epoch: 40, Global Step: 403400, Data Step: 806800, Loss: 3.140625, Token per second per gpu: 24583.006196777485
Epoch: 40, Global Step: 403500, Data Step: 807000, Loss: 3.046875, Token per second per gpu: 25223.105198973648
Epoch: 40, Global Step: 403600, Data Step: 807200, Loss: 2.90625, Token per second per gpu: 25249.213314352648
Epoch: 40, Global Step: 403700, Data Step: 807400, Loss: 3.21875, Token per second per gpu: 25271.42882591464
Epoch: 40, Global Step: 403800, Data Step: 807600, Loss: 3.09375, Token per second per gpu: 25273.20859967961
Epoch: 40, Global Step: 403900, Data Step: 807800, Loss: 3.09375, Token per second per gpu: 25276.178214322594
Epoch: 40, Global Step: 404000, Data Step: 808000, Loss: 2.96875, Token per second per gpu: 25307.038487388556
Epoch: 40, Global Step: 404100, Data Step: 808200, Loss: 2.5, Token per second per gpu: 25298.194165743695
Epoch: 40, Global Step: 404200, Data Step: 808400, Loss: 2.828125, Token per second per gpu: 25303.23877324157
Epoch: 40, Global Step: 404300, Data Step: 808600, Loss: 2.984375, Token per second per gpu: 25299.103683898193
Epoch: 40, Global Step: 404400, Data Step: 808800, Loss: 3.234375, Token per second per gpu: 25315.827874907187
Epoch: 40, Global Step: 404500, Data Step: 809000, Loss: 3.078125, Token per second per gpu: 25315.76935462865
Epoch: 40, Global Step: 404600, Data Step: 809200, Loss: 2.78125, Token per second per gpu: 25323.026871270467
Epoch: 40, Global Step: 404700, Data Step: 809400, Loss: 3.015625, Token per second per gpu: 25315.67863016233
Epoch: 40, Global Step: 404800, Data Step: 809600, Loss: 3.078125, Token per second per gpu: 25311.461243901795
Epoch: 40, Global Step: 404900, Data Step: 809800, Loss: 2.71875, Token per second per gpu: 25331.707419450904
Epoch: 40, Global Step: 405000, Data Step: 810000, Loss: 2.890625, Token per second per gpu: 25323.486019179436
Epoch: 40, Global Step: 405100, Data Step: 810200, Loss: 3.1875, Token per second per gpu: 25327.890682203433
Epoch: 40, Global Step: 405200, Data Step: 810400, Loss: 3.171875, Token per second per gpu: 25351.439316349526
Epoch: 40, Global Step: 405300, Data Step: 810600, Loss: 3.078125, Token per second per gpu: 25339.85511582771
Epoch: 40, Global Step: 405400, Data Step: 810800, Loss: 2.765625, Token per second per gpu: 25331.0188674472
Epoch: 40, Global Step: 405500, Data Step: 811000, Loss: 3.21875, Token per second per gpu: 25351.91588293481
Epoch: 40, Global Step: 405600, Data Step: 811200, Loss: 3.125, Token per second per gpu: 25339.636963688685
Epoch: 40, Global Step: 405700, Data Step: 811400, Loss: 2.75, Token per second per gpu: 25329.039050759962
Epoch: 40, Global Step: 405800, Data Step: 811600, Loss: 3.03125, Token per second per gpu: 25331.85483457725
Epoch: 40, Global Step: 405900, Data Step: 811800, Loss: 2.875, Token per second per gpu: 25320.04666347401
Epoch: 40, Global Step: 406000, Data Step: 812000, Loss: 3.046875, Token per second per gpu: 25329.26419057843
Epoch: 40, Global Step: 406100, Data Step: 812200, Loss: 3.109375, Token per second per gpu: 25330.928193005355
Epoch: 40, Global Step: 406200, Data Step: 812400, Loss: 3.078125, Token per second per gpu: 25337.345317485124
Epoch: 40, Global Step: 406300, Data Step: 812600, Loss: 2.921875, Token per second per gpu: 25318.83638327444
Epoch: 40, Global Step: 406400, Data Step: 812800, Loss: 2.625, Token per second per gpu: 25316.15257982664
Epoch: 40, Global Step: 406500, Data Step: 813000, Loss: 2.765625, Token per second per gpu: 25321.964243162092
Epoch: 40, Global Step: 406600, Data Step: 813200, Loss: 3.015625, Token per second per gpu: 25321.45403500054
Epoch: 40, Global Step: 406700, Data Step: 813400, Loss: 2.953125, Token per second per gpu: 25322.28533663802
Epoch: 40, Global Step: 406800, Data Step: 813600, Loss: 3.1875, Token per second per gpu: 25323.978630299163
Epoch: 40, Global Step: 406900, Data Step: 813800, Loss: 3.34375, Token per second per gpu: 25327.084501849804
Epoch: 40, Global Step: 407000, Data Step: 814000, Loss: 2.921875, Token per second per gpu: 25339.96041916685
Epoch: 40, Global Step: 407100, Data Step: 814200, Loss: 2.984375, Token per second per gpu: 24733.690156880402
Epoch: 40, Global Step: 407200, Data Step: 814400, Loss: 3.125, Token per second per gpu: 25357.964040648767
Epoch: 40, Global Step: 407300, Data Step: 814600, Loss: 2.890625, Token per second per gpu: 25364.26550571936
Epoch: 40, Global Step: 407400, Data Step: 814800, Loss: 3.171875, Token per second per gpu: 25346.74180864757
Epoch: 40, Global Step: 407500, Data Step: 815000, Loss: 3.546875, Token per second per gpu: 25362.773705170093
Epoch: 40, Global Step: 407600, Data Step: 815200, Loss: 3.09375, Token per second per gpu: 25351.63356947303
Epoch: 40, Global Step: 407700, Data Step: 815400, Loss: 2.875, Token per second per gpu: 25354.626859151696
Epoch: 40, Global Step: 407800, Data Step: 815600, Loss: 3.03125, Token per second per gpu: 25369.150393924167
Epoch: 40, Global Step: 407900, Data Step: 815800, Loss: 2.984375, Token per second per gpu: 25379.345136385993
Epoch: 40, Global Step: 408000, Data Step: 816000, Loss: 3.25, Token per second per gpu: 25375.89778830953
Epoch: 40, Global Step: 408100, Data Step: 816200, Loss: 2.890625, Token per second per gpu: 25383.896526225628
Epoch: 40, Global Step: 408200, Data Step: 816400, Loss: 3.171875, Token per second per gpu: 24741.814555062123
Epoch: 40, Global Step: 408300, Data Step: 816600, Loss: 3.328125, Token per second per gpu: 25384.217698692355
Epoch: 40, Global Step: 408400, Data Step: 816800, Loss: 2.9375, Token per second per gpu: 25383.370644530303
Epoch: 40, Global Step: 408500, Data Step: 817000, Loss: 3.28125, Token per second per gpu: 25374.784131447395
Epoch: 40, Global Step: 408600, Data Step: 817200, Loss: 2.90625, Token per second per gpu: 25374.925811830242
Epoch: 40, Global Step: 408700, Data Step: 817400, Loss: 3.09375, Token per second per gpu: 25376.579772821504
Epoch: 40, Global Step: 408800, Data Step: 817600, Loss: 2.96875, Token per second per gpu: 25385.48363796366
Epoch: 40, Global Step: 408900, Data Step: 817800, Loss: 2.90625, Token per second per gpu: 25371.517979138982
Epoch: 40, Global Step: 409000, Data Step: 818000, Loss: 3.046875, Token per second per gpu: 25371.94083726482
Epoch: 40, Global Step: 409100, Data Step: 818200, Loss: 2.640625, Token per second per gpu: 25379.773694510757
Epoch: 40, Global Step: 409200, Data Step: 818400, Loss: 2.875, Token per second per gpu: 25365.864918063457
Epoch: 40, Global Step: 409300, Data Step: 818600, Loss: 2.875, Token per second per gpu: 25379.70970581633
Epoch: 40, Global Step: 409400, Data Step: 818800, Loss: 2.96875, Token per second per gpu: 25384.099811823762
Epoch: 40, Global Step: 409500, Data Step: 819000, Loss: 2.875, Token per second per gpu: 25376.937758726202
Epoch: 40, Global Step: 409600, Data Step: 819200, Loss: 3.046875, Token per second per gpu: 25388.802320029095
Epoch: 40, Global Step: 409700, Data Step: 819400, Loss: 2.984375, Token per second per gpu: 25405.203791742584
Epoch: 40, Global Step: 409800, Data Step: 819600, Loss: 2.546875, Token per second per gpu: 24763.120257137372
Epoch: 40, Global Step: 409900, Data Step: 819800, Loss: 3.15625, Token per second per gpu: 25409.21805275339
Epoch: 40, Global Step: 410000, Data Step: 820000, Loss: 2.828125, Token per second per gpu: 25408.290926797454
I0401 11:49:50.394978 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0401 11:49:50.395342 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-01 11:49:50,395] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-01 11:49:50,399] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-01 11:49:50,399] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-01 11:49:50,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-01 11:49:50,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-01 11:49:50,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-01 11:49:50,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-01 11:49:50,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-01 11:49:50,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-01 11:49:50,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-01 11:49:51,239] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-01 11:49:51,240] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-01 11:49:51,240] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 11:49:51,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-01 11:49:51,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-01 11:49:51,241] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-01 11:49:51,241] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 11:49:51,241] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-01 11:49:51,242] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 11:49:51,243] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-01 11:49:51,243] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-01 11:49:51,243] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 11:49:51,244] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-01 11:49:51,244] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-01 11:49:51,244] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-01 11:49:51,244] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-01 11:49:51,244] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 11:49:51,244] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0401 11:49:51.244709 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0401 11:49:51.245292 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0401 11:49:51.245515 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0401 11:49:51.246732 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 40, Global Step: 410100, Data Step: 820200, Loss: 2.953125, Token per second per gpu: 25215.25330345543
Epoch: 40, Global Step: 410200, Data Step: 820400, Loss: 2.734375, Token per second per gpu: 25393.557390420636
Epoch: 40, Global Step: 410300, Data Step: 820600, Loss: 3.125, Token per second per gpu: 25386.731830144417
Epoch: 40, Global Step: 410400, Data Step: 820800, Loss: 3.109375, Token per second per gpu: 25390.18532475414
Epoch: 40, Global Step: 410500, Data Step: 821000, Loss: 2.734375, Token per second per gpu: 25410.754397081233
Epoch: 40, Global Step: 410600, Data Step: 821200, Loss: 2.78125, Token per second per gpu: 25416.922008677393
Epoch: 40, Global Step: 410700, Data Step: 821400, Loss: 2.9375, Token per second per gpu: 25416.944630848044
Epoch: 40, Global Step: 410800, Data Step: 821600, Loss: 3.171875, Token per second per gpu: 25420.434970748538
Epoch: 40, Global Step: 410900, Data Step: 821800, Loss: 2.9375, Token per second per gpu: 25412.46938052541
Epoch: 40, Global Step: 411000, Data Step: 822000, Loss: 3.203125, Token per second per gpu: 25412.869760195437
Epoch: 40, Global Step: 411100, Data Step: 822200, Loss: 2.9375, Token per second per gpu: 25404.426822491536
Epoch: 40, Global Step: 411200, Data Step: 822400, Loss: 2.765625, Token per second per gpu: 25419.566615335872
Epoch: 40, Global Step: 411300, Data Step: 822600, Loss: 2.859375, Token per second per gpu: 25405.652192269295
Epoch: 40, Global Step: 411400, Data Step: 822800, Loss: 3.09375, Token per second per gpu: 25410.78240717035
Epoch: 40, Global Step: 411500, Data Step: 823000, Loss: 3.171875, Token per second per gpu: 25393.939824702917
Epoch: 40, Global Step: 411600, Data Step: 823200, Loss: 3.109375, Token per second per gpu: 25402.67280563839
Epoch: 40, Global Step: 411700, Data Step: 823400, Loss: 3.078125, Token per second per gpu: 25408.503849261662
Epoch: 40, Global Step: 411800, Data Step: 823600, Loss: 3.03125, Token per second per gpu: 25413.14306706317
Epoch: 40, Global Step: 411900, Data Step: 823800, Loss: 2.84375, Token per second per gpu: 25379.619535628804
Epoch: 40, Global Step: 412000, Data Step: 824000, Loss: 3.328125, Token per second per gpu: 25323.320014770616
Epoch: 40, Global Step: 412100, Data Step: 824200, Loss: 2.890625, Token per second per gpu: 25267.576424981835
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0401 12:29:45.669985 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 41, Global Step: 412200, Data Step: 824400, Loss: 2.953125, Token per second per gpu: 24619.27782738767
Epoch: 41, Global Step: 412300, Data Step: 824600, Loss: 2.953125, Token per second per gpu: 25167.616039127053
Epoch: 41, Global Step: 412400, Data Step: 824800, Loss: 2.796875, Token per second per gpu: 25128.54350226457
Epoch: 41, Global Step: 412500, Data Step: 825000, Loss: 2.703125, Token per second per gpu: 25112.808611332373
Epoch: 41, Global Step: 412600, Data Step: 825200, Loss: 3.109375, Token per second per gpu: 25098.696542606725
Epoch: 41, Global Step: 412700, Data Step: 825400, Loss: 3.375, Token per second per gpu: 25119.258912132686
Epoch: 41, Global Step: 412800, Data Step: 825600, Loss: 3.109375, Token per second per gpu: 25185.466302578545
Epoch: 41, Global Step: 412900, Data Step: 825800, Loss: 2.65625, Token per second per gpu: 25245.855940512552
Epoch: 41, Global Step: 413000, Data Step: 826000, Loss: 3.046875, Token per second per gpu: 25271.861254944826
Epoch: 41, Global Step: 413100, Data Step: 826200, Loss: 3.1875, Token per second per gpu: 25273.153025874642
Epoch: 41, Global Step: 413200, Data Step: 826400, Loss: 3.1875, Token per second per gpu: 24657.87340043862
Epoch: 41, Global Step: 413300, Data Step: 826600, Loss: 2.796875, Token per second per gpu: 25314.887232672598
Epoch: 41, Global Step: 413400, Data Step: 826800, Loss: 3.046875, Token per second per gpu: 25310.128060524643
Epoch: 41, Global Step: 413500, Data Step: 827000, Loss: 2.953125, Token per second per gpu: 25289.920374783043
Epoch: 41, Global Step: 413600, Data Step: 827200, Loss: 3.140625, Token per second per gpu: 25280.33840289107
Epoch: 41, Global Step: 413700, Data Step: 827400, Loss: 3.0625, Token per second per gpu: 25303.062221731096
Epoch: 41, Global Step: 413800, Data Step: 827600, Loss: 3.0625, Token per second per gpu: 25297.748331813655
Epoch: 41, Global Step: 413900, Data Step: 827800, Loss: 2.703125, Token per second per gpu: 25313.590290273492
Epoch: 41, Global Step: 414000, Data Step: 828000, Loss: 3.0625, Token per second per gpu: 25308.53779007583
Epoch: 41, Global Step: 414100, Data Step: 828200, Loss: 3.21875, Token per second per gpu: 25302.828060244297
Epoch: 41, Global Step: 414200, Data Step: 828400, Loss: 3.0, Token per second per gpu: 25311.503620837542
Epoch: 41, Global Step: 414300, Data Step: 828600, Loss: 3.0, Token per second per gpu: 25311.17097366237
Epoch: 41, Global Step: 414400, Data Step: 828800, Loss: 2.765625, Token per second per gpu: 25320.227591862764
Epoch: 41, Global Step: 414500, Data Step: 829000, Loss: 3.09375, Token per second per gpu: 25316.389587842692
Epoch: 41, Global Step: 414600, Data Step: 829200, Loss: 3.453125, Token per second per gpu: 25328.765744153043
Epoch: 41, Global Step: 414700, Data Step: 829400, Loss: 3.0, Token per second per gpu: 25330.839856431736
Epoch: 41, Global Step: 414800, Data Step: 829600, Loss: 3.125, Token per second per gpu: 25318.469527502617
Epoch: 41, Global Step: 414900, Data Step: 829800, Loss: 2.75, Token per second per gpu: 25351.855918731653
Epoch: 41, Global Step: 415000, Data Step: 830000, Loss: 2.96875, Token per second per gpu: 25333.211081117563
Epoch: 41, Global Step: 415100, Data Step: 830200, Loss: 3.125, Token per second per gpu: 25326.249115029714
Epoch: 41, Global Step: 415200, Data Step: 830400, Loss: 2.90625, Token per second per gpu: 25338.948937298086
Epoch: 41, Global Step: 415300, Data Step: 830600, Loss: 3.078125, Token per second per gpu: 25355.1576708806
Epoch: 41, Global Step: 415400, Data Step: 830800, Loss: 3.3125, Token per second per gpu: 25361.334101902667
Epoch: 41, Global Step: 415500, Data Step: 831000, Loss: 3.0, Token per second per gpu: 25355.69744628364
Epoch: 41, Global Step: 415600, Data Step: 831200, Loss: 2.8125, Token per second per gpu: 25356.390906724144
Epoch: 41, Global Step: 415700, Data Step: 831400, Loss: 3.046875, Token per second per gpu: 25366.078887850028
Epoch: 41, Global Step: 415800, Data Step: 831600, Loss: 2.640625, Token per second per gpu: 25376.455026569514
Epoch: 41, Global Step: 415900, Data Step: 831800, Loss: 2.90625, Token per second per gpu: 25362.21967685909
Epoch: 41, Global Step: 416000, Data Step: 832000, Loss: 3.234375, Token per second per gpu: 25366.00346268881
Epoch: 41, Global Step: 416100, Data Step: 832200, Loss: 3.0625, Token per second per gpu: 25369.286097321958
Epoch: 41, Global Step: 416200, Data Step: 832400, Loss: 2.984375, Token per second per gpu: 25375.053634691987
Epoch: 41, Global Step: 416300, Data Step: 832600, Loss: 3.109375, Token per second per gpu: 25383.836784029165
Epoch: 41, Global Step: 416400, Data Step: 832800, Loss: 3.109375, Token per second per gpu: 25381.847156143947
Epoch: 41, Global Step: 416500, Data Step: 833000, Loss: 3.203125, Token per second per gpu: 25376.72776398278
Epoch: 41, Global Step: 416600, Data Step: 833200, Loss: 3.0625, Token per second per gpu: 25371.93593448115
Epoch: 41, Global Step: 416700, Data Step: 833400, Loss: 3.203125, Token per second per gpu: 25369.90629151731
Epoch: 41, Global Step: 416800, Data Step: 833600, Loss: 2.96875, Token per second per gpu: 25386.78358294043
Epoch: 41, Global Step: 416900, Data Step: 833800, Loss: 3.046875, Token per second per gpu: 25362.883459325574
Epoch: 41, Global Step: 417000, Data Step: 834000, Loss: 2.859375, Token per second per gpu: 24754.83596175102
Epoch: 41, Global Step: 417100, Data Step: 834200, Loss: 2.5, Token per second per gpu: 25386.316161474424
Epoch: 41, Global Step: 417200, Data Step: 834400, Loss: 2.96875, Token per second per gpu: 25390.022020317985
Epoch: 41, Global Step: 417300, Data Step: 834600, Loss: 3.0, Token per second per gpu: 25379.147099111
Epoch: 41, Global Step: 417400, Data Step: 834800, Loss: 2.921875, Token per second per gpu: 25379.30210540672
Epoch: 41, Global Step: 417500, Data Step: 835000, Loss: 2.78125, Token per second per gpu: 25384.336440147425
Epoch: 41, Global Step: 417600, Data Step: 835200, Loss: 3.0625, Token per second per gpu: 25392.307406068725
Epoch: 41, Global Step: 417700, Data Step: 835400, Loss: 3.03125, Token per second per gpu: 25389.481209448088
Epoch: 41, Global Step: 417800, Data Step: 835600, Loss: 3.25, Token per second per gpu: 25390.976473762115
Epoch: 41, Global Step: 417900, Data Step: 835800, Loss: 3.0625, Token per second per gpu: 25389.974843871965
Epoch: 41, Global Step: 418000, Data Step: 836000, Loss: 3.171875, Token per second per gpu: 25386.247284820518
Epoch: 41, Global Step: 418100, Data Step: 836200, Loss: 3.140625, Token per second per gpu: 25388.002715758153
Epoch: 41, Global Step: 418200, Data Step: 836400, Loss: 2.859375, Token per second per gpu: 24755.60546336143
Epoch: 41, Global Step: 418300, Data Step: 836600, Loss: 2.984375, Token per second per gpu: 25379.99323150905
Epoch: 41, Global Step: 418400, Data Step: 836800, Loss: 3.0625, Token per second per gpu: 25378.897930592095
Epoch: 41, Global Step: 418500, Data Step: 837000, Loss: 3.203125, Token per second per gpu: 25387.20310849642
Epoch: 41, Global Step: 418600, Data Step: 837200, Loss: 2.8125, Token per second per gpu: 25380.38533606365
Epoch: 41, Global Step: 418700, Data Step: 837400, Loss: 2.8125, Token per second per gpu: 25401.66752853331
Epoch: 41, Global Step: 418800, Data Step: 837600, Loss: 3.28125, Token per second per gpu: 25397.246941699566
Epoch: 41, Global Step: 418900, Data Step: 837800, Loss: 3.015625, Token per second per gpu: 25400.623981333585
Epoch: 41, Global Step: 419000, Data Step: 838000, Loss: 3.1875, Token per second per gpu: 25387.154608648554
Epoch: 41, Global Step: 419100, Data Step: 838200, Loss: 3.015625, Token per second per gpu: 25352.783666410804
Epoch: 41, Global Step: 419200, Data Step: 838400, Loss: 3.15625, Token per second per gpu: 25360.33667276538
Epoch: 41, Global Step: 419300, Data Step: 838600, Loss: 3.0625, Token per second per gpu: 25366.607941912924
Epoch: 41, Global Step: 419400, Data Step: 838800, Loss: 2.96875, Token per second per gpu: 25357.043631942623
Epoch: 41, Global Step: 419500, Data Step: 839000, Loss: 3.421875, Token per second per gpu: 25348.499280297805
Epoch: 41, Global Step: 419600, Data Step: 839200, Loss: 3.171875, Token per second per gpu: 25355.56960546502
Epoch: 41, Global Step: 419700, Data Step: 839400, Loss: 3.0, Token per second per gpu: 25385.71357011101
Epoch: 41, Global Step: 419800, Data Step: 839600, Loss: 3.015625, Token per second per gpu: 23307.86268490879
Epoch: 41, Global Step: 419900, Data Step: 839800, Loss: 2.921875, Token per second per gpu: 25370.10413131977
Epoch: 41, Global Step: 420000, Data Step: 840000, Loss: 3.0, Token per second per gpu: 25357.472022755246
I0401 14:59:34.883866 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0401 14:59:34.884313 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-01 14:59:34,884] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-01 14:59:34,888] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-01 14:59:34,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-01 14:59:35,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-01 14:59:35,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-01 14:59:35,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-01 14:59:35,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-01 14:59:35,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-01 14:59:35,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-01 14:59:35,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-01 14:59:37,554] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-01 14:59:37,554] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-01 14:59:37,554] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 14:59:37,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-01 14:59:37,748] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-01 14:59:37,748] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 14:59:37,770] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-01 14:59:37,770] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-01 14:59:37,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 14:59:37,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-01 14:59:37,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-01 14:59:37,779] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-01 14:59:37,779] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-01 14:59:37,779] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 14:59:37,779] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 14:59:37,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-01 14:59:37,787] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-01 14:59:37,787] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0401 14:59:37.787436 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0401 14:59:37.788129 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0401 14:59:37.788383 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0401 14:59:37.789792 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 41, Global Step: 420100, Data Step: 840200, Loss: 2.953125, Token per second per gpu: 24753.63974757483
Epoch: 41, Global Step: 420200, Data Step: 840400, Loss: 2.859375, Token per second per gpu: 23842.924893742213
Epoch: 41, Global Step: 420300, Data Step: 840600, Loss: 3.0625, Token per second per gpu: 24709.448516471537
Epoch: 41, Global Step: 420400, Data Step: 840800, Loss: 3.140625, Token per second per gpu: 25374.3599526123
Epoch: 41, Global Step: 420500, Data Step: 841000, Loss: 2.828125, Token per second per gpu: 25366.45080011184
Epoch: 41, Global Step: 420600, Data Step: 841200, Loss: 3.09375, Token per second per gpu: 23687.69463203316
Epoch: 41, Global Step: 420700, Data Step: 841400, Loss: 3.3125, Token per second per gpu: 25380.516733527376
Epoch: 41, Global Step: 420800, Data Step: 841600, Loss: 3.234375, Token per second per gpu: 25357.082275947305
Epoch: 41, Global Step: 420900, Data Step: 841800, Loss: 3.09375, Token per second per gpu: 24740.72352785431
Epoch: 41, Global Step: 421000, Data Step: 842000, Loss: 3.015625, Token per second per gpu: 25354.48577787999
Epoch: 41, Global Step: 421100, Data Step: 842200, Loss: 3.03125, Token per second per gpu: 25285.093628614337
Epoch: 41, Global Step: 421200, Data Step: 842400, Loss: 3.078125, Token per second per gpu: 25220.06531990154
Epoch: 41, Global Step: 421300, Data Step: 842600, Loss: 3.03125, Token per second per gpu: 25153.874232814465
Epoch: 41, Global Step: 421400, Data Step: 842800, Loss: 2.8125, Token per second per gpu: 25111.554686746127
Epoch: 41, Global Step: 421500, Data Step: 843000, Loss: 2.84375, Token per second per gpu: 25106.703310406607
Epoch: 41, Global Step: 421600, Data Step: 843200, Loss: 2.984375, Token per second per gpu: 25078.35013999949
Epoch: 41, Global Step: 421700, Data Step: 843400, Loss: 3.0, Token per second per gpu: 25128.156579185845
Epoch: 41, Global Step: 421800, Data Step: 843600, Loss: 2.96875, Token per second per gpu: 25213.501365350556
Epoch: 41, Global Step: 421900, Data Step: 843800, Loss: 3.046875, Token per second per gpu: 25262.017182691074
Epoch: 41, Global Step: 422000, Data Step: 844000, Loss: 3.21875, Token per second per gpu: 25271.222582729028
Epoch: 41, Global Step: 422100, Data Step: 844200, Loss: 3.125, Token per second per gpu: 25275.404251361564
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0401 15:41:03.337117 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 42, Global Step: 422200, Data Step: 844400, Loss: 2.96875, Token per second per gpu: 24666.1879074265
Epoch: 42, Global Step: 422300, Data Step: 844600, Loss: 2.8125, Token per second per gpu: 25286.52024876489
Epoch: 42, Global Step: 422400, Data Step: 844800, Loss: 3.09375, Token per second per gpu: 25296.258835933026
Epoch: 42, Global Step: 422500, Data Step: 845000, Loss: 2.78125, Token per second per gpu: 25290.56555218055
Epoch: 42, Global Step: 422600, Data Step: 845200, Loss: 3.234375, Token per second per gpu: 25310.797180317983
Epoch: 42, Global Step: 422700, Data Step: 845400, Loss: 3.109375, Token per second per gpu: 25308.416628240968
Epoch: 42, Global Step: 422800, Data Step: 845600, Loss: 2.84375, Token per second per gpu: 25308.426755947283
Epoch: 42, Global Step: 422900, Data Step: 845800, Loss: 3.265625, Token per second per gpu: 25322.54156965927
Epoch: 42, Global Step: 423000, Data Step: 846000, Loss: 2.8125, Token per second per gpu: 25316.41325174768
Epoch: 42, Global Step: 423100, Data Step: 846200, Loss: 2.765625, Token per second per gpu: 24707.43933469081
Epoch: 42, Global Step: 423200, Data Step: 846400, Loss: 2.671875, Token per second per gpu: 25333.44357385453
Epoch: 42, Global Step: 423300, Data Step: 846600, Loss: 3.1875, Token per second per gpu: 25320.50910013515
Epoch: 42, Global Step: 423400, Data Step: 846800, Loss: 3.0625, Token per second per gpu: 25308.18136019277
Epoch: 42, Global Step: 423500, Data Step: 847000, Loss: 2.96875, Token per second per gpu: 25332.11290814381
Epoch: 42, Global Step: 423600, Data Step: 847200, Loss: 3.015625, Token per second per gpu: 25335.367753184146
Epoch: 42, Global Step: 423700, Data Step: 847400, Loss: 2.515625, Token per second per gpu: 25352.149091578787
Epoch: 42, Global Step: 423800, Data Step: 847600, Loss: 2.65625, Token per second per gpu: 25340.5250641142
Epoch: 42, Global Step: 423900, Data Step: 847800, Loss: 2.84375, Token per second per gpu: 25315.18172628642
Epoch: 42, Global Step: 424000, Data Step: 848000, Loss: 2.9375, Token per second per gpu: 25333.426200506594
Epoch: 42, Global Step: 424100, Data Step: 848200, Loss: 2.875, Token per second per gpu: 25320.398597838495
Epoch: 42, Global Step: 424200, Data Step: 848400, Loss: 2.78125, Token per second per gpu: 25320.105256602823
Epoch: 42, Global Step: 424300, Data Step: 848600, Loss: 3.03125, Token per second per gpu: 25315.596076768554
Epoch: 42, Global Step: 424400, Data Step: 848800, Loss: 3.140625, Token per second per gpu: 25324.46563284292
Epoch: 42, Global Step: 424500, Data Step: 849000, Loss: 2.84375, Token per second per gpu: 25336.0280052186
Epoch: 42, Global Step: 424600, Data Step: 849200, Loss: 2.78125, Token per second per gpu: 25333.580967592236
Epoch: 42, Global Step: 424700, Data Step: 849400, Loss: 2.921875, Token per second per gpu: 25338.258131332932
Epoch: 42, Global Step: 424800, Data Step: 849600, Loss: 2.90625, Token per second per gpu: 25359.565959182535
Epoch: 42, Global Step: 424900, Data Step: 849800, Loss: 3.3125, Token per second per gpu: 25354.19675510029
Epoch: 42, Global Step: 425000, Data Step: 850000, Loss: 3.0625, Token per second per gpu: 25335.53078042663
Epoch: 42, Global Step: 425100, Data Step: 850200, Loss: 2.765625, Token per second per gpu: 25292.95016145399
Epoch: 42, Global Step: 425200, Data Step: 850400, Loss: 3.109375, Token per second per gpu: 25262.138217650798
Epoch: 42, Global Step: 425300, Data Step: 850600, Loss: 3.171875, Token per second per gpu: 25196.770883972786
Epoch: 42, Global Step: 425400, Data Step: 850800, Loss: 3.125, Token per second per gpu: 25150.61526575745
Epoch: 42, Global Step: 425500, Data Step: 851000, Loss: 2.984375, Token per second per gpu: 25113.856890027633
Epoch: 42, Global Step: 425600, Data Step: 851200, Loss: 3.0, Token per second per gpu: 25089.461922770704
Epoch: 42, Global Step: 425700, Data Step: 851400, Loss: 2.859375, Token per second per gpu: 25087.258910715907
Epoch: 42, Global Step: 425800, Data Step: 851600, Loss: 3.0625, Token per second per gpu: 25095.11693957793
Epoch: 42, Global Step: 425900, Data Step: 851800, Loss: 2.890625, Token per second per gpu: 25099.913509617258
Epoch: 42, Global Step: 426000, Data Step: 852000, Loss: 3.046875, Token per second per gpu: 25087.443040381404
Epoch: 42, Global Step: 426100, Data Step: 852200, Loss: 2.921875, Token per second per gpu: 25057.06880426883
Epoch: 42, Global Step: 426200, Data Step: 852400, Loss: 3.1875, Token per second per gpu: 25078.49680784682
Epoch: 42, Global Step: 426300, Data Step: 852600, Loss: 3.203125, Token per second per gpu: 25083.994418566213
Epoch: 42, Global Step: 426400, Data Step: 852800, Loss: 3.0, Token per second per gpu: 25079.596845351953
Epoch: 42, Global Step: 426500, Data Step: 853000, Loss: 3.046875, Token per second per gpu: 25064.126983584214
Epoch: 42, Global Step: 426600, Data Step: 853200, Loss: 3.109375, Token per second per gpu: 25053.63850433481
Epoch: 42, Global Step: 426700, Data Step: 853400, Loss: 3.046875, Token per second per gpu: 25018.131493451594
Epoch: 42, Global Step: 426800, Data Step: 853600, Loss: 3.125, Token per second per gpu: 25011.109600846863
Epoch: 42, Global Step: 426900, Data Step: 853800, Loss: 2.671875, Token per second per gpu: 24415.352918705175
Epoch: 42, Global Step: 427000, Data Step: 854000, Loss: 2.4375, Token per second per gpu: 25071.202877682685
Epoch: 42, Global Step: 427100, Data Step: 854200, Loss: 3.046875, Token per second per gpu: 25076.41930347004
Epoch: 42, Global Step: 427200, Data Step: 854400, Loss: 2.921875, Token per second per gpu: 25055.35062659961
Epoch: 42, Global Step: 427300, Data Step: 854600, Loss: 3.0625, Token per second per gpu: 25046.62227182532
Epoch: 42, Global Step: 427400, Data Step: 854800, Loss: 3.0625, Token per second per gpu: 25043.832154398304
Epoch: 42, Global Step: 427500, Data Step: 855000, Loss: 2.96875, Token per second per gpu: 25030.63458554657
Epoch: 42, Global Step: 427600, Data Step: 855200, Loss: 2.71875, Token per second per gpu: 25007.856559870062
Epoch: 42, Global Step: 427700, Data Step: 855400, Loss: 3.0, Token per second per gpu: 25026.11913104234
Epoch: 42, Global Step: 427800, Data Step: 855600, Loss: 3.015625, Token per second per gpu: 25021.909237922628
Epoch: 42, Global Step: 427900, Data Step: 855800, Loss: 3.078125, Token per second per gpu: 24989.424433841028
Epoch: 42, Global Step: 428000, Data Step: 856000, Loss: 3.15625, Token per second per gpu: 24981.778914967497
Epoch: 42, Global Step: 428100, Data Step: 856200, Loss: 2.703125, Token per second per gpu: 24359.84802890338
Epoch: 42, Global Step: 428200, Data Step: 856400, Loss: 3.21875, Token per second per gpu: 24983.19863963583
Epoch: 42, Global Step: 428300, Data Step: 856600, Loss: 2.796875, Token per second per gpu: 24955.98162847695
Epoch: 42, Global Step: 428400, Data Step: 856800, Loss: 3.265625, Token per second per gpu: 24939.162447743325
Epoch: 42, Global Step: 428500, Data Step: 857000, Loss: 3.109375, Token per second per gpu: 24996.25146103378
Epoch: 42, Global Step: 428600, Data Step: 857200, Loss: 3.09375, Token per second per gpu: 25015.33746250706
Epoch: 42, Global Step: 428700, Data Step: 857400, Loss: 2.90625, Token per second per gpu: 24995.63393595383
Epoch: 42, Global Step: 428800, Data Step: 857600, Loss: 2.953125, Token per second per gpu: 25002.69780484383
Epoch: 42, Global Step: 428900, Data Step: 857800, Loss: 2.96875, Token per second per gpu: 25005.261517103743
Epoch: 42, Global Step: 429000, Data Step: 858000, Loss: 3.078125, Token per second per gpu: 25013.769464387566
Epoch: 42, Global Step: 429100, Data Step: 858200, Loss: 2.921875, Token per second per gpu: 24976.512855709294
Epoch: 42, Global Step: 429200, Data Step: 858400, Loss: 2.90625, Token per second per gpu: 24995.0999686839
Epoch: 42, Global Step: 429300, Data Step: 858600, Loss: 2.984375, Token per second per gpu: 24983.481126120467
Epoch: 42, Global Step: 429400, Data Step: 858800, Loss: 2.84375, Token per second per gpu: 25007.084187290195
Epoch: 42, Global Step: 429500, Data Step: 859000, Loss: 3.125, Token per second per gpu: 25003.554111352325
Epoch: 42, Global Step: 429600, Data Step: 859200, Loss: 3.109375, Token per second per gpu: 25011.819194644424
Epoch: 42, Global Step: 429700, Data Step: 859400, Loss: 2.96875, Token per second per gpu: 25013.575433755825
Epoch: 42, Global Step: 429800, Data Step: 859600, Loss: 3.171875, Token per second per gpu: 25024.457191922822
Epoch: 42, Global Step: 429900, Data Step: 859800, Loss: 3.109375, Token per second per gpu: 25035.70955775645
Epoch: 42, Global Step: 430000, Data Step: 860000, Loss: 2.890625, Token per second per gpu: 25042.333159861
I0401 18:10:51.117881 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0401 18:10:51.118248 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-01 18:10:51,118] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-01 18:10:51,122] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-01 18:10:51,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-01 18:10:51,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-01 18:10:51,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-01 18:10:51,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-01 18:10:51,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-01 18:10:51,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-01 18:10:51,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-01 18:10:51,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-01 18:10:53,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-01 18:10:53,838] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-01 18:10:53,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 18:10:53,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-01 18:10:53,928] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-01 18:10:53,928] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 18:10:53,954] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-01 18:10:53,954] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-01 18:10:53,954] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 18:10:53,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-01 18:10:53,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-01 18:10:53,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-01 18:10:53,966] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-01 18:10:53,966] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-01 18:10:53,966] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-01 18:10:53,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 18:10:53,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 18:10:53,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0401 18:10:53.966837 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0401 18:10:53.967567 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0401 18:10:53.967848 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0401 18:10:53.969507 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 42, Global Step: 430100, Data Step: 860200, Loss: 3.140625, Token per second per gpu: 24465.826181631273
Epoch: 42, Global Step: 430200, Data Step: 860400, Loss: 3.015625, Token per second per gpu: 25060.6245624215
Epoch: 42, Global Step: 430300, Data Step: 860600, Loss: 3.3125, Token per second per gpu: 25067.46490788108
Epoch: 42, Global Step: 430400, Data Step: 860800, Loss: 2.75, Token per second per gpu: 25065.967816257347
Epoch: 42, Global Step: 430500, Data Step: 861000, Loss: 2.828125, Token per second per gpu: 25077.954192237627
Epoch: 42, Global Step: 430600, Data Step: 861200, Loss: 2.6875, Token per second per gpu: 25067.70612588197
Epoch: 42, Global Step: 430700, Data Step: 861400, Loss: 2.859375, Token per second per gpu: 25051.849462659862
Epoch: 42, Global Step: 430800, Data Step: 861600, Loss: 2.90625, Token per second per gpu: 25060.36117683239
Epoch: 42, Global Step: 430900, Data Step: 861800, Loss: 3.203125, Token per second per gpu: 25052.635098571867
Epoch: 42, Global Step: 431000, Data Step: 862000, Loss: 3.03125, Token per second per gpu: 25036.3897767899
Epoch: 42, Global Step: 431100, Data Step: 862200, Loss: 2.6875, Token per second per gpu: 25030.327743995556
Epoch: 42, Global Step: 431200, Data Step: 862400, Loss: 2.890625, Token per second per gpu: 25039.73983303498
Epoch: 42, Global Step: 431300, Data Step: 862600, Loss: 3.140625, Token per second per gpu: 25028.819630661088
Epoch: 42, Global Step: 431400, Data Step: 862800, Loss: 2.96875, Token per second per gpu: 25003.850463166196
Epoch: 42, Global Step: 431500, Data Step: 863000, Loss: 2.6875, Token per second per gpu: 25009.764838930085
Epoch: 42, Global Step: 431600, Data Step: 863200, Loss: 2.96875, Token per second per gpu: 24989.039301762175
Epoch: 42, Global Step: 431700, Data Step: 863400, Loss: 3.0625, Token per second per gpu: 24990.79751338038
Epoch: 42, Global Step: 431800, Data Step: 863600, Loss: 3.03125, Token per second per gpu: 24978.075566477368
Epoch: 42, Global Step: 431900, Data Step: 863800, Loss: 2.828125, Token per second per gpu: 24946.767319697083
Epoch: 42, Global Step: 432000, Data Step: 864000, Loss: 3.078125, Token per second per gpu: 24390.184871172354
Epoch: 42, Global Step: 432100, Data Step: 864200, Loss: 3.125, Token per second per gpu: 25033.549592381412
Epoch: 42, Global Step: 432200, Data Step: 864400, Loss: 3.015625, Token per second per gpu: 25035.11192537085
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0401 18:53:24.449307 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 43, Global Step: 432300, Data Step: 864600, Loss: 2.71875, Token per second per gpu: 24435.66618584984
Epoch: 43, Global Step: 432400, Data Step: 864800, Loss: 3.0625, Token per second per gpu: 25029.804636952264
Epoch: 43, Global Step: 432500, Data Step: 865000, Loss: 2.71875, Token per second per gpu: 25016.981196636323
Epoch: 43, Global Step: 432600, Data Step: 865200, Loss: 3.203125, Token per second per gpu: 25025.1055375941
Epoch: 43, Global Step: 432700, Data Step: 865400, Loss: 2.984375, Token per second per gpu: 25022.246557770235
Epoch: 43, Global Step: 432800, Data Step: 865600, Loss: 3.015625, Token per second per gpu: 25041.276412186013
Epoch: 43, Global Step: 432900, Data Step: 865800, Loss: 2.765625, Token per second per gpu: 24415.75368414175
Epoch: 43, Global Step: 433000, Data Step: 866000, Loss: 2.734375, Token per second per gpu: 25038.22451321357
Epoch: 43, Global Step: 433100, Data Step: 866200, Loss: 3.03125, Token per second per gpu: 25025.564521938708
Epoch: 43, Global Step: 433200, Data Step: 866400, Loss: 2.984375, Token per second per gpu: 25029.747950183042
Epoch: 43, Global Step: 433300, Data Step: 866600, Loss: 2.921875, Token per second per gpu: 25015.30213246936
Epoch: 43, Global Step: 433400, Data Step: 866800, Loss: 3.078125, Token per second per gpu: 25079.610175313777
Epoch: 43, Global Step: 433500, Data Step: 867000, Loss: 2.75, Token per second per gpu: 25209.729106651335
Epoch: 43, Global Step: 433600, Data Step: 867200, Loss: 2.953125, Token per second per gpu: 25165.97850862222
Epoch: 43, Global Step: 433700, Data Step: 867400, Loss: 3.296875, Token per second per gpu: 25107.22457475002
Epoch: 43, Global Step: 433800, Data Step: 867600, Loss: 3.0625, Token per second per gpu: 25026.24097516903
Epoch: 43, Global Step: 433900, Data Step: 867800, Loss: 3.015625, Token per second per gpu: 25026.53522066495
Epoch: 43, Global Step: 434000, Data Step: 868000, Loss: 2.84375, Token per second per gpu: 25024.54029400473
Epoch: 43, Global Step: 434100, Data Step: 868200, Loss: 2.890625, Token per second per gpu: 25020.44054094827
Epoch: 43, Global Step: 434200, Data Step: 868400, Loss: 3.21875, Token per second per gpu: 25008.77597271123
Epoch: 43, Global Step: 434300, Data Step: 868600, Loss: 3.21875, Token per second per gpu: 25033.614597068867
Epoch: 43, Global Step: 434400, Data Step: 868800, Loss: 2.90625, Token per second per gpu: 25232.123401781282
Epoch: 43, Global Step: 434500, Data Step: 869000, Loss: 2.859375, Token per second per gpu: 25231.041195281563
Epoch: 43, Global Step: 434600, Data Step: 869200, Loss: 3.125, Token per second per gpu: 25109.437671340624
Epoch: 43, Global Step: 434700, Data Step: 869400, Loss: 3.125, Token per second per gpu: 25068.903285068733
Epoch: 43, Global Step: 434800, Data Step: 869600, Loss: 3.15625, Token per second per gpu: 25077.280354858056
Epoch: 43, Global Step: 434900, Data Step: 869800, Loss: 2.75, Token per second per gpu: 25081.88423466531
Epoch: 43, Global Step: 435000, Data Step: 870000, Loss: 2.921875, Token per second per gpu: 25157.582428408085
Epoch: 43, Global Step: 435100, Data Step: 870200, Loss: 2.9375, Token per second per gpu: 25200.112901120556
Epoch: 43, Global Step: 435200, Data Step: 870400, Loss: 2.90625, Token per second per gpu: 25156.27410185299
Epoch: 43, Global Step: 435300, Data Step: 870600, Loss: 3.0625, Token per second per gpu: 25092.64329233882
Epoch: 43, Global Step: 435400, Data Step: 870800, Loss: 2.640625, Token per second per gpu: 25052.148934752975
Epoch: 43, Global Step: 435500, Data Step: 871000, Loss: 3.21875, Token per second per gpu: 25222.416587030344
Epoch: 43, Global Step: 435600, Data Step: 871200, Loss: 3.15625, Token per second per gpu: 25479.890833067784
Epoch: 43, Global Step: 435700, Data Step: 871400, Loss: 2.9375, Token per second per gpu: 25455.65063613283
Epoch: 43, Global Step: 435800, Data Step: 871600, Loss: 2.796875, Token per second per gpu: 25369.158439118495
Epoch: 43, Global Step: 435900, Data Step: 871800, Loss: 2.921875, Token per second per gpu: 25517.763667451298
Epoch: 43, Global Step: 436000, Data Step: 872000, Loss: 2.890625, Token per second per gpu: 25602.437786372087
Epoch: 43, Global Step: 436100, Data Step: 872200, Loss: 2.8125, Token per second per gpu: 25503.3714284684
Epoch: 43, Global Step: 436200, Data Step: 872400, Loss: 3.0625, Token per second per gpu: 25373.636674590307
Epoch: 43, Global Step: 436300, Data Step: 872600, Loss: 3.390625, Token per second per gpu: 25457.654319868343
Epoch: 43, Global Step: 436400, Data Step: 872800, Loss: 3.171875, Token per second per gpu: 25722.390728924307
Epoch: 43, Global Step: 436500, Data Step: 873000, Loss: 3.0625, Token per second per gpu: 25712.58497783024
Epoch: 43, Global Step: 436600, Data Step: 873200, Loss: 2.796875, Token per second per gpu: 25576.36277392937
Epoch: 43, Global Step: 436700, Data Step: 873400, Loss: 2.640625, Token per second per gpu: 25461.841566505078
Epoch: 43, Global Step: 436800, Data Step: 873600, Loss: 3.21875, Token per second per gpu: 24713.34363293688
Epoch: 43, Global Step: 436900, Data Step: 873800, Loss: 2.921875, Token per second per gpu: 25550.10557221409
Epoch: 43, Global Step: 437000, Data Step: 874000, Loss: 3.28125, Token per second per gpu: 24500.396497838246
Epoch: 43, Global Step: 437100, Data Step: 874200, Loss: 2.96875, Token per second per gpu: 22997.225147189834
Epoch: 43, Global Step: 437200, Data Step: 874400, Loss: 3.015625, Token per second per gpu: 22748.550296795755
Epoch: 43, Global Step: 437300, Data Step: 874600, Loss: 2.875, Token per second per gpu: 22309.611988781693
Epoch: 43, Global Step: 437400, Data Step: 874800, Loss: 3.25, Token per second per gpu: 22509.64994730907
Epoch: 43, Global Step: 437500, Data Step: 875000, Loss: 2.96875, Token per second per gpu: 22309.853154104403
Epoch: 43, Global Step: 437600, Data Step: 875200, Loss: 2.859375, Token per second per gpu: 22387.208724507305
Epoch: 43, Global Step: 437700, Data Step: 875400, Loss: 2.953125, Token per second per gpu: 22091.932047784416
Epoch: 43, Global Step: 437800, Data Step: 875600, Loss: 2.75, Token per second per gpu: 22361.720026614894
Epoch: 43, Global Step: 437900, Data Step: 875800, Loss: 2.96875, Token per second per gpu: 22401.678716360708
Epoch: 43, Global Step: 438000, Data Step: 876000, Loss: 3.21875, Token per second per gpu: 22195.746350732665
Epoch: 43, Global Step: 438100, Data Step: 876200, Loss: 2.859375, Token per second per gpu: 21791.582890288224
Epoch: 43, Global Step: 438200, Data Step: 876400, Loss: 2.75, Token per second per gpu: 22557.870438237835
Epoch: 43, Global Step: 438300, Data Step: 876600, Loss: 2.65625, Token per second per gpu: 22228.334131056075
Epoch: 43, Global Step: 438400, Data Step: 876800, Loss: 3.078125, Token per second per gpu: 22623.218903108416
Epoch: 43, Global Step: 438500, Data Step: 877000, Loss: 3.109375, Token per second per gpu: 22309.91261186111
Epoch: 43, Global Step: 438600, Data Step: 877200, Loss: 2.75, Token per second per gpu: 22610.452252769293
Epoch: 43, Global Step: 438700, Data Step: 877400, Loss: 2.96875, Token per second per gpu: 22838.979404746617
Epoch: 43, Global Step: 438800, Data Step: 877600, Loss: 3.1875, Token per second per gpu: 22426.856540128454
Epoch: 43, Global Step: 438900, Data Step: 877800, Loss: 3.0625, Token per second per gpu: 22491.427281394564
Epoch: 43, Global Step: 439000, Data Step: 878000, Loss: 3.03125, Token per second per gpu: 22692.08843263206
Epoch: 43, Global Step: 439100, Data Step: 878200, Loss: 2.84375, Token per second per gpu: 22180.951800352996
Epoch: 43, Global Step: 439200, Data Step: 878400, Loss: 3.078125, Token per second per gpu: 22546.17491782764
Epoch: 43, Global Step: 439300, Data Step: 878600, Loss: 2.84375, Token per second per gpu: 22345.881689345508
Epoch: 43, Global Step: 439400, Data Step: 878800, Loss: 2.96875, Token per second per gpu: 22465.74994757754
Epoch: 43, Global Step: 439500, Data Step: 879000, Loss: 3.046875, Token per second per gpu: 22653.886283828735
Epoch: 43, Global Step: 439600, Data Step: 879200, Loss: 3.21875, Token per second per gpu: 22247.784186011068
Epoch: 43, Global Step: 439700, Data Step: 879400, Loss: 3.0, Token per second per gpu: 22066.57102010124
Epoch: 43, Global Step: 439800, Data Step: 879600, Loss: 3.171875, Token per second per gpu: 22418.07309957394
Epoch: 43, Global Step: 439900, Data Step: 879800, Loss: 2.9375, Token per second per gpu: 22361.02356118336
Epoch: 43, Global Step: 440000, Data Step: 880000, Loss: 2.875, Token per second per gpu: 22653.803863718742
I0401 21:28:56.986034 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0401 21:28:56.986557 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-01 21:28:56,996] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-01 21:28:57,009] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-01 21:28:57,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-01 21:28:57,602] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-01 21:28:57,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-01 21:28:57,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-01 21:28:57,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-01 21:28:57,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-01 21:28:57,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-01 21:28:57,647] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-01 21:28:58,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-01 21:28:58,213] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-01 21:28:58,213] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 21:28:58,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-01 21:28:58,772] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-01 21:28:58,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 21:28:59,199] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-01 21:28:59,200] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-01 21:28:59,200] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 21:28:59,259] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-01 21:28:59,259] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-01 21:28:59,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 21:28:59,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-01 21:28:59,298] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-01 21:28:59,298] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-01 21:28:59,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-01 21:28:59,342] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-01 21:28:59,342] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0401 21:28:59.343401 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0401 21:28:59.344724 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0401 21:28:59.345241 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0401 21:28:59.369497 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 43, Global Step: 440100, Data Step: 880200, Loss: 3.125, Token per second per gpu: 22560.067785562103
Epoch: 43, Global Step: 440200, Data Step: 880400, Loss: 3.078125, Token per second per gpu: 22327.211993206216
Epoch: 43, Global Step: 440300, Data Step: 880600, Loss: 3.140625, Token per second per gpu: 23012.28788351576
Epoch: 43, Global Step: 440400, Data Step: 880800, Loss: 3.046875, Token per second per gpu: 23524.827000896425
Epoch: 43, Global Step: 440500, Data Step: 881000, Loss: 2.765625, Token per second per gpu: 23089.416401556853
Epoch: 43, Global Step: 440600, Data Step: 881200, Loss: 2.578125, Token per second per gpu: 23160.156209287426
Epoch: 43, Global Step: 440700, Data Step: 881400, Loss: 2.9375, Token per second per gpu: 22585.667969975642
Epoch: 43, Global Step: 440800, Data Step: 881600, Loss: 2.921875, Token per second per gpu: 24041.619155844455
Epoch: 43, Global Step: 440900, Data Step: 881800, Loss: 2.9375, Token per second per gpu: 23231.60267496553
Epoch: 43, Global Step: 441000, Data Step: 882000, Loss: 3.03125, Token per second per gpu: 23130.014166899582
Epoch: 43, Global Step: 441100, Data Step: 882200, Loss: 2.78125, Token per second per gpu: 22355.01580794974
Epoch: 43, Global Step: 441200, Data Step: 882400, Loss: 2.875, Token per second per gpu: 22811.972947644485
Epoch: 43, Global Step: 441300, Data Step: 882600, Loss: 2.828125, Token per second per gpu: 22906.19848131256
Epoch: 43, Global Step: 441400, Data Step: 882800, Loss: 3.015625, Token per second per gpu: 23008.34829302234
Epoch: 43, Global Step: 441500, Data Step: 883000, Loss: 2.8125, Token per second per gpu: 22789.258347501884
Epoch: 43, Global Step: 441600, Data Step: 883200, Loss: 3.09375, Token per second per gpu: 22612.51470693354
Epoch: 43, Global Step: 441700, Data Step: 883400, Loss: 2.953125, Token per second per gpu: 23067.02564310572
Epoch: 43, Global Step: 441800, Data Step: 883600, Loss: 2.859375, Token per second per gpu: 23039.891235865005
Epoch: 43, Global Step: 441900, Data Step: 883800, Loss: 2.671875, Token per second per gpu: 22745.306101788763
Epoch: 43, Global Step: 442000, Data Step: 884000, Loss: 3.1875, Token per second per gpu: 23132.20222396379
Epoch: 43, Global Step: 442100, Data Step: 884200, Loss: 2.953125, Token per second per gpu: 22916.28998255566
Epoch: 43, Global Step: 442200, Data Step: 884400, Loss: 3.0, Token per second per gpu: 22657.455097403723
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0401 22:16:21.554813 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 44, Global Step: 442300, Data Step: 884600, Loss: 3.03125, Token per second per gpu: 22243.3364383591
Epoch: 44, Global Step: 442400, Data Step: 884800, Loss: 2.9375, Token per second per gpu: 22532.566758200574
Epoch: 44, Global Step: 442500, Data Step: 885000, Loss: 3.0, Token per second per gpu: 22641.60465750404
Epoch: 44, Global Step: 442600, Data Step: 885200, Loss: 2.640625, Token per second per gpu: 22778.456494587717
Epoch: 44, Global Step: 442700, Data Step: 885400, Loss: 2.9375, Token per second per gpu: 22368.350438222602
Epoch: 44, Global Step: 442800, Data Step: 885600, Loss: 3.0, Token per second per gpu: 22396.97961721876
Epoch: 44, Global Step: 442900, Data Step: 885800, Loss: 3.109375, Token per second per gpu: 22786.50452678218
Epoch: 44, Global Step: 443000, Data Step: 886000, Loss: 3.3125, Token per second per gpu: 22082.877944036867
Epoch: 44, Global Step: 443100, Data Step: 886200, Loss: 3.40625, Token per second per gpu: 23137.399471983415
Epoch: 44, Global Step: 443200, Data Step: 886400, Loss: 3.234375, Token per second per gpu: 22623.293897901156
Epoch: 44, Global Step: 443300, Data Step: 886600, Loss: 2.921875, Token per second per gpu: 22895.77128787751
Epoch: 44, Global Step: 443400, Data Step: 886800, Loss: 2.84375, Token per second per gpu: 22454.58474158315
Epoch: 44, Global Step: 443500, Data Step: 887000, Loss: 2.90625, Token per second per gpu: 22561.143085888918
Epoch: 44, Global Step: 443600, Data Step: 887200, Loss: 2.90625, Token per second per gpu: 22727.86154877617
Epoch: 44, Global Step: 443700, Data Step: 887400, Loss: 2.765625, Token per second per gpu: 22575.896125631203
Epoch: 44, Global Step: 443800, Data Step: 887600, Loss: 3.0625, Token per second per gpu: 22809.788065199024
Epoch: 44, Global Step: 443900, Data Step: 887800, Loss: 2.71875, Token per second per gpu: 22565.38844728946
Epoch: 44, Global Step: 444000, Data Step: 888000, Loss: 3.15625, Token per second per gpu: 22538.852134174802
Epoch: 44, Global Step: 444100, Data Step: 888200, Loss: 3.0, Token per second per gpu: 22323.08089277787
Epoch: 44, Global Step: 444200, Data Step: 888400, Loss: 2.875, Token per second per gpu: 22118.50750102248
Epoch: 44, Global Step: 444300, Data Step: 888600, Loss: 3.0, Token per second per gpu: 22314.559867597225
Epoch: 44, Global Step: 444400, Data Step: 888800, Loss: 3.25, Token per second per gpu: 23013.844121420017
Epoch: 44, Global Step: 444500, Data Step: 889000, Loss: 2.765625, Token per second per gpu: 22652.0224189022
Epoch: 44, Global Step: 444600, Data Step: 889200, Loss: 3.09375, Token per second per gpu: 22795.90221407311
Epoch: 44, Global Step: 444700, Data Step: 889400, Loss: 3.40625, Token per second per gpu: 23173.98778876422
Epoch: 44, Global Step: 444800, Data Step: 889600, Loss: 2.859375, Token per second per gpu: 22887.43544638937
Epoch: 44, Global Step: 444900, Data Step: 889800, Loss: 2.875, Token per second per gpu: 22812.86059761581
Epoch: 44, Global Step: 445000, Data Step: 890000, Loss: 2.9375, Token per second per gpu: 23084.008646641912
Epoch: 44, Global Step: 445100, Data Step: 890200, Loss: 3.1875, Token per second per gpu: 22872.50811419454
Epoch: 44, Global Step: 445200, Data Step: 890400, Loss: 3.015625, Token per second per gpu: 22671.88593959818
Epoch: 44, Global Step: 445300, Data Step: 890600, Loss: 2.828125, Token per second per gpu: 22607.91152587171
Epoch: 44, Global Step: 445400, Data Step: 890800, Loss: 2.984375, Token per second per gpu: 22703.07186042887
Epoch: 44, Global Step: 445500, Data Step: 891000, Loss: 2.953125, Token per second per gpu: 22968.40318071013
Epoch: 44, Global Step: 445600, Data Step: 891200, Loss: 2.9375, Token per second per gpu: 22749.73052955287
Epoch: 44, Global Step: 445700, Data Step: 891400, Loss: 3.171875, Token per second per gpu: 22466.954963929034
Epoch: 44, Global Step: 445800, Data Step: 891600, Loss: 2.671875, Token per second per gpu: 22828.217953366726
Epoch: 44, Global Step: 445900, Data Step: 891800, Loss: 2.828125, Token per second per gpu: 22962.526568822737
Epoch: 44, Global Step: 446000, Data Step: 892000, Loss: 2.796875, Token per second per gpu: 22842.423285880064
Epoch: 44, Global Step: 446100, Data Step: 892200, Loss: 3.03125, Token per second per gpu: 22721.853614578733
Epoch: 44, Global Step: 446200, Data Step: 892400, Loss: 2.90625, Token per second per gpu: 22485.660290507072
Epoch: 44, Global Step: 446300, Data Step: 892600, Loss: 2.921875, Token per second per gpu: 22574.21934240622
Epoch: 44, Global Step: 446400, Data Step: 892800, Loss: 3.03125, Token per second per gpu: 22443.657973508387
Epoch: 44, Global Step: 446500, Data Step: 893000, Loss: 2.890625, Token per second per gpu: 22672.79587166366
Epoch: 44, Global Step: 446600, Data Step: 893200, Loss: 2.796875, Token per second per gpu: 22349.249950615846
Epoch: 44, Global Step: 446700, Data Step: 893400, Loss: 2.8125, Token per second per gpu: 22249.857027909205
Epoch: 44, Global Step: 446800, Data Step: 893600, Loss: 2.453125, Token per second per gpu: 21775.437667468388
Epoch: 44, Global Step: 446900, Data Step: 893800, Loss: 2.828125, Token per second per gpu: 22780.379975392858
Epoch: 44, Global Step: 447000, Data Step: 894000, Loss: 3.203125, Token per second per gpu: 22574.15003026448
Epoch: 44, Global Step: 447100, Data Step: 894200, Loss: 2.890625, Token per second per gpu: 23027.957679289237
Epoch: 44, Global Step: 447200, Data Step: 894400, Loss: 2.984375, Token per second per gpu: 22927.171551391053
Epoch: 44, Global Step: 447300, Data Step: 894600, Loss: 3.234375, Token per second per gpu: 23021.033711651715
Epoch: 44, Global Step: 447400, Data Step: 894800, Loss: 3.046875, Token per second per gpu: 22642.200512452517
Epoch: 44, Global Step: 447500, Data Step: 895000, Loss: 3.015625, Token per second per gpu: 22672.12551150394
Epoch: 44, Global Step: 447600, Data Step: 895200, Loss: 3.0, Token per second per gpu: 22492.501660306647
Epoch: 44, Global Step: 447700, Data Step: 895400, Loss: 3.0, Token per second per gpu: 22504.37800726334
Epoch: 44, Global Step: 447800, Data Step: 895600, Loss: 2.8125, Token per second per gpu: 22697.382421841856
Epoch: 44, Global Step: 447900, Data Step: 895800, Loss: 2.953125, Token per second per gpu: 22728.10773610239
Epoch: 44, Global Step: 448000, Data Step: 896000, Loss: 2.859375, Token per second per gpu: 22120.056020478158
Epoch: 44, Global Step: 448100, Data Step: 896200, Loss: 2.8125, Token per second per gpu: 22967.09648301783
Epoch: 44, Global Step: 448200, Data Step: 896400, Loss: 2.828125, Token per second per gpu: 22817.649946504043
Epoch: 44, Global Step: 448300, Data Step: 896600, Loss: 2.796875, Token per second per gpu: 23238.689581435927
Epoch: 44, Global Step: 448400, Data Step: 896800, Loss: 3.109375, Token per second per gpu: 22852.274835305725
Epoch: 44, Global Step: 448500, Data Step: 897000, Loss: 2.921875, Token per second per gpu: 22816.124998412688
Epoch: 44, Global Step: 448600, Data Step: 897200, Loss: 2.609375, Token per second per gpu: 23204.592351814546
Epoch: 44, Global Step: 448700, Data Step: 897400, Loss: 3.1875, Token per second per gpu: 23164.614605571453
Epoch: 44, Global Step: 448800, Data Step: 897600, Loss: 2.734375, Token per second per gpu: 22534.63167453974
Epoch: 44, Global Step: 448900, Data Step: 897800, Loss: 2.90625, Token per second per gpu: 22889.377507139892
Epoch: 44, Global Step: 449000, Data Step: 898000, Loss: 3.015625, Token per second per gpu: 22997.47063711518
Epoch: 44, Global Step: 449100, Data Step: 898200, Loss: 2.9375, Token per second per gpu: 22330.398525906247
Epoch: 44, Global Step: 449200, Data Step: 898400, Loss: 3.0, Token per second per gpu: 22515.213963073194
Epoch: 44, Global Step: 449300, Data Step: 898600, Loss: 2.890625, Token per second per gpu: 22677.773257736797
Epoch: 44, Global Step: 449400, Data Step: 898800, Loss: 3.03125, Token per second per gpu: 22870.570040687595
Epoch: 44, Global Step: 449500, Data Step: 899000, Loss: 2.859375, Token per second per gpu: 23366.798507667358
Epoch: 44, Global Step: 449600, Data Step: 899200, Loss: 3.015625, Token per second per gpu: 23453.519390844485
Epoch: 44, Global Step: 449700, Data Step: 899400, Loss: 2.796875, Token per second per gpu: 22884.87583849545
Epoch: 44, Global Step: 449800, Data Step: 899600, Loss: 2.90625, Token per second per gpu: 22627.57582342918
Epoch: 44, Global Step: 449900, Data Step: 899800, Loss: 3.171875, Token per second per gpu: 22775.46493482869
Epoch: 44, Global Step: 450000, Data Step: 900000, Loss: 2.5625, Token per second per gpu: 22629.364659107734
I0402 00:59:57.061340 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0402 00:59:57.061793 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-02 00:59:57,062] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-02 00:59:57,066] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-02 00:59:57,066] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-02 00:59:58,880] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-02 00:59:58,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-02 00:59:58,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-02 00:59:58,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-02 00:59:58,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-02 00:59:58,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-02 00:59:58,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-02 00:59:59,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-02 00:59:59,479] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-02 00:59:59,479] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 01:00:00,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-02 01:00:00,262] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-02 01:00:00,262] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 01:00:00,363] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-02 01:00:00,363] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-02 01:00:00,363] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 01:00:00,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-02 01:00:00,519] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-02 01:00:00,519] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 01:00:00,610] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-02 01:00:00,610] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-02 01:00:00,610] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 01:00:00,625] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-02 01:00:00,625] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-02 01:00:00,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0402 01:00:00.626168 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0402 01:00:00.630956 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0402 01:00:00.632032 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0402 01:00:00.636785 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 44, Global Step: 450100, Data Step: 900200, Loss: 2.609375, Token per second per gpu: 22087.880195568396
Epoch: 44, Global Step: 450200, Data Step: 900400, Loss: 2.8125, Token per second per gpu: 22263.964186772126
Epoch: 44, Global Step: 450300, Data Step: 900600, Loss: 3.21875, Token per second per gpu: 22198.887959456973
Epoch: 44, Global Step: 450400, Data Step: 900800, Loss: 2.640625, Token per second per gpu: 22763.480811356214
Epoch: 44, Global Step: 450500, Data Step: 901000, Loss: 3.390625, Token per second per gpu: 22543.0633629674
Epoch: 44, Global Step: 450600, Data Step: 901200, Loss: 2.828125, Token per second per gpu: 22865.913494160228
Epoch: 44, Global Step: 450700, Data Step: 901400, Loss: 3.109375, Token per second per gpu: 22616.437108197126
Epoch: 44, Global Step: 450800, Data Step: 901600, Loss: 3.015625, Token per second per gpu: 22599.375879377614
Epoch: 44, Global Step: 450900, Data Step: 901800, Loss: 2.859375, Token per second per gpu: 22610.01562520443
Epoch: 44, Global Step: 451000, Data Step: 902000, Loss: 3.046875, Token per second per gpu: 22945.94280843396
Epoch: 44, Global Step: 451100, Data Step: 902200, Loss: 3.265625, Token per second per gpu: 23429.90452172653
Epoch: 44, Global Step: 451200, Data Step: 902400, Loss: 3.046875, Token per second per gpu: 22751.031378054588
Epoch: 44, Global Step: 451300, Data Step: 902600, Loss: 3.015625, Token per second per gpu: 22976.831681056778
Epoch: 44, Global Step: 451400, Data Step: 902800, Loss: 3.234375, Token per second per gpu: 23347.41758052769
Epoch: 44, Global Step: 451500, Data Step: 903000, Loss: 2.859375, Token per second per gpu: 22616.27243202364
Epoch: 44, Global Step: 451600, Data Step: 903200, Loss: 3.078125, Token per second per gpu: 22731.885073074543
Epoch: 44, Global Step: 451700, Data Step: 903400, Loss: 2.65625, Token per second per gpu: 22930.15743967047
Epoch: 44, Global Step: 451800, Data Step: 903600, Loss: 3.046875, Token per second per gpu: 22666.130417100412
Epoch: 44, Global Step: 451900, Data Step: 903800, Loss: 2.90625, Token per second per gpu: 22537.363928068087
Epoch: 44, Global Step: 452000, Data Step: 904000, Loss: 3.0, Token per second per gpu: 22853.57096439678
Epoch: 44, Global Step: 452100, Data Step: 904200, Loss: 3.1875, Token per second per gpu: 22949.61195912862
Epoch: 44, Global Step: 452200, Data Step: 904400, Loss: 2.90625, Token per second per gpu: 22537.73093617085
Epoch: 44, Global Step: 452300, Data Step: 904600, Loss: 2.953125, Token per second per gpu: 22377.4182888998
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0402 01:48:57.067554 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 45, Global Step: 452400, Data Step: 904800, Loss: 2.96875, Token per second per gpu: 21690.03108682357
Epoch: 45, Global Step: 452500, Data Step: 905000, Loss: 2.859375, Token per second per gpu: 22924.17608653805
Epoch: 45, Global Step: 452600, Data Step: 905200, Loss: 2.625, Token per second per gpu: 22399.80349225668
Epoch: 45, Global Step: 452700, Data Step: 905400, Loss: 2.78125, Token per second per gpu: 22896.54034921176
Epoch: 45, Global Step: 452800, Data Step: 905600, Loss: 3.03125, Token per second per gpu: 22551.849464799434
Epoch: 45, Global Step: 452900, Data Step: 905800, Loss: 2.703125, Token per second per gpu: 22197.559907385887
Epoch: 45, Global Step: 453000, Data Step: 906000, Loss: 3.125, Token per second per gpu: 22357.211962833364
Epoch: 45, Global Step: 453100, Data Step: 906200, Loss: 2.84375, Token per second per gpu: 22319.187038369182
Epoch: 45, Global Step: 453200, Data Step: 906400, Loss: 3.0, Token per second per gpu: 22920.56142516353
Epoch: 45, Global Step: 453300, Data Step: 906600, Loss: 2.671875, Token per second per gpu: 22509.914039101826
Epoch: 45, Global Step: 453400, Data Step: 906800, Loss: 2.921875, Token per second per gpu: 22211.258515204583
Epoch: 45, Global Step: 453500, Data Step: 907000, Loss: 2.953125, Token per second per gpu: 22398.331639263717
Epoch: 45, Global Step: 453600, Data Step: 907200, Loss: 2.46875, Token per second per gpu: 22396.779626833315
Epoch: 45, Global Step: 453700, Data Step: 907400, Loss: 2.96875, Token per second per gpu: 22636.825838652112
Epoch: 45, Global Step: 453800, Data Step: 907600, Loss: 3.09375, Token per second per gpu: 22394.55925097272
Epoch: 45, Global Step: 453900, Data Step: 907800, Loss: 3.0625, Token per second per gpu: 22732.594650698724
Epoch: 45, Global Step: 454000, Data Step: 908000, Loss: 3.09375, Token per second per gpu: 23242.048510153756
Epoch: 45, Global Step: 454100, Data Step: 908200, Loss: 2.765625, Token per second per gpu: 21797.398608459447
Epoch: 45, Global Step: 454200, Data Step: 908400, Loss: 3.109375, Token per second per gpu: 22884.90896218578
Epoch: 45, Global Step: 454300, Data Step: 908600, Loss: 3.1875, Token per second per gpu: 22496.271886292852
Epoch: 45, Global Step: 454400, Data Step: 908800, Loss: 2.90625, Token per second per gpu: 22564.877980427504
Epoch: 45, Global Step: 454500, Data Step: 909000, Loss: 2.796875, Token per second per gpu: 22708.966422975194
Epoch: 45, Global Step: 454600, Data Step: 909200, Loss: 2.9375, Token per second per gpu: 22693.930587398652
Epoch: 45, Global Step: 454700, Data Step: 909400, Loss: 2.953125, Token per second per gpu: 22687.619026300843
Epoch: 45, Global Step: 454800, Data Step: 909600, Loss: 2.9375, Token per second per gpu: 22511.439944894475
Epoch: 45, Global Step: 454900, Data Step: 909800, Loss: 2.875, Token per second per gpu: 22433.779241602
Epoch: 45, Global Step: 455000, Data Step: 910000, Loss: 2.671875, Token per second per gpu: 22705.62975430314
Epoch: 45, Global Step: 455100, Data Step: 910200, Loss: 2.71875, Token per second per gpu: 22503.15451045577
Epoch: 45, Global Step: 455200, Data Step: 910400, Loss: 3.046875, Token per second per gpu: 22833.20749697673
Epoch: 45, Global Step: 455300, Data Step: 910600, Loss: 2.859375, Token per second per gpu: 22521.920985958594
Epoch: 45, Global Step: 455400, Data Step: 910800, Loss: 2.984375, Token per second per gpu: 22713.03450758715
Epoch: 45, Global Step: 455500, Data Step: 911000, Loss: 3.046875, Token per second per gpu: 22358.42729687569
Epoch: 45, Global Step: 455600, Data Step: 911200, Loss: 3.125, Token per second per gpu: 22239.53844283844
Epoch: 45, Global Step: 455700, Data Step: 911400, Loss: 3.015625, Token per second per gpu: 22594.57763454564
Epoch: 45, Global Step: 455800, Data Step: 911600, Loss: 2.609375, Token per second per gpu: 22479.82986486665
Epoch: 45, Global Step: 455900, Data Step: 911800, Loss: 3.03125, Token per second per gpu: 22322.84608298964
Epoch: 45, Global Step: 456000, Data Step: 912000, Loss: 2.890625, Token per second per gpu: 22299.60886191688
Epoch: 45, Global Step: 456100, Data Step: 912200, Loss: 3.21875, Token per second per gpu: 22603.227150962513
Epoch: 45, Global Step: 456200, Data Step: 912400, Loss: 3.09375, Token per second per gpu: 22448.402338495667
Epoch: 45, Global Step: 456300, Data Step: 912600, Loss: 2.75, Token per second per gpu: 22568.745162936055
Epoch: 45, Global Step: 456400, Data Step: 912800, Loss: 3.140625, Token per second per gpu: 22593.778266022564
Epoch: 45, Global Step: 456500, Data Step: 913000, Loss: 2.953125, Token per second per gpu: 22327.534385836178
Epoch: 45, Global Step: 456600, Data Step: 913200, Loss: 2.78125, Token per second per gpu: 22351.183097356887
Epoch: 45, Global Step: 456700, Data Step: 913400, Loss: 2.984375, Token per second per gpu: 21883.490374313224
Epoch: 45, Global Step: 456800, Data Step: 913600, Loss: 3.1875, Token per second per gpu: 22662.690079382726
Epoch: 45, Global Step: 456900, Data Step: 913800, Loss: 3.03125, Token per second per gpu: 22836.384989992253
Epoch: 45, Global Step: 457000, Data Step: 914000, Loss: 3.125, Token per second per gpu: 22533.194213310322
Epoch: 45, Global Step: 457100, Data Step: 914200, Loss: 3.015625, Token per second per gpu: 23333.06821985268
Epoch: 45, Global Step: 457200, Data Step: 914400, Loss: 3.078125, Token per second per gpu: 22592.092064990044
Epoch: 45, Global Step: 457300, Data Step: 914600, Loss: 3.09375, Token per second per gpu: 22906.498283146924
Epoch: 45, Global Step: 457400, Data Step: 914800, Loss: 2.828125, Token per second per gpu: 23090.81081195981
Epoch: 45, Global Step: 457500, Data Step: 915000, Loss: 2.71875, Token per second per gpu: 22495.07101220341
Epoch: 45, Global Step: 457600, Data Step: 915200, Loss: 3.0625, Token per second per gpu: 22448.721523779404
Epoch: 45, Global Step: 457700, Data Step: 915400, Loss: 3.28125, Token per second per gpu: 22769.99089015297
Epoch: 45, Global Step: 457800, Data Step: 915600, Loss: 2.6875, Token per second per gpu: 22629.606894286633
Epoch: 45, Global Step: 457900, Data Step: 915800, Loss: 3.09375, Token per second per gpu: 22754.416637938568
Epoch: 45, Global Step: 458000, Data Step: 916000, Loss: 2.765625, Token per second per gpu: 22278.821849517593
Epoch: 45, Global Step: 458100, Data Step: 916200, Loss: 3.015625, Token per second per gpu: 22788.5355120763
Epoch: 45, Global Step: 458200, Data Step: 916400, Loss: 2.9375, Token per second per gpu: 22726.376583992358
Epoch: 45, Global Step: 458300, Data Step: 916600, Loss: 2.875, Token per second per gpu: 22815.633031405978
Epoch: 45, Global Step: 458400, Data Step: 916800, Loss: 2.609375, Token per second per gpu: 22651.463213040442
Epoch: 45, Global Step: 458500, Data Step: 917000, Loss: 2.875, Token per second per gpu: 22889.52371714694
Epoch: 45, Global Step: 458600, Data Step: 917200, Loss: 3.0625, Token per second per gpu: 22453.78552290619
Epoch: 45, Global Step: 458700, Data Step: 917400, Loss: 2.734375, Token per second per gpu: 22859.608403612707
Epoch: 45, Global Step: 458800, Data Step: 917600, Loss: 2.609375, Token per second per gpu: 22605.52404237492
Epoch: 45, Global Step: 458900, Data Step: 917800, Loss: 3.109375, Token per second per gpu: 22914.759993940435
Epoch: 45, Global Step: 459000, Data Step: 918000, Loss: 2.890625, Token per second per gpu: 22850.188341322853
Epoch: 45, Global Step: 459100, Data Step: 918200, Loss: 2.65625, Token per second per gpu: 22425.373802656577
Epoch: 45, Global Step: 459200, Data Step: 918400, Loss: 2.890625, Token per second per gpu: 22493.383260385046
Epoch: 45, Global Step: 459300, Data Step: 918600, Loss: 2.6875, Token per second per gpu: 22579.505108939236
Epoch: 45, Global Step: 459400, Data Step: 918800, Loss: 3.0625, Token per second per gpu: 22496.038194903565
Epoch: 45, Global Step: 459500, Data Step: 919000, Loss: 2.9375, Token per second per gpu: 22367.949950406484
Epoch: 45, Global Step: 459600, Data Step: 919200, Loss: 3.015625, Token per second per gpu: 22633.168103987482
Epoch: 45, Global Step: 459700, Data Step: 919400, Loss: 3.328125, Token per second per gpu: 22434.902120701347
Epoch: 45, Global Step: 459800, Data Step: 919600, Loss: 3.09375, Token per second per gpu: 22552.890885837503
Epoch: 45, Global Step: 459900, Data Step: 919800, Loss: 2.90625, Token per second per gpu: 22363.964161524935
Epoch: 45, Global Step: 460000, Data Step: 920000, Loss: 2.734375, Token per second per gpu: 22703.15911975138
I0402 04:32:22.281199 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0402 04:32:22.281625 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-02 04:32:22,282] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-02 04:32:22,286] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-02 04:32:22,286] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-02 04:32:22,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-02 04:32:22,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-02 04:32:22,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-02 04:32:22,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-02 04:32:22,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-02 04:32:22,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-02 04:32:22,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-02 04:32:24,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-02 04:32:24,183] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-02 04:32:24,183] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 04:32:24,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-02 04:32:24,226] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-02 04:32:24,227] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 04:32:24,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-02 04:32:24,410] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-02 04:32:24,410] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 04:32:24,480] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-02 04:32:24,480] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-02 04:32:24,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 04:32:24,484] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-02 04:32:24,484] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-02 04:32:24,485] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 04:32:24,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-02 04:32:24,510] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-02 04:32:24,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0402 04:32:24.511735 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0402 04:32:24.512513 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0402 04:32:24.512839 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0402 04:32:24.514768 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 45, Global Step: 460100, Data Step: 920200, Loss: 2.734375, Token per second per gpu: 22130.6350236893
Epoch: 45, Global Step: 460200, Data Step: 920400, Loss: 2.703125, Token per second per gpu: 22300.93021457649
Epoch: 45, Global Step: 460300, Data Step: 920600, Loss: 3.0625, Token per second per gpu: 23133.170214381902
Epoch: 45, Global Step: 460400, Data Step: 920800, Loss: 3.15625, Token per second per gpu: 22064.067100097633
Epoch: 45, Global Step: 460500, Data Step: 921000, Loss: 3.0, Token per second per gpu: 22387.924788539804
Epoch: 45, Global Step: 460600, Data Step: 921200, Loss: 2.84375, Token per second per gpu: 22574.383027059714
Epoch: 45, Global Step: 460700, Data Step: 921400, Loss: 3.203125, Token per second per gpu: 22566.141164592507
Epoch: 45, Global Step: 460800, Data Step: 921600, Loss: 3.09375, Token per second per gpu: 22610.943706428126
Epoch: 45, Global Step: 460900, Data Step: 921800, Loss: 2.953125, Token per second per gpu: 23014.779868253005
Epoch: 45, Global Step: 461000, Data Step: 922000, Loss: 2.90625, Token per second per gpu: 22473.306092715426
Epoch: 45, Global Step: 461100, Data Step: 922200, Loss: 2.78125, Token per second per gpu: 23299.596329849683
Epoch: 45, Global Step: 461200, Data Step: 922400, Loss: 3.25, Token per second per gpu: 23026.4355686712
Epoch: 45, Global Step: 461300, Data Step: 922600, Loss: 2.9375, Token per second per gpu: 23233.09577300096
Epoch: 45, Global Step: 461400, Data Step: 922800, Loss: 2.765625, Token per second per gpu: 23182.41393816383
Epoch: 45, Global Step: 461500, Data Step: 923000, Loss: 2.921875, Token per second per gpu: 23046.04257066349
Epoch: 45, Global Step: 461600, Data Step: 923200, Loss: 3.109375, Token per second per gpu: 22384.262298970363
Epoch: 45, Global Step: 461700, Data Step: 923400, Loss: 2.953125, Token per second per gpu: 22836.56955184813
Epoch: 45, Global Step: 461800, Data Step: 923600, Loss: 2.953125, Token per second per gpu: 22550.203782822544
Epoch: 45, Global Step: 461900, Data Step: 923800, Loss: 3.09375, Token per second per gpu: 22612.961675216506
Epoch: 45, Global Step: 462000, Data Step: 924000, Loss: 2.890625, Token per second per gpu: 22804.91500509536
Epoch: 45, Global Step: 462100, Data Step: 924200, Loss: 2.8125, Token per second per gpu: 23161.23596397202
Epoch: 45, Global Step: 462200, Data Step: 924400, Loss: 2.890625, Token per second per gpu: 23058.919566245368
Epoch: 45, Global Step: 462300, Data Step: 924600, Loss: 3.015625, Token per second per gpu: 22426.952140185753
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0402 05:22:27.479572 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 46, Global Step: 462400, Data Step: 924800, Loss: 2.859375, Token per second per gpu: 20681.03040514076
Epoch: 46, Global Step: 462500, Data Step: 925000, Loss: 3.078125, Token per second per gpu: 23058.793412738847
Epoch: 46, Global Step: 462600, Data Step: 925200, Loss: 2.828125, Token per second per gpu: 23110.43488292585
Epoch: 46, Global Step: 462700, Data Step: 925400, Loss: 3.171875, Token per second per gpu: 22891.45051534038
Epoch: 46, Global Step: 462800, Data Step: 925600, Loss: 3.1875, Token per second per gpu: 22512.657294126126
Epoch: 46, Global Step: 462900, Data Step: 925800, Loss: 3.109375, Token per second per gpu: 22444.711487235138
Epoch: 46, Global Step: 463000, Data Step: 926000, Loss: 3.015625, Token per second per gpu: 23306.53425621707
Epoch: 46, Global Step: 463100, Data Step: 926200, Loss: 3.1875, Token per second per gpu: 23074.198126237494
Epoch: 46, Global Step: 463200, Data Step: 926400, Loss: 3.203125, Token per second per gpu: 22205.914520208873
Epoch: 46, Global Step: 463300, Data Step: 926600, Loss: 2.84375, Token per second per gpu: 22504.56780708197
Epoch: 46, Global Step: 463400, Data Step: 926800, Loss: 3.0625, Token per second per gpu: 22715.989226325626
Epoch: 46, Global Step: 463500, Data Step: 927000, Loss: 3.015625, Token per second per gpu: 22941.03941965264
Epoch: 46, Global Step: 463600, Data Step: 927200, Loss: 2.71875, Token per second per gpu: 22614.0839359864
Epoch: 46, Global Step: 463700, Data Step: 927400, Loss: 3.078125, Token per second per gpu: 22556.741197655916
Epoch: 46, Global Step: 463800, Data Step: 927600, Loss: 2.890625, Token per second per gpu: 22656.967016846756
Epoch: 46, Global Step: 463900, Data Step: 927800, Loss: 2.671875, Token per second per gpu: 22852.5265354527
Epoch: 46, Global Step: 464000, Data Step: 928000, Loss: 2.890625, Token per second per gpu: 22849.73583591364
Epoch: 46, Global Step: 464100, Data Step: 928200, Loss: 2.90625, Token per second per gpu: 22899.095144183237
Epoch: 46, Global Step: 464200, Data Step: 928400, Loss: 3.078125, Token per second per gpu: 23024.43709170259
Epoch: 46, Global Step: 464300, Data Step: 928600, Loss: 2.921875, Token per second per gpu: 22671.53756937983
Epoch: 46, Global Step: 464400, Data Step: 928800, Loss: 2.984375, Token per second per gpu: 23392.729475168053
Epoch: 46, Global Step: 464500, Data Step: 929000, Loss: 2.921875, Token per second per gpu: 23194.140291175547
Epoch: 46, Global Step: 464600, Data Step: 929200, Loss: 2.84375, Token per second per gpu: 22894.065875483422
Epoch: 46, Global Step: 464700, Data Step: 929400, Loss: 3.0, Token per second per gpu: 22455.74827661233
Epoch: 46, Global Step: 464800, Data Step: 929600, Loss: 2.453125, Token per second per gpu: 22659.513128976974
Epoch: 46, Global Step: 464900, Data Step: 929800, Loss: 2.703125, Token per second per gpu: 23297.426412605073
Epoch: 46, Global Step: 465000, Data Step: 930000, Loss: 2.984375, Token per second per gpu: 22714.484071882405
Epoch: 46, Global Step: 465100, Data Step: 930200, Loss: 2.734375, Token per second per gpu: 23121.8445592901
Epoch: 46, Global Step: 465200, Data Step: 930400, Loss: 2.875, Token per second per gpu: 22060.657011228985
Epoch: 46, Global Step: 465300, Data Step: 930600, Loss: 2.625, Token per second per gpu: 22625.61368576896
Epoch: 46, Global Step: 465400, Data Step: 930800, Loss: 2.609375, Token per second per gpu: 22563.19027415635
Epoch: 46, Global Step: 465500, Data Step: 931000, Loss: 3.0, Token per second per gpu: 23140.58674868701
Epoch: 46, Global Step: 465600, Data Step: 931200, Loss: 2.96875, Token per second per gpu: 22581.329409095277
Epoch: 46, Global Step: 465700, Data Step: 931400, Loss: 3.09375, Token per second per gpu: 22671.44540433391
Epoch: 46, Global Step: 465800, Data Step: 931600, Loss: 3.125, Token per second per gpu: 22558.612332656157
Epoch: 46, Global Step: 465900, Data Step: 931800, Loss: 2.53125, Token per second per gpu: 22529.905296195448
Epoch: 46, Global Step: 466000, Data Step: 932000, Loss: 2.90625, Token per second per gpu: 22938.115858712335
Epoch: 46, Global Step: 466100, Data Step: 932200, Loss: 2.8125, Token per second per gpu: 22095.58812588039
Epoch: 46, Global Step: 466200, Data Step: 932400, Loss: 3.078125, Token per second per gpu: 22362.514650413625
Epoch: 46, Global Step: 466300, Data Step: 932600, Loss: 3.0625, Token per second per gpu: 22566.371761762468
Epoch: 46, Global Step: 466400, Data Step: 932800, Loss: 2.75, Token per second per gpu: 22353.75335593027
Epoch: 46, Global Step: 466500, Data Step: 933000, Loss: 3.015625, Token per second per gpu: 22376.089514942454
Epoch: 46, Global Step: 466600, Data Step: 933200, Loss: 2.984375, Token per second per gpu: 21078.186565650285
Epoch: 46, Global Step: 466700, Data Step: 933400, Loss: 3.078125, Token per second per gpu: 23466.432205847348
Epoch: 46, Global Step: 466800, Data Step: 933600, Loss: 2.9375, Token per second per gpu: 22473.125348967184
Epoch: 46, Global Step: 466900, Data Step: 933800, Loss: 2.921875, Token per second per gpu: 22270.184629036026
Epoch: 46, Global Step: 467000, Data Step: 934000, Loss: 3.015625, Token per second per gpu: 22366.230032995107
Epoch: 46, Global Step: 467100, Data Step: 934200, Loss: 2.875, Token per second per gpu: 22648.0456699484
Epoch: 46, Global Step: 467200, Data Step: 934400, Loss: 2.875, Token per second per gpu: 22486.38224792874
Epoch: 46, Global Step: 467300, Data Step: 934600, Loss: 2.9375, Token per second per gpu: 22379.269036214846
Epoch: 46, Global Step: 467400, Data Step: 934800, Loss: 2.75, Token per second per gpu: 22453.197039152412
Epoch: 46, Global Step: 467500, Data Step: 935000, Loss: 3.015625, Token per second per gpu: 22624.64749108522
Epoch: 46, Global Step: 467600, Data Step: 935200, Loss: 3.046875, Token per second per gpu: 22271.873175282737
Epoch: 46, Global Step: 467700, Data Step: 935400, Loss: 3.109375, Token per second per gpu: 22633.27047512661
Epoch: 46, Global Step: 467800, Data Step: 935600, Loss: 2.796875, Token per second per gpu: 22606.234256607346
Epoch: 46, Global Step: 467900, Data Step: 935800, Loss: 3.0625, Token per second per gpu: 21427.576616976457
Epoch: 46, Global Step: 468000, Data Step: 936000, Loss: 3.21875, Token per second per gpu: 22748.09572478161
Epoch: 46, Global Step: 468100, Data Step: 936200, Loss: 2.765625, Token per second per gpu: 22712.94482347585
Epoch: 46, Global Step: 468200, Data Step: 936400, Loss: 3.21875, Token per second per gpu: 22769.63580740427
Epoch: 46, Global Step: 468300, Data Step: 936600, Loss: 2.828125, Token per second per gpu: 23142.512775334286
Epoch: 46, Global Step: 468400, Data Step: 936800, Loss: 2.890625, Token per second per gpu: 23068.603743365697
Epoch: 46, Global Step: 468500, Data Step: 937000, Loss: 3.0, Token per second per gpu: 22761.68336266608
Epoch: 46, Global Step: 468600, Data Step: 937200, Loss: 3.0625, Token per second per gpu: 23261.651297241937
Epoch: 46, Global Step: 468700, Data Step: 937400, Loss: 3.109375, Token per second per gpu: 22448.886856230227
Epoch: 46, Global Step: 468800, Data Step: 937600, Loss: 2.640625, Token per second per gpu: 22511.98634287728
Epoch: 46, Global Step: 468900, Data Step: 937800, Loss: 2.9375, Token per second per gpu: 22411.623288562478
Epoch: 46, Global Step: 469000, Data Step: 938000, Loss: 3.03125, Token per second per gpu: 23290.48443167066
Epoch: 46, Global Step: 469100, Data Step: 938200, Loss: 3.0, Token per second per gpu: 22581.195721191954
Epoch: 46, Global Step: 469200, Data Step: 938400, Loss: 2.84375, Token per second per gpu: 22466.25234583984
Epoch: 46, Global Step: 469300, Data Step: 938600, Loss: 3.0, Token per second per gpu: 22746.160045168006
Epoch: 46, Global Step: 469400, Data Step: 938800, Loss: 3.046875, Token per second per gpu: 22620.354558964907
Epoch: 46, Global Step: 469500, Data Step: 939000, Loss: 3.140625, Token per second per gpu: 23111.43032483474
Epoch: 46, Global Step: 469600, Data Step: 939200, Loss: 2.78125, Token per second per gpu: 22782.98926664415
Epoch: 46, Global Step: 469700, Data Step: 939400, Loss: 3.25, Token per second per gpu: 22837.99459932917
Epoch: 46, Global Step: 469800, Data Step: 939600, Loss: 2.890625, Token per second per gpu: 23100.593057211674
Epoch: 46, Global Step: 469900, Data Step: 939800, Loss: 2.640625, Token per second per gpu: 23139.683252796876
Epoch: 46, Global Step: 470000, Data Step: 940000, Loss: 3.015625, Token per second per gpu: 23499.535485391436
I0402 08:04:01.148141 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0402 08:04:01.153165 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-02 08:04:01,154] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-02 08:04:01,182] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-02 08:04:01,187] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-02 08:04:02,345] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-02 08:04:02,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-02 08:04:02,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-02 08:04:02,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-02 08:04:02,436] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-02 08:04:02,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-02 08:04:02,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-02 08:04:03,873] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-02 08:04:03,873] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-02 08:04:03,874] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 08:04:04,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-02 08:04:04,226] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-02 08:04:04,226] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 08:04:04,242] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-02 08:04:04,242] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-02 08:04:04,242] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 08:04:04,243] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-02 08:04:04,243] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-02 08:04:04,243] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 08:04:04,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-02 08:04:04,266] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-02 08:04:04,266] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 08:04:04,379] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-02 08:04:04,379] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-02 08:04:04,379] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0402 08:04:04.380304 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0402 08:04:04.381105 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0402 08:04:04.381366 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0402 08:04:04.382595 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 46, Global Step: 470100, Data Step: 940200, Loss: 2.953125, Token per second per gpu: 22620.543736156647
Epoch: 46, Global Step: 470200, Data Step: 940400, Loss: 3.109375, Token per second per gpu: 22751.704826976296
Epoch: 46, Global Step: 470300, Data Step: 940600, Loss: 3.125, Token per second per gpu: 22545.277014117008
Epoch: 46, Global Step: 470400, Data Step: 940800, Loss: 3.21875, Token per second per gpu: 22648.98621852477
Epoch: 46, Global Step: 470500, Data Step: 941000, Loss: 3.015625, Token per second per gpu: 23567.404050202975
Epoch: 46, Global Step: 470600, Data Step: 941200, Loss: 2.984375, Token per second per gpu: 23271.921076123654
Epoch: 46, Global Step: 470700, Data Step: 941400, Loss: 2.71875, Token per second per gpu: 23133.345782349952
Epoch: 46, Global Step: 470800, Data Step: 941600, Loss: 2.890625, Token per second per gpu: 22787.948352215342
Epoch: 46, Global Step: 470900, Data Step: 941800, Loss: 3.375, Token per second per gpu: 23408.046218704058
Epoch: 46, Global Step: 471000, Data Step: 942000, Loss: 2.90625, Token per second per gpu: 22805.90759468213
Epoch: 46, Global Step: 471100, Data Step: 942200, Loss: 2.53125, Token per second per gpu: 22846.44344674182
Epoch: 46, Global Step: 471200, Data Step: 942400, Loss: 2.6875, Token per second per gpu: 23021.737894706763
Epoch: 46, Global Step: 471300, Data Step: 942600, Loss: 3.046875, Token per second per gpu: 23486.18610988377
Epoch: 46, Global Step: 471400, Data Step: 942800, Loss: 2.90625, Token per second per gpu: 22898.820669541437
Epoch: 46, Global Step: 471500, Data Step: 943000, Loss: 2.9375, Token per second per gpu: 23487.548889813133
Epoch: 46, Global Step: 471600, Data Step: 943200, Loss: 2.609375, Token per second per gpu: 22957.462421086377
Epoch: 46, Global Step: 471700, Data Step: 943400, Loss: 2.796875, Token per second per gpu: 22744.448368265133
Epoch: 46, Global Step: 471800, Data Step: 943600, Loss: 3.171875, Token per second per gpu: 23376.565552520115
Epoch: 46, Global Step: 471900, Data Step: 943800, Loss: 3.171875, Token per second per gpu: 22586.78790329081
Epoch: 46, Global Step: 472000, Data Step: 944000, Loss: 2.71875, Token per second per gpu: 22753.76171549518
Epoch: 46, Global Step: 472100, Data Step: 944200, Loss: 2.90625, Token per second per gpu: 23664.11355243087
Epoch: 46, Global Step: 472200, Data Step: 944400, Loss: 2.75, Token per second per gpu: 21850.570499693662
Epoch: 46, Global Step: 472300, Data Step: 944600, Loss: 2.875, Token per second per gpu: 22887.656394480207
Epoch: 46, Global Step: 472400, Data Step: 944800, Loss: 3.328125, Token per second per gpu: 22603.310768386207
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0402 08:54:39.782957 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 47, Global Step: 472500, Data Step: 945000, Loss: 2.890625, Token per second per gpu: 22459.137507927593
Epoch: 47, Global Step: 472600, Data Step: 945200, Loss: 2.75, Token per second per gpu: 22639.59748521501
Epoch: 47, Global Step: 472700, Data Step: 945400, Loss: 2.859375, Token per second per gpu: 22684.708233935427
Epoch: 47, Global Step: 472800, Data Step: 945600, Loss: 3.171875, Token per second per gpu: 22406.516567410017
Epoch: 47, Global Step: 472900, Data Step: 945800, Loss: 2.828125, Token per second per gpu: 23216.82784505695
Epoch: 47, Global Step: 473000, Data Step: 946000, Loss: 3.171875, Token per second per gpu: 22560.826594806982
Epoch: 47, Global Step: 473100, Data Step: 946200, Loss: 3.0625, Token per second per gpu: 22501.312640701595
Epoch: 47, Global Step: 473200, Data Step: 946400, Loss: 2.921875, Token per second per gpu: 22432.304919962513
Epoch: 47, Global Step: 473300, Data Step: 946600, Loss: 3.15625, Token per second per gpu: 22443.44038560103
Epoch: 47, Global Step: 473400, Data Step: 946800, Loss: 2.890625, Token per second per gpu: 22809.375749077288
Epoch: 47, Global Step: 473500, Data Step: 947000, Loss: 2.875, Token per second per gpu: 22589.498729157895
Epoch: 47, Global Step: 473600, Data Step: 947200, Loss: 2.59375, Token per second per gpu: 22532.699744829897
Epoch: 47, Global Step: 473700, Data Step: 947400, Loss: 3.015625, Token per second per gpu: 22362.13275201097
Epoch: 47, Global Step: 473800, Data Step: 947600, Loss: 3.25, Token per second per gpu: 22562.91957847267
Epoch: 47, Global Step: 473900, Data Step: 947800, Loss: 3.015625, Token per second per gpu: 22286.084339875848
Epoch: 47, Global Step: 474000, Data Step: 948000, Loss: 3.046875, Token per second per gpu: 22751.48349630665
Epoch: 47, Global Step: 474100, Data Step: 948200, Loss: 2.921875, Token per second per gpu: 22753.928357106353
Epoch: 47, Global Step: 474200, Data Step: 948400, Loss: 3.125, Token per second per gpu: 22551.15794599155
Epoch: 47, Global Step: 474300, Data Step: 948600, Loss: 2.65625, Token per second per gpu: 22812.146517329453
Epoch: 47, Global Step: 474400, Data Step: 948800, Loss: 3.21875, Token per second per gpu: 22819.800339065605
Epoch: 47, Global Step: 474500, Data Step: 949000, Loss: 3.125, Token per second per gpu: 22330.979269208245
Epoch: 47, Global Step: 474600, Data Step: 949200, Loss: 2.875, Token per second per gpu: 22699.73948301593
Epoch: 47, Global Step: 474700, Data Step: 949400, Loss: 2.953125, Token per second per gpu: 22465.933121453643
Epoch: 47, Global Step: 474800, Data Step: 949600, Loss: 2.90625, Token per second per gpu: 22486.33356624842
Epoch: 47, Global Step: 474900, Data Step: 949800, Loss: 3.078125, Token per second per gpu: 22673.99715512145
Epoch: 47, Global Step: 475000, Data Step: 950000, Loss: 2.953125, Token per second per gpu: 22800.06515676522
Epoch: 47, Global Step: 475100, Data Step: 950200, Loss: 3.171875, Token per second per gpu: 22413.03276294133
Epoch: 47, Global Step: 475200, Data Step: 950400, Loss: 3.109375, Token per second per gpu: 22340.446530497185
Epoch: 47, Global Step: 475300, Data Step: 950600, Loss: 2.953125, Token per second per gpu: 22572.411923858035
Epoch: 47, Global Step: 475400, Data Step: 950800, Loss: 2.5625, Token per second per gpu: 22607.513626297605
Epoch: 47, Global Step: 475500, Data Step: 951000, Loss: 2.84375, Token per second per gpu: 22809.16621567646
Epoch: 47, Global Step: 475600, Data Step: 951200, Loss: 2.796875, Token per second per gpu: 22585.57020965849
Epoch: 47, Global Step: 475700, Data Step: 951400, Loss: 3.046875, Token per second per gpu: 22893.430050498722
Epoch: 47, Global Step: 475800, Data Step: 951600, Loss: 2.828125, Token per second per gpu: 22452.28974946796
Epoch: 47, Global Step: 475900, Data Step: 951800, Loss: 2.8125, Token per second per gpu: 23118.409624767286
Epoch: 47, Global Step: 476000, Data Step: 952000, Loss: 3.03125, Token per second per gpu: 22376.570833224625
Epoch: 47, Global Step: 476100, Data Step: 952200, Loss: 3.265625, Token per second per gpu: 22481.973455244402
Epoch: 47, Global Step: 476200, Data Step: 952400, Loss: 2.59375, Token per second per gpu: 22719.344152725025
Epoch: 47, Global Step: 476300, Data Step: 952600, Loss: 2.796875, Token per second per gpu: 21674.440234083322
Epoch: 47, Global Step: 476400, Data Step: 952800, Loss: 3.03125, Token per second per gpu: 22451.89831041615
Epoch: 47, Global Step: 476500, Data Step: 953000, Loss: 3.03125, Token per second per gpu: 21875.701337325776
Epoch: 47, Global Step: 476600, Data Step: 953200, Loss: 3.0, Token per second per gpu: 22417.166940529303
Epoch: 47, Global Step: 476700, Data Step: 953400, Loss: 2.890625, Token per second per gpu: 22559.39114049039
Epoch: 47, Global Step: 476800, Data Step: 953600, Loss: 2.4375, Token per second per gpu: 22831.837723857196
Epoch: 47, Global Step: 476900, Data Step: 953800, Loss: 2.921875, Token per second per gpu: 23137.638169294514
Epoch: 47, Global Step: 477000, Data Step: 954000, Loss: 2.953125, Token per second per gpu: 22806.434622795117
Epoch: 47, Global Step: 477100, Data Step: 954200, Loss: 3.03125, Token per second per gpu: 22816.851826267382
Epoch: 47, Global Step: 477200, Data Step: 954400, Loss: 3.03125, Token per second per gpu: 22988.207299911017
Epoch: 47, Global Step: 477300, Data Step: 954600, Loss: 3.046875, Token per second per gpu: 23493.270482270233
Epoch: 47, Global Step: 477400, Data Step: 954800, Loss: 2.9375, Token per second per gpu: 23123.451814638323
Epoch: 47, Global Step: 477500, Data Step: 955000, Loss: 2.953125, Token per second per gpu: 22810.990297505705
Epoch: 47, Global Step: 477600, Data Step: 955200, Loss: 3.0625, Token per second per gpu: 22992.813964463356
Epoch: 47, Global Step: 477700, Data Step: 955400, Loss: 2.71875, Token per second per gpu: 22513.08299466332
Epoch: 47, Global Step: 477800, Data Step: 955600, Loss: 3.109375, Token per second per gpu: 22800.84798664517
Epoch: 47, Global Step: 477900, Data Step: 955800, Loss: 3.0, Token per second per gpu: 21203.36458251057
Epoch: 47, Global Step: 478000, Data Step: 956000, Loss: 2.734375, Token per second per gpu: 22844.815451122646
Epoch: 47, Global Step: 478100, Data Step: 956200, Loss: 2.96875, Token per second per gpu: 22878.020171657638
Epoch: 47, Global Step: 478200, Data Step: 956400, Loss: 2.953125, Token per second per gpu: 22869.393126853865
Epoch: 47, Global Step: 478300, Data Step: 956600, Loss: 3.046875, Token per second per gpu: 22919.100268526174
Epoch: 47, Global Step: 478400, Data Step: 956800, Loss: 2.671875, Token per second per gpu: 22693.44332156566
Epoch: 47, Global Step: 478500, Data Step: 957000, Loss: 2.6875, Token per second per gpu: 22960.6408573541
Epoch: 47, Global Step: 478600, Data Step: 957200, Loss: 3.328125, Token per second per gpu: 23374.185702355
Epoch: 47, Global Step: 478700, Data Step: 957400, Loss: 3.109375, Token per second per gpu: 23416.61505178531
Epoch: 47, Global Step: 478800, Data Step: 957600, Loss: 3.046875, Token per second per gpu: 22987.12003951192
Epoch: 47, Global Step: 478900, Data Step: 957800, Loss: 2.78125, Token per second per gpu: 23030.19347260976
Epoch: 47, Global Step: 479000, Data Step: 958000, Loss: 3.21875, Token per second per gpu: 22988.45019115736
Epoch: 47, Global Step: 479100, Data Step: 958200, Loss: 2.90625, Token per second per gpu: 23016.346091358508
Epoch: 47, Global Step: 479200, Data Step: 958400, Loss: 3.140625, Token per second per gpu: 23126.518271822857
Epoch: 47, Global Step: 479300, Data Step: 958600, Loss: 3.0, Token per second per gpu: 22739.54509466316
Epoch: 47, Global Step: 479400, Data Step: 958800, Loss: 3.0, Token per second per gpu: 22898.222864579322
Epoch: 47, Global Step: 479500, Data Step: 959000, Loss: 2.78125, Token per second per gpu: 22868.634849941234
Epoch: 47, Global Step: 479600, Data Step: 959200, Loss: 2.671875, Token per second per gpu: 23205.722483031514
Epoch: 47, Global Step: 479700, Data Step: 959400, Loss: 3.140625, Token per second per gpu: 22803.9085539217
Epoch: 47, Global Step: 479800, Data Step: 959600, Loss: 3.046875, Token per second per gpu: 22844.43802811956
Epoch: 47, Global Step: 479900, Data Step: 959800, Loss: 2.96875, Token per second per gpu: 22895.623392382713
Epoch: 47, Global Step: 480000, Data Step: 960000, Loss: 2.84375, Token per second per gpu: 22730.362755675254
I0402 11:34:56.849930 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0402 11:34:56.850378 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-02 11:34:56,850] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-02 11:34:56,854] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-02 11:34:56,855] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-02 11:34:57,434] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-02 11:34:57,436] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-02 11:34:57,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-02 11:34:57,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-02 11:34:57,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-02 11:34:57,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-02 11:34:57,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-02 11:34:59,606] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-02 11:34:59,607] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-02 11:34:59,607] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 11:34:59,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-02 11:34:59,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-02 11:34:59,690] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-02 11:34:59,690] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-02 11:34:59,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 11:34:59,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 11:34:59,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-02 11:34:59,707] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-02 11:34:59,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 11:34:59,716] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-02 11:34:59,716] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-02 11:34:59,716] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 11:34:59,728] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-02 11:34:59,729] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-02 11:34:59,729] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0402 11:34:59.730005 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0402 11:34:59.730807 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0402 11:34:59.731099 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0402 11:34:59.732648 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 47, Global Step: 480100, Data Step: 960200, Loss: 3.15625, Token per second per gpu: 22308.26369339264
Epoch: 47, Global Step: 480200, Data Step: 960400, Loss: 3.03125, Token per second per gpu: 22921.066191333615
Epoch: 47, Global Step: 480300, Data Step: 960600, Loss: 2.703125, Token per second per gpu: 22723.35363324759
Epoch: 47, Global Step: 480400, Data Step: 960800, Loss: 3.046875, Token per second per gpu: 22556.32226838095
Epoch: 47, Global Step: 480500, Data Step: 961000, Loss: 2.859375, Token per second per gpu: 22803.916044504003
Epoch: 47, Global Step: 480600, Data Step: 961200, Loss: 2.84375, Token per second per gpu: 22759.40141290008
Epoch: 47, Global Step: 480700, Data Step: 961400, Loss: 3.265625, Token per second per gpu: 22734.287899754152
Epoch: 47, Global Step: 480800, Data Step: 961600, Loss: 2.90625, Token per second per gpu: 22762.72928413734
Epoch: 47, Global Step: 480900, Data Step: 961800, Loss: 2.921875, Token per second per gpu: 23158.293130941274
Epoch: 47, Global Step: 481000, Data Step: 962000, Loss: 2.625, Token per second per gpu: 23397.32785859883
Epoch: 47, Global Step: 481100, Data Step: 962200, Loss: 3.15625, Token per second per gpu: 22608.077815147633
Epoch: 47, Global Step: 481200, Data Step: 962400, Loss: 2.90625, Token per second per gpu: 22792.800185809807
Epoch: 47, Global Step: 481300, Data Step: 962600, Loss: 3.171875, Token per second per gpu: 22494.690772608166
Epoch: 47, Global Step: 481400, Data Step: 962800, Loss: 3.078125, Token per second per gpu: 22550.418604574927
Epoch: 47, Global Step: 481500, Data Step: 963000, Loss: 3.078125, Token per second per gpu: 22556.276568712674
Epoch: 47, Global Step: 481600, Data Step: 963200, Loss: 2.984375, Token per second per gpu: 22452.44637064302
Epoch: 47, Global Step: 481700, Data Step: 963400, Loss: 2.890625, Token per second per gpu: 22459.555215262895
Epoch: 47, Global Step: 481800, Data Step: 963600, Loss: 2.859375, Token per second per gpu: 22880.845392506137
Epoch: 47, Global Step: 481900, Data Step: 963800, Loss: 2.890625, Token per second per gpu: 23053.99316633478
Epoch: 47, Global Step: 482000, Data Step: 964000, Loss: 2.703125, Token per second per gpu: 22946.741398144964
Epoch: 47, Global Step: 482100, Data Step: 964200, Loss: 3.0625, Token per second per gpu: 21926.943787547927
Epoch: 47, Global Step: 482200, Data Step: 964400, Loss: 2.75, Token per second per gpu: 22453.268824171155
Epoch: 47, Global Step: 482300, Data Step: 964600, Loss: 3.03125, Token per second per gpu: 22624.733089212405
Epoch: 47, Global Step: 482400, Data Step: 964800, Loss: 3.265625, Token per second per gpu: 22598.836054371073
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0402 12:27:15.425814 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 48, Global Step: 482500, Data Step: 965000, Loss: 2.984375, Token per second per gpu: 21553.68061002456
Epoch: 48, Global Step: 482600, Data Step: 965200, Loss: 2.75, Token per second per gpu: 22732.20651005727
Epoch: 48, Global Step: 482700, Data Step: 965400, Loss: 3.296875, Token per second per gpu: 22218.265243939855
Epoch: 48, Global Step: 482800, Data Step: 965600, Loss: 3.0, Token per second per gpu: 22463.606065965858
Epoch: 48, Global Step: 482900, Data Step: 965800, Loss: 2.921875, Token per second per gpu: 22911.149039900145
Epoch: 48, Global Step: 483000, Data Step: 966000, Loss: 3.109375, Token per second per gpu: 23224.4210072368
Epoch: 48, Global Step: 483100, Data Step: 966200, Loss: 2.765625, Token per second per gpu: 22885.52194118496
Epoch: 48, Global Step: 483200, Data Step: 966400, Loss: 3.09375, Token per second per gpu: 23209.173202582922
Epoch: 48, Global Step: 483300, Data Step: 966600, Loss: 3.34375, Token per second per gpu: 22682.0948281436
Epoch: 48, Global Step: 483400, Data Step: 966800, Loss: 2.828125, Token per second per gpu: 22822.831672566255
Epoch: 48, Global Step: 483500, Data Step: 967000, Loss: 2.921875, Token per second per gpu: 22982.385396119786
Epoch: 48, Global Step: 483600, Data Step: 967200, Loss: 3.078125, Token per second per gpu: 22623.35275009517
Epoch: 48, Global Step: 483700, Data Step: 967400, Loss: 3.0, Token per second per gpu: 23128.86354227764
Epoch: 48, Global Step: 483800, Data Step: 967600, Loss: 2.796875, Token per second per gpu: 23393.299060628244
Epoch: 48, Global Step: 483900, Data Step: 967800, Loss: 3.140625, Token per second per gpu: 23128.155536111
Epoch: 48, Global Step: 484000, Data Step: 968000, Loss: 3.015625, Token per second per gpu: 23366.0465263082
Epoch: 48, Global Step: 484100, Data Step: 968200, Loss: 2.734375, Token per second per gpu: 23047.495552953213
Epoch: 48, Global Step: 484200, Data Step: 968400, Loss: 2.609375, Token per second per gpu: 23453.232557294956
Epoch: 48, Global Step: 484300, Data Step: 968600, Loss: 3.15625, Token per second per gpu: 22862.927003637647
Epoch: 48, Global Step: 484400, Data Step: 968800, Loss: 2.796875, Token per second per gpu: 23007.959969319458
Epoch: 48, Global Step: 484500, Data Step: 969000, Loss: 3.03125, Token per second per gpu: 22691.87107374866
Epoch: 48, Global Step: 484600, Data Step: 969200, Loss: 2.875, Token per second per gpu: 22523.78777627901
Epoch: 48, Global Step: 484700, Data Step: 969400, Loss: 2.734375, Token per second per gpu: 22653.79723614832
Epoch: 48, Global Step: 484800, Data Step: 969600, Loss: 3.03125, Token per second per gpu: 22808.576571014884
Epoch: 48, Global Step: 484900, Data Step: 969800, Loss: 3.078125, Token per second per gpu: 22725.262474706036
Epoch: 48, Global Step: 485000, Data Step: 970000, Loss: 2.84375, Token per second per gpu: 22796.94134444047
Epoch: 48, Global Step: 485100, Data Step: 970200, Loss: 2.796875, Token per second per gpu: 23722.751528963876
Epoch: 48, Global Step: 485200, Data Step: 970400, Loss: 2.890625, Token per second per gpu: 23713.475634721606
Epoch: 48, Global Step: 485300, Data Step: 970600, Loss: 3.015625, Token per second per gpu: 23129.933073365624
Epoch: 48, Global Step: 485400, Data Step: 970800, Loss: 3.125, Token per second per gpu: 22778.832599595276
Epoch: 48, Global Step: 485500, Data Step: 971000, Loss: 3.125, Token per second per gpu: 23022.863667966467
Epoch: 48, Global Step: 485600, Data Step: 971200, Loss: 3.03125, Token per second per gpu: 23106.692385480477
Epoch: 48, Global Step: 485700, Data Step: 971400, Loss: 2.765625, Token per second per gpu: 23162.385237299997
Epoch: 48, Global Step: 485800, Data Step: 971600, Loss: 3.15625, Token per second per gpu: 22437.019397300453
Epoch: 48, Global Step: 485900, Data Step: 971800, Loss: 3.0625, Token per second per gpu: 22759.004209128623
Epoch: 48, Global Step: 486000, Data Step: 972000, Loss: 3.296875, Token per second per gpu: 23393.44965001482
Epoch: 48, Global Step: 486100, Data Step: 972200, Loss: 3.390625, Token per second per gpu: 22691.125418211162
Epoch: 48, Global Step: 486200, Data Step: 972400, Loss: 2.625, Token per second per gpu: 25039.61489888815
Epoch: 48, Global Step: 486300, Data Step: 972600, Loss: 2.84375, Token per second per gpu: 25009.661744240868
Epoch: 48, Global Step: 486400, Data Step: 972800, Loss: 2.609375, Token per second per gpu: 24992.569882248034
Epoch: 48, Global Step: 486500, Data Step: 973000, Loss: 2.78125, Token per second per gpu: 24389.2458205831
Epoch: 48, Global Step: 486600, Data Step: 973200, Loss: 3.09375, Token per second per gpu: 25038.86402265871
Epoch: 48, Global Step: 486700, Data Step: 973400, Loss: 2.921875, Token per second per gpu: 25042.194546197083
Epoch: 48, Global Step: 486800, Data Step: 973600, Loss: 2.609375, Token per second per gpu: 25021.671284662556
Epoch: 48, Global Step: 486900, Data Step: 973800, Loss: 2.828125, Token per second per gpu: 25021.196894729037
Epoch: 48, Global Step: 487000, Data Step: 974000, Loss: 3.15625, Token per second per gpu: 25002.92271806212
Epoch: 48, Global Step: 487100, Data Step: 974200, Loss: 2.96875, Token per second per gpu: 24984.649686646015
Epoch: 48, Global Step: 487200, Data Step: 974400, Loss: 2.734375, Token per second per gpu: 24954.418278530233
Epoch: 48, Global Step: 487300, Data Step: 974600, Loss: 2.78125, Token per second per gpu: 24312.591824367686
Epoch: 48, Global Step: 487400, Data Step: 974800, Loss: 3.15625, Token per second per gpu: 24972.10446040494
Epoch: 48, Global Step: 487500, Data Step: 975000, Loss: 2.78125, Token per second per gpu: 24931.881198563708
Epoch: 48, Global Step: 487600, Data Step: 975200, Loss: 2.8125, Token per second per gpu: 24958.258642003777
Epoch: 48, Global Step: 487700, Data Step: 975400, Loss: 3.015625, Token per second per gpu: 24949.777792585664
Epoch: 48, Global Step: 487800, Data Step: 975600, Loss: 3.015625, Token per second per gpu: 24325.394737400395
Epoch: 48, Global Step: 487900, Data Step: 975800, Loss: 2.6875, Token per second per gpu: 25012.73967577956
Epoch: 48, Global Step: 488000, Data Step: 976000, Loss: 3.109375, Token per second per gpu: 25079.463077886823
Epoch: 48, Global Step: 488100, Data Step: 976200, Loss: 2.65625, Token per second per gpu: 25116.58137243301
Epoch: 48, Global Step: 488200, Data Step: 976400, Loss: 3.140625, Token per second per gpu: 25159.042222924796
Epoch: 48, Global Step: 488300, Data Step: 976600, Loss: 2.796875, Token per second per gpu: 25181.56384140225
Epoch: 48, Global Step: 488400, Data Step: 976800, Loss: 2.859375, Token per second per gpu: 25209.369141188436
Epoch: 48, Global Step: 488500, Data Step: 977000, Loss: 2.796875, Token per second per gpu: 25227.10615929039
Epoch: 48, Global Step: 488600, Data Step: 977200, Loss: 2.703125, Token per second per gpu: 25222.81384438328
Epoch: 48, Global Step: 488700, Data Step: 977400, Loss: 2.984375, Token per second per gpu: 25223.554410156245
Epoch: 48, Global Step: 488800, Data Step: 977600, Loss: 3.296875, Token per second per gpu: 25144.18559486542
Epoch: 48, Global Step: 488900, Data Step: 977800, Loss: 3.171875, Token per second per gpu: 25081.731017317263
Epoch: 48, Global Step: 489000, Data Step: 978000, Loss: 2.671875, Token per second per gpu: 25050.135117549962
Epoch: 48, Global Step: 489100, Data Step: 978200, Loss: 3.078125, Token per second per gpu: 25033.35831558254
Epoch: 48, Global Step: 489200, Data Step: 978400, Loss: 2.859375, Token per second per gpu: 25034.54866817876
Epoch: 48, Global Step: 489300, Data Step: 978600, Loss: 2.8125, Token per second per gpu: 25025.942744327716
Epoch: 48, Global Step: 489400, Data Step: 978800, Loss: 3.015625, Token per second per gpu: 25002.93565612919
Epoch: 48, Global Step: 489500, Data Step: 979000, Loss: 3.046875, Token per second per gpu: 24984.53873742623
Epoch: 48, Global Step: 489600, Data Step: 979200, Loss: 2.734375, Token per second per gpu: 25013.13527612001
Epoch: 48, Global Step: 489700, Data Step: 979400, Loss: 2.953125, Token per second per gpu: 25025.274913455487
Epoch: 48, Global Step: 489800, Data Step: 979600, Loss: 2.75, Token per second per gpu: 25011.179823047474
Epoch: 48, Global Step: 489900, Data Step: 979800, Loss: 3.15625, Token per second per gpu: 25002.415349382485
Epoch: 48, Global Step: 490000, Data Step: 980000, Loss: 2.96875, Token per second per gpu: 25000.962496847274
I0402 14:58:09.831717 140366881915904 logging.py:61] Saving current state to ckpt/vocab_32k_gpt2
I0402 14:58:09.832160 140366881915904 logging.py:61] Saving DeepSpeed Model and Optimizer
[2024-04-02 14:58:09,832] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2024-04-02 14:58:09,836] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt
[2024-04-02 14:58:09,836] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt...
[2024-04-02 14:58:10,320] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/mp_rank_00_model_states.pt.
[2024-04-02 14:58:10,322] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-04-02 14:58:10,322] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-04-02 14:58:10,322] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-02 14:58:10,322] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-04-02 14:58:10,322] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-04-02 14:58:10,323] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-04-02 14:58:12,274] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-04-02 14:58:12,274] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-04-02 14:58:12,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 14:58:12,458] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-04-02 14:58:12,458] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-04-02 14:58:12,458] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 14:58:12,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-04-02 14:58:12,474] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-04-02 14:58:12,474] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 14:58:12,507] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-02 14:58:12,515] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-02 14:58:12,515] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 14:58:12,516] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-04-02 14:58:12,516] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-04-02 14:58:12,516] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
[2024-04-02 14:58:12,521] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-04-02 14:58:12,521] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ckpt/vocab_32k_gpt2/pytorch_model/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-04-02 14:58:12,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
I0402 14:58:12.522248 140366881915904 logging.py:61] DeepSpeed Model and Optimizer saved to output dir ckpt/vocab_32k_gpt2/pytorch_model
I0402 14:58:12.522927 140366881915904 logging.py:61] Scheduler state saved in ckpt/vocab_32k_gpt2/scheduler.bin
I0402 14:58:12.523183 140366881915904 logging.py:61] Sampler state for dataloader 0 saved in ckpt/vocab_32k_gpt2/sampler.bin
I0402 14:58:12.524599 140366881915904 logging.py:61] Random states saved in ckpt/vocab_32k_gpt2/random_states_0.pkl
Epoch: 48, Global Step: 490100, Data Step: 980200, Loss: 3.0, Token per second per gpu: 24427.668805808047
Epoch: 48, Global Step: 490200, Data Step: 980400, Loss: 2.890625, Token per second per gpu: 24976.270497600726
Epoch: 48, Global Step: 490300, Data Step: 980600, Loss: 3.078125, Token per second per gpu: 24956.719910781037
Epoch: 48, Global Step: 490400, Data Step: 980800, Loss: 3.453125, Token per second per gpu: 24949.371723319295
Epoch: 48, Global Step: 490500, Data Step: 981000, Loss: 3.0625, Token per second per gpu: 24933.90769879911
Epoch: 48, Global Step: 490600, Data Step: 981200, Loss: 2.875, Token per second per gpu: 24940.854118444306
Epoch: 48, Global Step: 490700, Data Step: 981400, Loss: 2.90625, Token per second per gpu: 24925.96106127467
Epoch: 48, Global Step: 490800, Data Step: 981600, Loss: 2.984375, Token per second per gpu: 25004.937853983345
Epoch: 48, Global Step: 490900, Data Step: 981800, Loss: 2.984375, Token per second per gpu: 25069.65138552732
Epoch: 48, Global Step: 491000, Data Step: 982000, Loss: 3.0, Token per second per gpu: 25051.1369203561
Epoch: 48, Global Step: 491100, Data Step: 982200, Loss: 2.984375, Token per second per gpu: 25027.05782809287
Epoch: 48, Global Step: 491200, Data Step: 982400, Loss: 3.171875, Token per second per gpu: 25013.651419231926
Epoch: 48, Global Step: 491300, Data Step: 982600, Loss: 3.203125, Token per second per gpu: 25007.487270881797
Epoch: 48, Global Step: 491400, Data Step: 982800, Loss: 2.859375, Token per second per gpu: 24993.086418047795
Epoch: 48, Global Step: 491500, Data Step: 983000, Loss: 3.078125, Token per second per gpu: 25015.00613076619
Epoch: 48, Global Step: 491600, Data Step: 983200, Loss: 3.015625, Token per second per gpu: 25009.590857426345
Epoch: 48, Global Step: 491700, Data Step: 983400, Loss: 2.71875, Token per second per gpu: 25000.13513563604
Epoch: 48, Global Step: 491800, Data Step: 983600, Loss: 2.78125, Token per second per gpu: 24986.3630392602
Epoch: 48, Global Step: 491900, Data Step: 983800, Loss: 2.703125, Token per second per gpu: 24371.218987952292
Epoch: 48, Global Step: 492000, Data Step: 984000, Loss: 2.859375, Token per second per gpu: 24964.341171167216
Epoch: 48, Global Step: 492100, Data Step: 984200, Loss: 2.9375, Token per second per gpu: 24948.693596501187
Epoch: 48, Global Step: 492200, Data Step: 984400, Loss: 2.84375, Token per second per gpu: 24926.93541616133
Epoch: 48, Global Step: 492300, Data Step: 984600, Loss: 3.0625, Token per second per gpu: 24912.57639835019
Epoch: 48, Global Step: 492400, Data Step: 984800, Loss: 2.828125, Token per second per gpu: 24929.361424210598
Epoch: 48, Global Step: 492500, Data Step: 985000, Loss: 2.78125, Token per second per gpu: 24971.42530377154
Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
W0402 15:46:44.614060 140366881915904 iterable_dataset.py:1267] Too many dataloader workers: 16 (max is dataset.n_shards=1). Stopping 15 dataloader workers.
Epoch: 49, Global Step: 492600, Data Step: 985200, Loss: 2.953125, Token per second per gpu: 24376.87841690573
Epoch: 49, Global Step: 492700, Data Step: 985400, Loss: 3.1875, Token per second per gpu: 24982.122490277445
Epoch: 49, Global Step: 492800, Data Step: 985600, Loss: 3.078125, Token per second per gpu: 24987.607231530732
Epoch: 49, Global Step: 492900, Data Step: 985800, Loss: 2.9375, Token per second per gpu: 24997.844888050055
Epoch: 49, Global Step: 493000, Data Step: 986000, Loss: 2.671875, Token per second per gpu: 24999.909806908057
Epoch: 49, Global Step: 493100, Data Step: 986200, Loss: 2.90625, Token per second per gpu: 25007.82746366444
Epoch: 49, Global Step: 493200, Data Step: 986400, Loss: 2.875, Token per second per gpu: 25004.42926483088
Epoch: 49, Global Step: 493300, Data Step: 986600, Loss: 2.703125, Token per second per gpu: 25004.740181876205
Epoch: 49, Global Step: 493400, Data Step: 986800, Loss: 2.625, Token per second per gpu: 24979.529376029255
Epoch: 49, Global Step: 493500, Data Step: 987000, Loss: 2.71875, Token per second per gpu: 24970.183290197103
Epoch: 49, Global Step: 493600, Data Step: 987200, Loss: 2.984375, Token per second per gpu: 24973.990970979725
Epoch: 49, Global Step: 493700, Data Step: 987400, Loss: 3.09375, Token per second per gpu: 24929.469208588802
Epoch: 49, Global Step: 493800, Data Step: 987600, Loss: 2.984375, Token per second per gpu: 24929.755111810042
Epoch: 49, Global Step: 493900, Data Step: 987800, Loss: 2.765625, Token per second per gpu: 24944.384024448034
Epoch: 49, Global Step: 494000, Data Step: 988000, Loss: 3.0625, Token per second per gpu: 24955.90099186583
Epoch: 49, Global Step: 494100, Data Step: 988200, Loss: 3.0, Token per second per gpu: 25007.581805339105
Epoch: 49, Global Step: 494200, Data Step: 988400, Loss: 2.953125, Token per second per gpu: 25028.333457270553
Epoch: 49, Global Step: 494300, Data Step: 988600, Loss: 2.859375, Token per second per gpu: 25033.526713716987
Epoch: 49, Global Step: 494400, Data Step: 988800, Loss: 2.84375, Token per second per gpu: 25028.543585574706
Epoch: 49, Global Step: 494500, Data Step: 989000, Loss: 2.890625, Token per second per gpu: 25019.527162450955
Epoch: 49, Global Step: 494600, Data Step: 989200, Loss: 2.890625, Token per second per gpu: 25016.43787218802
Epoch: 49, Global Step: 494700, Data Step: 989400, Loss: 3.015625, Token per second per gpu: 25010.24376658999
Epoch: 49, Global Step: 494800, Data Step: 989600, Loss: 3.375, Token per second per gpu: 25011.50955158796
Epoch: 49, Global Step: 494900, Data Step: 989800, Loss: 3.09375, Token per second per gpu: 24997.254027987627
Epoch: 49, Global Step: 495000, Data Step: 990000, Loss: 2.453125, Token per second per gpu: 24999.473079141368
Epoch: 49, Global Step: 495100, Data Step: 990200, Loss: 3.03125, Token per second per gpu: 24999.591093989846
Epoch: 49, Global Step: 495200, Data Step: 990400, Loss: 3.03125, Token per second per gpu: 24984.58974190559
Epoch: 49, Global Step: 495300, Data Step: 990600, Loss: 2.984375, Token per second per gpu: 24978.30101828715
Epoch: 49, Global Step: 495400, Data Step: 990800, Loss: 2.890625, Token per second per gpu: 24987.59591167564
Epoch: 49, Global Step: 495500, Data Step: 991000, Loss: 3.1875, Token per second per gpu: 24980.056221029234
Epoch: 49, Global Step: 495600, Data Step: 991200, Loss: 3.078125, Token per second per gpu: 24961.139380926856
Epoch: 49, Global Step: 495700, Data Step: 991400, Loss: 2.953125, Token per second per gpu: 24943.44121751477
Epoch: 49, Global Step: 495800, Data Step: 991600, Loss: 3.09375, Token per second per gpu: 24959.631754090202
Epoch: 49, Global Step: 495900, Data Step: 991800, Loss: 2.921875, Token per second per gpu: 24943.722290556132
Epoch: 49, Global Step: 496000, Data Step: 992000, Loss: 3.078125, Token per second per gpu: 25050.585461739392
Epoch: 49, Global Step: 496100, Data Step: 992200, Loss: 2.546875, Token per second per gpu: 25096.8469370316
Epoch: 49, Global Step: 496200, Data Step: 992400, Loss: 2.984375, Token per second per gpu: 25087.11203616058
Epoch: 49, Global Step: 496300, Data Step: 992600, Loss: 2.734375, Token per second per gpu: 25069.032725781515
Epoch: 49, Global Step: 496400, Data Step: 992800, Loss: 2.734375, Token per second per gpu: 24402.117487076055
Epoch: 49, Global Step: 496500, Data Step: 993000, Loss: 2.796875, Token per second per gpu: 25051.39034490281
Epoch: 49, Global Step: 496600, Data Step: 993200, Loss: 3.265625, Token per second per gpu: 25040.00922166528
Epoch: 49, Global Step: 496700, Data Step: 993400, Loss: 3.15625, Token per second per gpu: 25046.265339324418
Epoch: 49, Global Step: 496800, Data Step: 993600, Loss: 2.90625, Token per second per gpu: 25055.278129350452
Epoch: 49, Global Step: 496900, Data Step: 993800, Loss: 2.484375, Token per second per gpu: 25047.5858793894
Epoch: 49, Global Step: 497000, Data Step: 994000, Loss: 2.796875, Token per second per gpu: 25057.08855540475
Epoch: 49, Global Step: 497100, Data Step: 994200, Loss: 2.96875, Token per second per gpu: 25058.78311088673
Epoch: 49, Global Step: 497200, Data Step: 994400, Loss: 2.921875, Token per second per gpu: 25065.743951902936
Epoch: 49, Global Step: 497300, Data Step: 994600, Loss: 3.078125, Token per second per gpu: 25048.49710050309
Epoch: 49, Global Step: 497400, Data Step: 994800, Loss: 3.09375, Token per second per gpu: 25053.46713350416
Epoch: 49, Global Step: 497500, Data Step: 995000, Loss: 3.1875, Token per second per gpu: 25040.798319752605
Epoch: 49, Global Step: 497600, Data Step: 995200, Loss: 2.828125, Token per second per gpu: 25032.625037118378
Epoch: 49, Global Step: 497700, Data Step: 995400, Loss: 2.953125, Token per second per gpu: 25014.093200786505
Epoch: 49, Global Step: 497800, Data Step: 995600, Loss: 3.03125, Token per second per gpu: 24396.150951779236
Epoch: 49, Global Step: 497900, Data Step: 995800, Loss: 3.125, Token per second per gpu: 25026.94613894882
Epoch: 49, Global Step: 498000, Data Step: 996000, Loss: 2.75, Token per second per gpu: 25004.09708333007
Epoch: 49, Global Step: 498100, Data Step: 996200, Loss: 2.921875, Token per second per gpu: 24980.59719138746
Epoch: 49, Global Step: 498200, Data Step: 996400, Loss: 3.109375, Token per second per gpu: 24979.245687544502
Epoch: 49, Global Step: 498300, Data Step: 996600, Loss: 2.8125, Token per second per gpu: 24956.1278996699
Epoch: 49, Global Step: 498400, Data Step: 996800, Loss: 3.0, Token per second per gpu: 24370.38218878752
Epoch: 49, Global Step: 498500, Data Step: 997000, Loss: 2.984375, Token per second per gpu: 24961.855169004382
Epoch: 49, Global Step: 498600, Data Step: 997200, Loss: 2.859375, Token per second per gpu: 24926.08779562525
Epoch: 49, Global Step: 498700, Data Step: 997400, Loss: 3.234375, Token per second per gpu: 24956.418487923147
Epoch: 49, Global Step: 498800, Data Step: 997600, Loss: 3.078125, Token per second per gpu: 25053.329124011878
Epoch: 49, Global Step: 498900, Data Step: 997800, Loss: 3.0, Token per second per gpu: 25062.141762888543
Epoch: 49, Global Step: 499000, Data Step: 998000, Loss: 3.109375, Token per second per gpu: 25054.34329022334
Epoch: 49, Global Step: 499100, Data Step: 998200, Loss: 2.515625, Token per second per gpu: 25053.108654924134
Epoch: 49, Global Step: 499200, Data Step: 998400, Loss: 2.90625, Token per second per gpu: 25026.842332198095
Epoch: 49, Global Step: 499300, Data Step: 998600, Loss: 3.078125, Token per second per gpu: 25034.56719051132
Epoch: 49, Global Step: 499400, Data Step: 998800, Loss: 3.171875, Token per second per gpu: 25003.31200448356
Epoch: 49, Global Step: 499500, Data Step: 999000, Loss: 2.9375, Token per second per gpu: 25002.5784668916
Epoch: 49, Global Step: 499600, Data Step: 999200, Loss: 2.34375, Token per second per gpu: 25007.652525449746
Epoch: 49, Global Step: 499700, Data Step: 999400, Loss: 2.921875, Token per second per gpu: 24987.72291173374
Epoch: 49, Global Step: 499800, Data Step: 999600, Loss: 2.90625, Token per second per gpu: 24982.746994893707
Epoch: 49, Global Step: 499900, Data Step: 999800, Loss: 3.1875, Token per second per gpu: 24994.85719800611
wandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                 Losses/total_loss ▇▇▇▅▄▅█▇▆▅▄▃▁▃▂▅▂▄▅▃▄▅▃▆▇▂▄▅▂▅▄▄▄▄▃▃▄▁▁▃
wandb:                Training/Data Step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                    Training/Epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:              Training/Global Step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                       Training/LR ███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb: Training/Token per second per gpu ▆▅▅▄▅▅▄▄███▁▄▅▃▄▄▅▄██▇▅█▄▄██▄██████▄▅▅██
wandb: 
wandb: Run summary:
wandb:                 Losses/total_loss 3.1875
wandb:                Training/Data Step 999800
wandb:                    Training/Epoch 49
wandb:              Training/Global Step 499900
wandb:                       Training/LR 0.0
wandb: Training/Token per second per gpu 24994.8572
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /data/yslan/nlp_project/VOCAB_32K_GPT2/wandb/offline-run-20240326_154710-fommoypo
wandb: Find logs at: ./wandb/offline-run-20240326_154710-fommoypo/logs
